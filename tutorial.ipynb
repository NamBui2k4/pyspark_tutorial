{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKhANDPI2-YD"
      },
      "source": [
        "# Pyspark tutorial 🎯"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y9ghrep9ckQ"
      },
      "source": [
        "**Ghi chú:**\n",
        "\n",
        " 🔹tutorial này được thực hiện trên google colab nên không bị lỗi khi chạy code. Nếu chạy trên jupyter notebook được host bởi local machine chắc chắn sẽ lỗi (chắc vì lý do cấu hình cài đặt hay gì đó :v). Mặc dù đã mò mẫm cả tuần nhưng mình vẫn chưa có giải pháp nào cho vấn đề này nên mình chấp nhận thực hiện nó trên google colab. 🤷‍♂️\n",
        "\n",
        " 🔹Thêm nữa, tutorial này được xây dựng dựa trên nhiều nguồn, trong đó có các blog mà mình kham khảo. Sau đây xin cảm ơn blog của [longcnttbkhn](https://longcnttbkhn.github.io/huong-dan-spark-co-ban-cho-nguoi-moi/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXX24zUvUB_y"
      },
      "source": [
        "\n",
        "![](https://th.bing.com/th/id/OIP.I3eg_GSGbjpQ0O8GDuHVdgHaFL?rs=1&pid=ImgDetMain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-Cj9XV-2-YF"
      },
      "source": [
        "## Installation ⬇️"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GFFs2FY2-YF"
      },
      "source": [
        "Để làm việc với spark, chúng ta cần cài đặt các thành phần sau:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "477uUkwe2-YF"
      },
      "source": [
        "- 📌 Java (OpenJDK 8 hoặc 11):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iqBQ9FIG4Ed"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvamacLl2-YF"
      },
      "source": [
        "- 📌 Apache Spark\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rjp-b0FGrrA"
      },
      "outputs": [],
      "source": [
        "# tải gói spark apache về\n",
        "!wget https://downloads.apache.org/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz\n",
        "\n",
        "# giải nén\n",
        "!tar -xvzf /content/spark-3.5.4-bin-hadoop3.tgz\n",
        "\n",
        "# di chuyển Spark vào thư mục /opt/spark (đây là quy ước, tự tìm hiểu thêm)\n",
        "!sudo mv /content/spark-3.5.4-bin-hadoop3 /opt/spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQP7D00oGyR6"
      },
      "source": [
        "🔧 Thiết lập biến môi trường như sau:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RR3VHbS8Glrt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/opt/spark\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z26vqdxlMLxp",
        "outputId": "876abd39-8def-4ea1-d315-58f067ea5e3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/opt/spark\n",
            "/usr/lib/jvm/java-8-openjdk-amd64\n"
          ]
        }
      ],
      "source": [
        "# kiểm tra môi trường\n",
        "! echo $SPARK_HOME\n",
        "! echo $JAVA_HOME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-D7dd0y2-YG"
      },
      "source": [
        "- 📌 pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHxirntQ2-YG",
        "outputId": "af1e32fb-1727-40f7-a89e-213e80e54a24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.4)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i6VQ3wE2-YG"
      },
      "source": [
        "- 📌 findspark - tùy chọn, nếu chạy trên Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVLN3of82-YH",
        "outputId": "942e1662-b5cf-452f-c9be-5ebf89d3aea8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: findspark in /usr/local/lib/python3.11/dist-packages (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install findspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zwpg9kB1X0nv"
      },
      "source": [
        "## Run Spark app 🚀"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfPtbeXrZXHk"
      },
      "outputs": [],
      "source": [
        "# khởi tạo một file .py\n",
        "! touch firstapp.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmXe6eUfZxUq",
        "outputId": "429b848e-0ce7-4adb-ddd0-aed23afca745"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting firstapp.py\n"
          ]
        }
      ],
      "source": [
        "# ghi nội dung vào file\n",
        "# mới đầu đừng quan tâm nó code gì, chỉ biết nó chạy là được, biết thì càng tốt :V\n",
        "%%writefile firstapp.py\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"First Spark Application\") \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    data = [(\"Alice\", 1), (\"Bob\", 2)]\n",
        "    df = spark.createDataFrame(data, [\"Name\", \"Value\"])\n",
        "    df.show()\n",
        "\n",
        "    spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJVBudvWZe7K",
        "outputId": "4163b41c-03bd-457d-e243-82c413782928"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "from pyspark.sql import SparkSession\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    spark = SparkSession.builder \\\n",
            "        .appName(\"First Spark Application\") \\\n",
            "        .master(\"local[*]\") \\\n",
            "        .getOrCreate()\n",
            "\n",
            "    data = [(\"Alice\", 1), (\"Bob\", 2)]\n",
            "    df = spark.createDataFrame(data, [\"Name\", \"Value\"])\n",
            "    df.show()\n",
            "\n",
            "    spark.stop()\n"
          ]
        }
      ],
      "source": [
        "# kiểm tra nội dung đã ghi vào\n",
        "! cat firstapp.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5qPDPiRaLdF",
        "outputId": "d06f7f83-e53a-4d81-b60d-402dc29be1c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25/02/26 09:28:11 INFO SparkContext: Running Spark version 3.5.4\n",
            "25/02/26 09:28:11 INFO SparkContext: OS info Linux, 6.1.85+, amd64\n",
            "25/02/26 09:28:11 INFO SparkContext: Java version 1.8.0_442\n",
            "25/02/26 09:28:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/02/26 09:28:11 INFO ResourceUtils: ==============================================================\n",
            "25/02/26 09:28:11 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/02/26 09:28:11 INFO ResourceUtils: ==============================================================\n",
            "25/02/26 09:28:11 INFO SparkContext: Submitted application: First Spark Application\n",
            "25/02/26 09:28:11 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/02/26 09:28:11 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/02/26 09:28:11 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/02/26 09:28:12 INFO SecurityManager: Changing view acls to: root\n",
            "25/02/26 09:28:12 INFO SecurityManager: Changing modify acls to: root\n",
            "25/02/26 09:28:12 INFO SecurityManager: Changing view acls groups to: \n",
            "25/02/26 09:28:12 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/02/26 09:28:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/02/26 09:28:13 INFO Utils: Successfully started service 'sparkDriver' on port 36777.\n",
            "25/02/26 09:28:13 INFO SparkEnv: Registering MapOutputTracker\n",
            "25/02/26 09:28:13 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/02/26 09:28:13 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/02/26 09:28:13 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/02/26 09:28:13 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/02/26 09:28:13 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8291414f-a0c9-4419-90ae-38b691fb1d3f\n",
            "25/02/26 09:28:13 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "25/02/26 09:28:13 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/02/26 09:28:14 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/02/26 09:28:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "25/02/26 09:28:14 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "25/02/26 09:28:14 INFO Executor: Starting executor ID driver on host 2ad909ab0e90\n",
            "25/02/26 09:28:14 INFO Executor: OS info Linux, 6.1.85+, amd64\n",
            "25/02/26 09:28:14 INFO Executor: Java version 1.8.0_442\n",
            "25/02/26 09:28:14 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/02/26 09:28:14 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@792e2fe4 for default.\n",
            "25/02/26 09:28:14 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39763.\n",
            "25/02/26 09:28:14 INFO NettyBlockTransferService: Server created on 2ad909ab0e90:39763\n",
            "25/02/26 09:28:14 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/02/26 09:28:14 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 2ad909ab0e90, 39763, None)\n",
            "25/02/26 09:28:14 INFO BlockManagerMasterEndpoint: Registering block manager 2ad909ab0e90:39763 with 366.3 MiB RAM, BlockManagerId(driver, 2ad909ab0e90, 39763, None)\n",
            "25/02/26 09:28:14 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 2ad909ab0e90, 39763, None)\n",
            "25/02/26 09:28:14 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 2ad909ab0e90, 39763, None)\n",
            "25/02/26 09:28:16 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/02/26 09:28:16 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/02/26 09:28:20 INFO CodeGenerator: Code generated in 502.998729 ms\n",
            "25/02/26 09:28:21 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
            "25/02/26 09:28:21 INFO DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "25/02/26 09:28:21 INFO DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)\n",
            "25/02/26 09:28:21 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/02/26 09:28:21 INFO DAGScheduler: Missing parents: List()\n",
            "25/02/26 09:28:21 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "25/02/26 09:28:21 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 12.7 KiB, free 366.3 MiB)\n",
            "25/02/26 09:28:21 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 366.3 MiB)\n",
            "25/02/26 09:28:21 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 2ad909ab0e90:39763 (size: 6.6 KiB, free: 366.3 MiB)\n",
            "25/02/26 09:28:21 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585\n",
            "25/02/26 09:28:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "25/02/26 09:28:21 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/02/26 09:28:21 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (2ad909ab0e90, executor driver, partition 0, PROCESS_LOCAL, 9016 bytes) \n",
            "25/02/26 09:28:21 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/02/26 09:28:23 INFO CodeGenerator: Code generated in 44.983168 ms\n",
            "25/02/26 09:28:23 INFO PythonRunner: Times: total = 990, boot = 811, init = 179, finish = 0\n",
            "25/02/26 09:28:23 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1971 bytes result sent to driver\n",
            "25/02/26 09:28:23 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2219 ms on 2ad909ab0e90 (executor driver) (1/1)\n",
            "25/02/26 09:28:23 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/02/26 09:28:23 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 60639\n",
            "25/02/26 09:28:23 INFO DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 2.556 s\n",
            "25/02/26 09:28:23 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/02/26 09:28:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/02/26 09:28:23 INFO DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 2.650840 s\n",
            "25/02/26 09:28:23 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
            "25/02/26 09:28:23 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "25/02/26 09:28:23 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)\n",
            "25/02/26 09:28:23 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/02/26 09:28:23 INFO DAGScheduler: Missing parents: List()\n",
            "25/02/26 09:28:23 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "25/02/26 09:28:23 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.7 KiB, free 366.3 MiB)\n",
            "25/02/26 09:28:23 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 366.3 MiB)\n",
            "25/02/26 09:28:23 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 2ad909ab0e90:39763 (size: 6.6 KiB, free: 366.3 MiB)\n",
            "25/02/26 09:28:23 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
            "25/02/26 09:28:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1))\n",
            "25/02/26 09:28:23 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/02/26 09:28:23 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (2ad909ab0e90, executor driver, partition 1, PROCESS_LOCAL, 9014 bytes) \n",
            "25/02/26 09:28:23 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/02/26 09:28:23 INFO PythonRunner: Times: total = 181, boot = -139, init = 320, finish = 0\n",
            "25/02/26 09:28:23 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1883 bytes result sent to driver\n",
            "25/02/26 09:28:23 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 219 ms on 2ad909ab0e90 (executor driver) (1/1)\n",
            "25/02/26 09:28:23 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.240 s\n",
            "25/02/26 09:28:23 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/02/26 09:28:23 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/02/26 09:28:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/02/26 09:28:23 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.251319 s\n",
            "25/02/26 09:28:26 INFO CodeGenerator: Code generated in 59.110357 ms\n",
            "+-----+-----+\n",
            "| Name|Value|\n",
            "+-----+-----+\n",
            "|Alice|    1|\n",
            "|  Bob|    2|\n",
            "+-----+-----+\n",
            "\n",
            "25/02/26 09:28:26 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/02/26 09:28:26 INFO SparkUI: Stopped Spark web UI at http://2ad909ab0e90:4041\n",
            "25/02/26 09:28:26 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/02/26 09:28:26 INFO MemoryStore: MemoryStore cleared\n",
            "25/02/26 09:28:26 INFO BlockManager: BlockManager stopped\n",
            "25/02/26 09:28:26 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/02/26 09:28:26 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/02/26 09:28:26 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/02/26 09:28:27 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/02/26 09:28:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-84a5e045-f5d8-4da4-b0e7-8bb846892fda/pyspark-29d40d6b-7090-43be-84f1-a45363b6c44b\n",
            "25/02/26 09:28:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-f079293f-8a21-475a-b2ca-0ddf09b04455\n",
            "25/02/26 09:28:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-84a5e045-f5d8-4da4-b0e7-8bb846892fda\n"
          ]
        }
      ],
      "source": [
        "# chạy file này\n",
        "! $SPARK_HOME/bin/spark-submit firstapp.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c4nXT46ah8F"
      },
      "source": [
        "Đã có một bảng với 2 cột (Name, Value) cùng với dòng là (Alice, 1) và (Bob, 2). Vậy là ứng dụng đã chạy thành công !\n",
        "\n",
        "Nếu chạy file .py theo cách bình thường thì mọi tài nguyên và tính toán sẽ được thực hiện trên local machine (trong trường hợp này là colab), nhưng lại bị giới hạn về khả năng xử lý phân tán.\n",
        "\n",
        "Lệnh `spark-submit` cho phép ta gỡ bỏ điều đó bằng cách \"submit\"  file .py của bạn đến cluster và  phân phối công việc cho các node trong cluster đó.\n",
        "\n",
        "Giờ thì ta sẽ đi sâu hơn vào từng phần của mã nguồn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8Jo6rOf2-YH"
      },
      "source": [
        "## SparkSession 🔥"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6VlbveA2-YH"
      },
      "source": [
        "Apache Spark là một hệ thống phân tán, chạy trên nhiều node. Để tương tác với spark, chúng ta cần một thứ gọi là \"điểm khởi đầu\" (Entry point). Trong trường hợp này chính là SparkSession. SparkSession giống như cửa chính của một tòa nhà, giúp bạn truy cập vào các thành phần bên trong như SparkContext, StreamContext, SQLContext,... (mấy cái này bàn sau)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64OXTUScUG-i"
      },
      "source": [
        "![](https://abhishekbaranwal10.files.wordpress.com/2018/09/introduction-to-apache-spark-20-12-638.jpg?resize=638%2C479&is-pending-load=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YmADrJF2-YH"
      },
      "outputs": [],
      "source": [
        "# khởi tạo SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Datacamp Pyspark Tutorial\") \\\n",
        "    .config(\"spark.memory.offHeap.enabled\",\"true\") \\\n",
        "    .config(\"spark.memory.offHeap.size\",\"5g\").getOrCreate()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP4IdQzRCM60"
      },
      "source": [
        "Dừng SparkSession khi đã hoàn thành tất cả các tác vụ xử lý dữ liệu, nhằm giải phóng tài nguyên (như bộ nhớ, CPU và kết nối đến cluster)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbkhNJQ52-YH"
      },
      "outputs": [],
      "source": [
        "# spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tnpHHwMAGS4"
      },
      "source": [
        "## SparkConf 🔥"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-inNEMrWAMzn"
      },
      "source": [
        "SparkConf là một class đặc biệt giúp chúng ta thay đổi lại cách khởi tạo session\n",
        "\n",
        "Ở phần trên, chúng ta khởi tạo một session thông qua một builder có sẵn và nó khá đơn giản. Nhưng nếu muốn tùy chỉnh cấu hình chi tiết (như bộ nhớ, số lượng core, v.v.) thì ta có cách sau:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0x72-FNBUE5",
        "outputId": "ab259ba9-547c-4097-9e1f-64e9682ebda3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "some-value\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkConf\n",
        "\n",
        "\n",
        "# Tạo SparkConf với các cấu hình cần thiết\n",
        "conf = SparkConf() \\\n",
        "    .setAppName(\"MyApp\") \\\n",
        "    .setMaster(\"local[*]\") \\\n",
        "    .set(\"spark.some.config.option\", \"some-value\")\n",
        "\n",
        "# Tạo SparkSession bằng cách truyền SparkConf\n",
        "spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .config(conf=conf) \\\n",
        "        .getOrCreate()\n",
        "\n",
        "# Kiểm tra cấu hình từ SparkSession\n",
        "print(spark.conf.get(\"spark.some.config.option\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZistYlEcBieI"
      },
      "source": [
        "Đối với đa số ứng dụng, việc sử dụng trực tiếp builder của SparkSession đã đủ nên có thể bỏ qua bước này."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VvnNMdM2-YH"
      },
      "source": [
        "## SparkContext 🔥"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAuKglvr2-YH"
      },
      "source": [
        "🔹SparkContext là class cốt lõi trong Apache Spark, giúp ứng dụng kết nối với cluster manager - trình quản lý các tác vụ tính toán phân tán.\n",
        "\n",
        "🔹SparkContext được sử dụng để khởi tạo các cấu trúc dữ liệu đặc biệt như RDD, accumulator, và broadcast variable (cái này sẽ nói sau)\n",
        "\n",
        "🔹Trước Spark phiên bản 2.x, SparkContext là entry point chính.\n",
        "\n",
        "🔹Từ Spark 2.x trở đi, SparkSession thay thế SparkContext, nhưng SparkContext vẫn tồn tại bên trong SparkSession.\n",
        "\n",
        "Để hiểu sâu hơn, hãy nhìn sơ đồ bên dưới:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ido4zEM78zm"
      },
      "source": [
        "![](https://sparkbyexamples.com/wp-content/uploads/2022/05/image04.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PFbmL-hbF4h"
      },
      "source": [
        "Như đã nói, khi chạy một Spark app, chương trình sẽ được \"submit\" tới cluster và được đưa đến các node đang có dữ liệu. Trong đó, một node sẽ được chọn làm **Master node** để chạy một chương trình gọi là **Driver Program**, các node còn lại sẽ đóng vai trò là **Worker**.\n",
        "\n",
        "Tại **Driver Program**, một đối tượng **SparkContext** sẽ được khởi tạo giúp ứng dụng Spark giao tiếp với **Cluster Manager** để yêu cầu tài nguyên. Sau khi tài nguyên được phân bổ cho các node, **Cluster Manager** sẽ khởi động các **Executor**.\n",
        "\n",
        "**Executor** là những tiến trình chạy trên các **Worker** và chúng sẽ xử lý các **Task** được giao bởi **Driver Program**.\n",
        "\n",
        "**Driver Program** sẽ tạo task và phân chia cho các **Worker** theo nguyên tắc xử lý cục bộ, tức là tài nguyên trên node nào sẽ được xử lý bởi **Executor** trên node đó."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2vZTRc4fcMq"
      },
      "source": [
        "Nói tới đây thì có lẽ chúng ta đã định hình được vai trò của SparkContext rồi. Hãy nhớ, bất cứ khi nào làm việc với Spark, luôn khởi tạo SparkContext sau khi đã có SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iS3Alc0L2-YH",
        "outputId": "cce42a49-ff69-4485-ae83-673ec408921b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SparkContext đã tồn tại, chỉ khởi tạo một lần. Đọc thêm ở bên dưới để biết thêm chi tiết\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Khởi tạo sparkContext\n",
        "\n",
        "try:\n",
        "  sc = SparkContext(\"local\", \"MyApp\")\n",
        "  print(sc)\n",
        "except:\n",
        "  print(\"SparkContext đã tồn tại, chỉ khởi tạo một lần. Đọc thêm ở bên dưới để biết thêm chi tiết\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "4gq70-rV2-YI",
        "outputId": "1eea7ba4-8c78-4462-dcb7-e6fbfed761dc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://2ad909ab0e90:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.4</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Datacamp Pyspark Tutorial</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ],
            "text/plain": [
              "<SparkContext master=local[*] appName=Datacamp Pyspark Tutorial>"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# kiểm tra có SparkContext nào đang chạy không\n",
        "SparkContext._active_spark_context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS9Ns5_l2-YI"
      },
      "source": [
        "🔹Lưu ý: sparkContext chỉ khởi tạo một lần\n",
        "\n",
        "🔹Chạy lần 2 bị báo lỗi `Cannot run multiple SparkContexts at once; existing SparkContext`\n",
        "\n",
        "🔹Có thể khắc phục bằng 2 cách:\n",
        "    🔸Dừng sparkContext,\n",
        "    🔸Lấy sparkContext hiện có"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUeyajYs2-YI",
        "outputId": "811175d0-3722-4dc9-a6e1-17ab489d2ef5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lưu ý: Không gọi sc.stop() nếu biến sc chưa tồn tại. Lúc đó phải chạy lệnh bên dưới\n"
          ]
        }
      ],
      "source": [
        "# Dừng SparkContext\n",
        "\n",
        "try:\n",
        "  # sc.stop()\n",
        "  # sc = SparkContext(\"local\", \"MyApp\")\n",
        "  print(sc)\n",
        "except:\n",
        "  print('Lưu ý: Không gọi sc.stop() nếu biến sc chưa tồn tại. Lúc đó phải chạy lệnh bên dưới')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Y93vaZH2-YI",
        "outputId": "ceadcb04-c400-4d97-e2e4-6dd7c287bdf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<SparkContext master=local[*] appName=Datacamp Pyspark Tutorial>\n"
          ]
        }
      ],
      "source": [
        "# Lấy SparkContext hiện có từ SparkSession\n",
        "\n",
        "try:\n",
        "  sc = spark.sparkContext\n",
        "  print(sc)\n",
        "except:\n",
        "  print('Chưa import spark phải không ? nhớ import nha :v')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rFZdy_I2-YI"
      },
      "source": [
        "## RDD 🔥\n",
        "\n",
        "RDD (Resilient Distributed Dataset) là cấu trúc dữ liệu cốt lõi của Apache Spark, cho phép xử lý dữ liệu phân tán trên các cluster một cách linh hoạt và hiệu quả."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OdUmVugmOk3"
      },
      "source": [
        "![](https://images.viblo.asia/2cd88166-3c16-4cdc-9298-ce9900ac1288.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmpiYtGcmU6z"
      },
      "source": [
        "🔴 **Các tính chất của RDD:**\n",
        "\n",
        "- Tính Phân tán (Distributed): Mỗi RDD được chia thành các phần nhỏ gọi là partitions, mỗi partition có thể được xử lý độc lập trên các node khác nhau trong 1 cluster\n",
        "\n",
        "- Tính Linh hoạt (Resilient): RDD có thể tự động phục hồi sau khi một phần của dữ liệu hoặc một phần của cluster bị lỗi.\n",
        "\n",
        "- Tính Bất biến : Sau khi được tạo, một RDD không thể thay đổi.\n",
        "\n",
        "- Đánh giá lười (Lazy Evaluation): Các phép biến đổi trên RDD không được thực hiện ngay lập tức mà chỉ khi có hành động (action) được gọi.\n",
        "\n",
        "- Tính tối ưu hóa (Optimized): RDDs có thể tối ưu hóa để tận dụng các hoạt động in-memory, giảm thiểu việc truy cập dữ liệu từ đĩa và tối ưu hóa việc chuyển dữ liệu giữa các phần của RDD trên cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzOKzzog2-YI",
        "outputId": "f6195d12-6a51-43f2-9377-bc38ffb5b6a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Scalar', 'java', 'hadoop', 'spark', 'akka', 'spark and hadoop', 'pyspark', 'pyspark and spark']\n",
            "<class 'list'>\n"
          ]
        }
      ],
      "source": [
        "words = [\n",
        "    'Scalar',\n",
        "    'java',\n",
        "    'hadoop',\n",
        "    'spark',\n",
        "    'akka',\n",
        "    'spark and hadoop',\n",
        "    'pyspark',\n",
        "    'pyspark and spark'\n",
        "]\n",
        "\n",
        "print(words)\n",
        "print(type(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ym4CpQyl2-YI",
        "outputId": "bb53d409-dc49-4aca-d40e-369b97e1200f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pyspark.rdd.RDD'>\n"
          ]
        }
      ],
      "source": [
        "rdd_words = sc.parallelize(words)\n",
        "print(type(rdd_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkV-sspxOyn7"
      },
      "source": [
        "**🔴 Các hành động (Actions) trên RDD**\n",
        "\n",
        "Các hành động thực thi tính toán trên RDD và trả về kết quả.\n",
        "Một số hành động phổ biến:\n",
        "\n",
        "- collect(): Lấy tất cả phần tử từ RDD về driver.\n",
        "- count(): Đếm số phần tử trong RDD.\n",
        "- first(): Lấy phần tử đầu tiên.\n",
        "- take(n): Lấy n phần tử đầu tiên.\n",
        "- reduce(f): Gộp các phần tử với một hàm f."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JEYDPTVkkEr"
      },
      "source": [
        "**1️⃣ collect()**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjsBmaJpk-S8"
      },
      "source": [
        "Thu thập toàn bộ dữ liệu từ các phân vùng của RDD trên các worker node và đưa về driver dưới dạng một danh sách Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S43jRCL5jtNG",
        "outputId": "1d47b779-9b06-4294-a66f-f165a2263219"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Scalar', 'java', 'hadoop', 'spark', 'akka', 'spark and hadoop', 'pyspark', 'pyspark and spark']\n",
            "<class 'list'>\n"
          ]
        }
      ],
      "source": [
        "a = rdd_words.collect()\n",
        "print(a)\n",
        "print(type(a))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2O2WgUQlt7J"
      },
      "source": [
        "Khi gọi collect(), Spark sẽ thực hiện toàn bộ các phép biến đổi (transformation) đã được định nghĩa trên RDD để tạo ra dữ liệu cuối cùng.\n",
        "\n",
        "Lưu ý rằng nó có thể dễ dàng gây ra vấn đề bộ nhớ nếu dữ liệu quá lớn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR3p-dSxoits"
      },
      "source": [
        "**2️⃣count()**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUIn-tAOlrwq"
      },
      "source": [
        "Đếm số lượng phần tử có trong RDD và trả về kết quả dưới dạng một số nguyên (integer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsl6MInM2-YI",
        "outputId": "a5d9316b-08ae-4202-8e0c-7d81ac29c7f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8\n"
          ]
        }
      ],
      "source": [
        "print(rdd_words.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8veLylko0Ex"
      },
      "source": [
        "Khi gọi count(), tất cả các phép biến đổi (transformation) đã được định nghĩa trên RDD sẽ được thực hiện để tạo ra dữ liệu cuối cùng trước khi đếm số lượng phần tử.\n",
        "\n",
        "Không giống như collect, count chỉ trả về số đếm, do đó nó không gây ra quá tải bộ nhớ trên driver ngay cả khi RDD chứa rất nhiều phần tử."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eY_0nm72ZTZ"
      },
      "source": [
        "**3️⃣first()**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nN_kEudH4O6-"
      },
      "source": [
        "Chỉ lấy về 1 phần tử mà không cần chuyển toàn bộ dữ liệu về driver, giúp tránh quá tải bộ nhớ.\n",
        "\n",
        "Nếu RDD rỗng, phương thức này sẽ ném ra lỗi (exception)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "iInYad2pOyUf",
        "outputId": "d05041db-9c8b-4ce8-e289-3d63ffc87a9b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Scalar'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd_words.first()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT-zKWbUhkmr"
      },
      "source": [
        "**4️⃣take()**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZLwx1TUmeIh"
      },
      "source": [
        "`take(n)` trả về một danh sách chứa n phần tử đầu tiên của RDD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzo1mKWuhw2r",
        "outputId": "4cafe82c-d0dc-4af5-9daf-840187ee9648"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Scalar', 'java', 'hadoop', 'spark', 'akka']"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd_words.take(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnNgYwUM4LBu"
      },
      "source": [
        "Khi gọi `take(n)`, Spark sẽ duyệt qua các partition của RDD cho đến khi thu thập đủ n phần tử. Nếu dữ liệu trong RDD ít hơn n, nó sẽ trả về tất cả các phần tử có sẵn."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**🔴Các phép biến đổi (Transformations) trên RDD**\n",
        "\n",
        "Các phép biến đổi trên RDD tạo ra RDD mới mà không thay đổi RDD ban đầu.\n",
        "\n",
        "Một số phép biến đổi phổ biến:\n",
        "\n",
        "- map(f): Áp dụng một hàm f lên từng phần tử.\n",
        "- filter(f): Lọc các phần tử thỏa mãn điều kiện f.\n",
        "- flatMap(f): Giống map(), nhưng mở rộng kết quả thành nhiều phần tử.\n",
        "- groupByKey(): Nhóm các phần tử có cùng khóa (key-value RDD).\n",
        "- reduceByKey(f): Kết hợp các giá trị có cùng khóa bằng một hàm f."
      ],
      "metadata": {
        "id": "q_DgNcZ5oAQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**filter()**"
      ],
      "metadata": {
        "id": "fmUuvbkOoTVy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "filter nhận vào một hàm điều kiện (predicate) và áp dụng hàm đó cho từng phần tử của RDD.\n",
        "\n",
        "Chỉ những phần tử mà hàm trả về True sẽ được giữ lại trong RDD kết quả.\n",
        "\n",
        "RDD ban đầu không bị thay đổi. Thay vào đó, filter tạo ra một RDD mới chứa các phần tử đã được lọc."
      ],
      "metadata": {
        "id": "0Wji1RB6oR3L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "H8LLtHhXmg4N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "84ec774e-97ef-4f8c-b21b-79cfc0a4dee6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'sc' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9bcac31d30d1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Tạo RDD từ một danh sách các số từ 1 đến 10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnumbers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Sử dụng filter để chỉ lấy ra các số chẵn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0meven_numbers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
          ]
        }
      ],
      "source": [
        "# Tạo RDD từ một danh sách các số từ 1 đến 10\n",
        "numbers = sc.parallelize(range(1, 11))\n",
        "\n",
        "# Sử dụng filter để chỉ lấy ra các số chẵn\n",
        "even_numbers = numbers.filter(lambda x: x % 2 == 0)\n",
        "\n",
        "# Thu thập kết quả và in ra\n",
        "print(even_numbers.collect())  # Kết quả: [2, 4, 6, 8, 10]\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py3_9",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}