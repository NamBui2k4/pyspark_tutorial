{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKhANDPI2-YD"
      },
      "source": [
        "# Pyspark tutorial üéØ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y9ghrep9ckQ"
      },
      "source": [
        "**Ghi ch√∫:**\n",
        "\n",
        " tutorial n√†y ƒë∆∞·ª£c th·ª±c hi·ªán tr√™n google colab n√™n kh√¥ng b·ªã l·ªói khi ch·∫°y code. N·∫øu ch·∫°y tr√™n jupyter notebook ƒë∆∞·ª£c host b·ªüi local machine ch·∫Øc ch·∫Øn s·∫Ω l·ªói (ch·∫Øc v√¨ l√Ω do c·∫•u h√¨nh c√†i ƒë·∫∑t hay g√¨ ƒë√≥ :v). M·∫∑c d√π ƒë√£ m√≤ m·∫´m c·∫£ tu·∫ßn nh∆∞ng m√¨nh v·∫´n ch∆∞a c√≥ gi·∫£i ph√°p n√†o cho v·∫•n ƒë·ªÅ n√†y n√™n m√¨nh ch·∫•p nh·∫≠n th·ª±c hi·ªán n√≥ tr√™n google colab. ü§∑‚Äç‚ôÇÔ∏è\n",
        "\n",
        " üîπTh√™m n·ªØa, tutorial n√†y ƒë∆∞·ª£c x√¢y d·ª±ng d·ª±a tr√™n nhi·ªÅu ngu·ªìn, trong ƒë√≥ c√≥ c√°c blog m√† m√¨nh kham kh·∫£o. Sau ƒë√¢y xin c·∫£m ∆°n blog c·ªßa [longcnttbkhn](https://longcnttbkhn.github.io/huong-dan-spark-co-ban-cho-nguoi-moi/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXX24zUvUB_y"
      },
      "source": [
        "\n",
        "![](https://th.bing.com/th/id/OIP.I3eg_GSGbjpQ0O8GDuHVdgHaFL?rs=1&pid=ImgDetMain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-Cj9XV-2-YF"
      },
      "source": [
        "## Installation ‚¨áÔ∏è"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GFFs2FY2-YF"
      },
      "source": [
        "ƒê·ªÉ l√†m vi·ªác v·ªõi spark, ch√∫ng ta c·∫ßn c√†i ƒë·∫∑t c√°c th√†nh ph·∫ßn sau:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "477uUkwe2-YF"
      },
      "source": [
        "- üìå Java (OpenJDK 8 ho·∫∑c 11):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6iqBQ9FIG4Ed"
      },
      "outputs": [],
      "source": [
        "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvamacLl2-YF"
      },
      "source": [
        "- üìå Apache Spark\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-rjp-b0FGrrA"
      },
      "outputs": [],
      "source": [
        "# # t·∫£i g√≥i spark apache v·ªÅ\n",
        "# !wget https://downloads.apache.org/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz\n",
        "\n",
        "# # gi·∫£i n√©n\n",
        "# !tar -xvzf /content/spark-3.5.4-bin-hadoop3.tgz\n",
        "\n",
        "# # di chuy·ªÉn Spark v√†o th∆∞ m·ª•c /opt/spark (ƒë√¢y l√† quy ∆∞·ªõc, t·ª± t√¨m hi·ªÉu th√™m)\n",
        "# !sudo mv /content/spark-3.5.4-bin-hadoop3 /opt/spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQP7D00oGyR6"
      },
      "source": [
        "üîß Thi·∫øt l·∫≠p bi·∫øn m√¥i tr∆∞·ªùng nh∆∞ sau:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RR3VHbS8Glrt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"C:\\\\Program Files\\\\Java\\\\jdk-11\"\n",
        "os.environ[\"SPARK_HOME\"] = \"C:\\spark\\spark-3.5.4-bin-hadoop3\"\n",
        "os.environ[\"PYSPARK_PYTHON\"] = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-D7dd0y2-YG"
      },
      "source": [
        "- üìå pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHxirntQ2-YG",
        "outputId": "af1e32fb-1727-40f7-a89e-213e80e54a24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in c:\\users\\nam\\anaconda3\\envs\\py3_9\\lib\\site-packages (3.5.4)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\nam\\anaconda3\\envs\\py3_9\\lib\\site-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i6VQ3wE2-YG"
      },
      "source": [
        "- üìå findspark - t√πy ch·ªçn, n·∫øu ch·∫°y tr√™n Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVLN3of82-YH",
        "outputId": "942e1662-b5cf-452f-c9be-5ebf89d3aea8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: findspark in c:\\users\\nam\\anaconda3\\envs\\py3_9\\lib\\site-packages (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install findspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "  <details><summary>üåä Kham kh·∫£o th√™m c√°ch c√†i ƒë·∫∑t tr√™n local machine: </summary><br>\n",
        "\n",
        "  <details><summary> üîπ V·ªõi ubuntu: </summary>\n",
        "    \n",
        "  Ghi ch√∫: ·ªû ƒë√¢y m√¨nh x√†i python 3.9 c·ªßa anaconda n√™n ƒë∆∞·ªùng d·∫´n c√†i ƒë·∫∑t s·∫Ω kh√°c so v·ªõi python t·∫£i t·ª´ web truy·ªÅn th·ªëng (l∆∞u √Ω n√†y d√†nh cho m·∫•y b·∫°n m·ªõi ƒë·ª•ng t·ªõi python üòÖ) .\n",
        "  \n",
        "  T√™n environment c·ªßa conda s·∫Ω l√† `py3_9`, c√≤n t√™n user trong hdh ubuntu th√¨ s·∫Ω ƒë·∫∑t l√† `nam`. <br>\n",
        "\n",
        "  Ch·∫°y l·ªánh sau:\n",
        "\n",
        "    üëâ sudo apt update\n",
        " \n",
        "    üëâ sudo apt install openjdk-11-jdk -y\n",
        "\n",
        "    üëâ java -version # ki·ªÉm tra th·ª≠ sau khi c√†i\n",
        "\n",
        "  C√†i bi·∫øn m√¥i tr∆∞·ªùng:\n",
        "\n",
        "    üëâ echo 'export SPARK_HOME=/opt/spark' >> ~/.bashrc\n",
        "\n",
        "    üëâ echo 'export PATH=$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH' >> ~/.bashrc\n",
        "\n",
        "    üëâ echo 'export PYSPARK_PYTHON=/home/nam/anaconda3/envs/py3_9/bin/python'\n",
        "    >> ~/.bashrc\n",
        "\n",
        "    üëâ source ~/.bashrc\n",
        "\n",
        "  <br>\n",
        "\n",
        "  <details><summary>V·ªõi window: </summary>\n",
        "  \n",
        "  ƒê·ªëi v·ªõi window th√¨ h∆°i th·ªß c√¥ng x√≠u, v√† v√¨ m√¨nh v·∫´n s·∫Ω host b·∫±ng jupyter notebook n√™n\n",
        "\n",
        "\n",
        "  üëâ T·∫£i JDK 11 t·ª´ [Oracle](https://www.oracle.com/java/technologies/javase/jdk11-archive-downloads.html) ho·∫∑c d√πng OpenJDK t·ª´ Adoptium.\n",
        "\n",
        "  </details>\n",
        "  </details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zwpg9kB1X0nv"
      },
      "source": [
        "## Run Spark app üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IfPtbeXrZXHk"
      },
      "outputs": [],
      "source": [
        "# kh·ªüi t·∫°o m·ªôt file .py\n",
        "# ! touch firstapp.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmXe6eUfZxUq",
        "outputId": "429b848e-0ce7-4adb-ddd0-aed23afca745"
      },
      "outputs": [],
      "source": [
        "# # ghi n·ªôi dung v√†o file\n",
        "# # m·ªõi ƒë·∫ßu ƒë·ª´ng quan t√¢m n√≥ code g√¨, ch·ªâ bi·∫øt n√≥ ch·∫°y l√† ƒë∆∞·ª£c, bi·∫øt th√¨ c√†ng t·ªët :V\n",
        "\n",
        "# %%writefile firstapp.py\n",
        "\n",
        "# from pyspark.sql import SparkSession\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     spark = SparkSession.builder \\\n",
        "#         .appName(\"First Spark Application\") \\\n",
        "#         .master(\"local[*]\") \\\n",
        "#         .getOrCreate()\n",
        "\n",
        "#     data = [(\"Alice\", 1), (\"Bob\", 2)]\n",
        "#     df = spark.createDataFrame(data, [\"Name\", \"Value\"])\n",
        "#     df.show()\n",
        "\n",
        "#     spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJVBudvWZe7K",
        "outputId": "4163b41c-03bd-457d-e243-82c413782928"
      },
      "outputs": [],
      "source": [
        "# ki·ªÉm tra n·ªôi dung ƒë√£ ghi v√†o\n",
        "# ! cat firstapp.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5qPDPiRaLdF",
        "outputId": "d06f7f83-e53a-4d81-b60d-402dc29be1c1"
      },
      "outputs": [],
      "source": [
        "# ch·∫°y file n√†y\n",
        "# ! $SPARK_HOME/bin/spark-submit firstapp.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c4nXT46ah8F"
      },
      "source": [
        "ƒê√£ c√≥ m·ªôt b·∫£ng v·ªõi 2 c·ªôt (Name, Value) c√πng v·ªõi d√≤ng l√† (Alice, 1) v√† (Bob, 2). V·∫≠y l√† ·ª©ng d·ª•ng ƒë√£ ch·∫°y th√†nh c√¥ng !\n",
        "\n",
        "N·∫øu ch·∫°y file .py theo c√°ch b√¨nh th∆∞·ªùng th√¨ m·ªçi t√†i nguy√™n v√† t√≠nh to√°n s·∫Ω ƒë∆∞·ª£c th·ª±c hi·ªán tr√™n local machine (trong tr∆∞·ªùng h·ª£p n√†y l√† colab), nh∆∞ng l·∫°i b·ªã gi·ªõi h·∫°n v·ªÅ kh·∫£ nƒÉng x·ª≠ l√Ω ph√¢n t√°n.\n",
        "\n",
        "L·ªánh `spark-submit` cho ph√©p ta g·ª° b·ªè ƒëi·ªÅu ƒë√≥ b·∫±ng c√°ch \"submit\"  file .py c·ªßa b·∫°n ƒë·∫øn cluster v√†  ph√¢n ph·ªëi c√¥ng vi·ªác cho c√°c node trong cluster ƒë√≥.\n",
        "\n",
        "Gi·ªù th√¨ ta s·∫Ω ƒëi s√¢u h∆°n v√†o t·ª´ng ph·∫ßn c·ªßa m√£ ngu·ªìn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8Jo6rOf2-YH"
      },
      "source": [
        "## SparkSession üî•"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6VlbveA2-YH"
      },
      "source": [
        "Apache Spark l√† m·ªôt h·ªá th·ªëng ph√¢n t√°n, ch·∫°y tr√™n nhi·ªÅu node. ƒê·ªÉ t∆∞∆°ng t√°c v·ªõi spark, ch√∫ng ta c·∫ßn m·ªôt th·ª© g·ªçi l√† \"ƒëi·ªÉm kh·ªüi ƒë·∫ßu\" (Entry point). Trong tr∆∞·ªùng h·ª£p n√†y ch√≠nh l√† SparkSession. SparkSession gi·ªëng nh∆∞ c·ª≠a ch√≠nh c·ªßa m·ªôt t√≤a nh√†, gi√∫p b·∫°n truy c·∫≠p v√†o c√°c th√†nh ph·∫ßn b√™n trong nh∆∞ SparkContext, StreamContext, SQLContext,... (m·∫•y c√°i n√†y b√†n sau)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64OXTUScUG-i"
      },
      "source": [
        "![](https://abhishekbaranwal10.files.wordpress.com/2018/09/introduction-to-apache-spark-20-12-638.jpg?resize=638%2C479&is-pending-load=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7YmADrJF2-YH"
      },
      "outputs": [],
      "source": [
        "# kh·ªüi t·∫°o SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Datacamp Pyspark Tutorial\") \\\n",
        "    .config(\"spark.memory.offHeap.enabled\",\"true\") \\\n",
        "    .config(\"spark.memory.offHeap.size\",\"5g\").getOrCreate()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP4IdQzRCM60"
      },
      "source": [
        "D·ª´ng SparkSession khi ƒë√£ ho√†n th√†nh t·∫•t c·∫£ c√°c t√°c v·ª• x·ª≠ l√Ω d·ªØ li·ªáu, nh·∫±m gi·∫£i ph√≥ng t√†i nguy√™n (nh∆∞ b·ªô nh·ªõ, CPU v√† k·∫øt n·ªëi ƒë·∫øn cluster)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "CbkhNJQ52-YH"
      },
      "outputs": [],
      "source": [
        "# spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tnpHHwMAGS4"
      },
      "source": [
        "## SparkConf üî•"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-inNEMrWAMzn"
      },
      "source": [
        "SparkConf l√† m·ªôt class ƒë·∫∑c bi·ªát gi√∫p ch√∫ng ta thay ƒë·ªïi l·∫°i c√°ch kh·ªüi t·∫°o session\n",
        "\n",
        "·ªû ph·∫ßn tr√™n, ch√∫ng ta kh·ªüi t·∫°o m·ªôt session th√¥ng qua m·ªôt builder c√≥ s·∫µn v√† n√≥ kh√° ƒë∆°n gi·∫£n. Nh∆∞ng n·∫øu mu·ªën t√πy ch·ªânh c·∫•u h√¨nh chi ti·∫øt (nh∆∞ b·ªô nh·ªõ, s·ªë l∆∞·ª£ng core, v.v.) th√¨ ta c√≥ c√°ch sau:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0x72-FNBUE5",
        "outputId": "ab259ba9-547c-4097-9e1f-64e9682ebda3"
      },
      "source": [
        "```python\n",
        "    from pyspark import SparkConf\n",
        "\n",
        "    # T·∫°o SparkConf v·ªõi c√°c c·∫•u h√¨nh c·∫ßn thi·∫øt\n",
        "    conf = SparkConf() \\\n",
        "        .setAppName(\"MyApp\") \\\n",
        "        .setMaster(\"local[*]\") \\\n",
        "        .set(\"spark.some.config.option\", \"some-value\")\n",
        "\n",
        "    # T·∫°o SparkSession b·∫±ng c√°ch truy·ªÅn SparkConf\n",
        "    spark = SparkSession \\\n",
        "            .builder \\\n",
        "            .config(conf=conf) \\\n",
        "            .getOrCreate()\n",
        "\n",
        "    # Ki·ªÉm tra c·∫•u h√¨nh t·ª´ SparkSession\n",
        "    print(spark.conf.get(\"spark.some.config.option\"))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZistYlEcBieI"
      },
      "source": [
        "ƒê·ªëi v·ªõi ƒëa s·ªë ·ª©ng d·ª•ng, vi·ªác s·ª≠ d·ª•ng tr·ª±c ti·∫øp builder c·ªßa SparkSession ƒë√£ ƒë·ªß n√™n c√≥ th·ªÉ b·ªè qua b∆∞·ªõc n√†y."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VvnNMdM2-YH"
      },
      "source": [
        "## SparkContext üî•"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAuKglvr2-YH"
      },
      "source": [
        "üîπSparkContext l√† class c·ªët l√µi trong Apache Spark, gi√∫p ·ª©ng d·ª•ng k·∫øt n·ªëi v·ªõi cluster manager - tr√¨nh qu·∫£n l√Ω c√°c t√°c v·ª• t√≠nh to√°n ph√¢n t√°n.\n",
        "\n",
        "üîπSparkContext ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ kh·ªüi t·∫°o c√°c c·∫•u tr√∫c d·ªØ li·ªáu ƒë·∫∑c bi·ªát nh∆∞ RDD, accumulator, v√† broadcast variable (c√°i n√†y s·∫Ω n√≥i sau)\n",
        "\n",
        "üîπTr∆∞·ªõc Spark phi√™n b·∫£n 2.x, SparkContext l√† entry point ch√≠nh.\n",
        "\n",
        "üîπT·ª´ Spark 2.x tr·ªü ƒëi, SparkSession thay th·∫ø SparkContext, nh∆∞ng SparkContext v·∫´n t·ªìn t·∫°i b√™n trong SparkSession.\n",
        "\n",
        "ƒê·ªÉ hi·ªÉu s√¢u h∆°n, h√£y nh√¨n s∆° ƒë·ªì b√™n d∆∞·ªõi:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ido4zEM78zm"
      },
      "source": [
        "![](https://sparkbyexamples.com/wp-content/uploads/2022/05/image04.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PFbmL-hbF4h"
      },
      "source": [
        "Nh∆∞ ƒë√£ n√≥i, khi ch·∫°y m·ªôt Spark app, ch∆∞∆°ng tr√¨nh s·∫Ω ƒë∆∞·ª£c \"submit\" t·ªõi cluster v√† ƒë∆∞·ª£c ƒë∆∞a ƒë·∫øn c√°c node ƒëang c√≥ d·ªØ li·ªáu. Trong ƒë√≥, m·ªôt node s·∫Ω ƒë∆∞·ª£c ch·ªçn l√†m **Master node** ƒë·ªÉ ch·∫°y m·ªôt ch∆∞∆°ng tr√¨nh g·ªçi l√† **Driver Program**, c√°c node c√≤n l·∫°i s·∫Ω ƒë√≥ng vai tr√≤ l√† **Worker**.\n",
        "\n",
        "T·∫°i **Driver Program**, m·ªôt ƒë·ªëi t∆∞·ª£ng **SparkContext** s·∫Ω ƒë∆∞·ª£c kh·ªüi t·∫°o gi√∫p ·ª©ng d·ª•ng Spark giao ti·∫øp v·ªõi **Cluster Manager** ƒë·ªÉ y√™u c·∫ßu t√†i nguy√™n. Sau khi t√†i nguy√™n ƒë∆∞·ª£c ph√¢n b·ªï cho c√°c node, **Cluster Manager** s·∫Ω kh·ªüi ƒë·ªông c√°c **Executor**.\n",
        "\n",
        "**Executor** l√† nh·ªØng ti·∫øn tr√¨nh ch·∫°y tr√™n c√°c **Worker** v√† ch√∫ng s·∫Ω x·ª≠ l√Ω c√°c **Task** ƒë∆∞·ª£c giao b·ªüi **Driver Program**.\n",
        "\n",
        "**Driver Program** s·∫Ω t·∫°o task v√† ph√¢n chia cho c√°c **Worker** theo nguy√™n t·∫Øc x·ª≠ l√Ω c·ª•c b·ªô, t·ª©c l√† t√†i nguy√™n tr√™n node n√†o s·∫Ω ƒë∆∞·ª£c x·ª≠ l√Ω b·ªüi **Executor** tr√™n node ƒë√≥."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2vZTRc4fcMq"
      },
      "source": [
        "N√≥i t·ªõi ƒë√¢y th√¨ c√≥ l·∫Ω ch√∫ng ta ƒë√£ ƒë·ªãnh h√¨nh ƒë∆∞·ª£c vai tr√≤ c·ªßa SparkContext r·ªìi. H√£y nh·ªõ, b·∫•t c·ª© khi n√†o l√†m vi·ªác v·ªõi Spark, lu√¥n kh·ªüi t·∫°o SparkContext sau khi ƒë√£ c√≥ SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Y93vaZH2-YI",
        "outputId": "ceadcb04-c400-4d97-e2e4-6dd7c287bdf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<SparkContext master=local[*] appName=Datacamp Pyspark Tutorial>\n"
          ]
        }
      ],
      "source": [
        "# L·∫•y SparkContext hi·ªán c√≥ t·ª´ SparkSession\n",
        "\n",
        "try:\n",
        "  sc = spark.sparkContext\n",
        "  print(sc)\n",
        "except:\n",
        "  print('Ch∆∞a import spark ph·∫£i kh√¥ng ? nh·ªõ import nha :v')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "4gq70-rV2-YI",
        "outputId": "1eea7ba4-8c78-4462-dcb7-e6fbfed761dc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.4</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Datacamp Pyspark Tutorial</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ],
            "text/plain": [
              "<SparkContext master=local[*] appName=Datacamp Pyspark Tutorial>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ki·ªÉm tra c√≥ SparkContext n√†o ƒëang ch·∫°y kh√¥ng\n",
        "sc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "M·ªôt c√°ch kh√°c ƒë·ªÉ kh·ªüi t·∫°o spark context m√† kh√¥ng c·∫ßn spark session:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iS3Alc0L2-YH",
        "outputId": "cce42a49-ff69-4485-ae83-673ec408921b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SparkContext ƒë√£ t·ªìn t·∫°i, ch·ªâ kh·ªüi t·∫°o m·ªôt l·∫ßn. ƒê·ªçc th√™m ·ªü b√™n d∆∞·ªõi ƒë·ªÉ bi·∫øt th√™m chi ti·∫øt\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Kh·ªüi t·∫°o sparkContext\n",
        "\n",
        "try:\n",
        "  sc = SparkContext(\"local\", \"newAppName\")\n",
        "  print(sc)\n",
        "except:\n",
        "  print(\"SparkContext ƒë√£ t·ªìn t·∫°i, ch·ªâ kh·ªüi t·∫°o m·ªôt l·∫ßn. ƒê·ªçc th√™m ·ªü b√™n d∆∞·ªõi ƒë·ªÉ bi·∫øt th√™m chi ti·∫øt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS9Ns5_l2-YI"
      },
      "source": [
        "üîπL∆∞u √Ω: sparkContext ch·ªâ kh·ªüi t·∫°o m·ªôt l·∫ßn\n",
        "\n",
        "üîπCh·∫°y l·∫ßn 2 b·ªã b√°o l·ªói `Cannot run multiple SparkContexts at once; existing SparkContext`\n",
        "\n",
        "üîπDo ƒë√≥, kh√¥ng c·∫ßn ph·∫£i kh·ªüi t·∫°o l·∫ßn n·ªØa, v√† nh·∫£y sang ph·∫ßn ti·∫øp theo: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "  <summary>ƒê·ªçc th√™m: D·ª´ng context nh∆∞ th·∫ø n√†o ?</summary> \n",
        "\n",
        "N·∫øu mu·ªën d·ª´ng context hi·ªán t·∫°i v√† t·∫°o m·ªõi, ch·∫°y l·ªánh sau:\n",
        "\n",
        "  ```python\n",
        "  # D·ª´ng SparkContext\n",
        "  try:\n",
        "    sc.stop()\n",
        "    sc = SparkContext(\"local\", \"newAppName\") # t·∫°o l·∫°i c√°i m·ªõi\n",
        "    print(sc)\n",
        "  except:\n",
        "    print('B·ªã l·ªói v√¨ kh√¥ng th·ªÉ g·ªçi ƒë·∫øn sc.stop(). C√≥ th·ªÉ bi·∫øn sc ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o tr∆∞·ªõc ƒë√≥. ')\n",
        "   ``` \n",
        "</details> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rFZdy_I2-YI"
      },
      "source": [
        "## RDD üî•\n",
        "\n",
        "RDD (Resilient Distributed Dataset) l√† c·∫•u tr√∫c d·ªØ li·ªáu c·ªët l√µi c·ªßa Apache Spark, cho ph√©p x·ª≠ l√Ω d·ªØ li·ªáu ph√¢n t√°n tr√™n c√°c cluster m·ªôt c√°ch linh ho·∫°t v√† hi·ªáu qu·∫£."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OdUmVugmOk3"
      },
      "source": [
        "![](https://images.viblo.asia/2cd88166-3c16-4cdc-9298-ce9900ac1288.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmpiYtGcmU6z"
      },
      "source": [
        "üî¥ **C√°c t√≠nh ch·∫•t c·ªßa RDD:**\n",
        "\n",
        "- T√≠nh Ph√¢n t√°n (Distributed): M·ªói RDD ƒë∆∞·ª£c chia th√†nh c√°c ph·∫ßn nh·ªè g·ªçi l√† partitions, m·ªói partition c√≥ th·ªÉ ƒë∆∞·ª£c x·ª≠ l√Ω ƒë·ªôc l·∫≠p tr√™n c√°c node kh√°c nhau trong 1 cluster\n",
        "\n",
        "- T√≠nh Linh ho·∫°t (Resilient): RDD c√≥ th·ªÉ t·ª± ƒë·ªông ph·ª•c h·ªìi sau khi m·ªôt ph·∫ßn c·ªßa d·ªØ li·ªáu ho·∫∑c m·ªôt ph·∫ßn c·ªßa cluster b·ªã l·ªói.\n",
        "\n",
        "- T√≠nh B·∫•t bi·∫øn : Sau khi ƒë∆∞·ª£c t·∫°o, m·ªôt RDD kh√¥ng th·ªÉ thay ƒë·ªïi.\n",
        "\n",
        "- ƒê√°nh gi√° l∆∞·ªùi (Lazy Evaluation): C√°c ph√©p bi·∫øn ƒë·ªïi tr√™n RDD kh√¥ng ƒë∆∞·ª£c th·ª±c hi·ªán ngay l·∫≠p t·ª©c m√† ch·ªâ khi c√≥ h√†nh ƒë·ªông (action) ƒë∆∞·ª£c g·ªçi.\n",
        "\n",
        "- T√≠nh t·ªëi ∆∞u h√≥a (Optimized): RDDs c√≥ th·ªÉ t·ªëi ∆∞u h√≥a ƒë·ªÉ t·∫≠n d·ª•ng c√°c ho·∫°t ƒë·ªông in-memory, gi·∫£m thi·ªÉu vi·ªác truy c·∫≠p d·ªØ li·ªáu t·ª´ ƒëƒ©a v√† t·ªëi ∆∞u h√≥a vi·ªác chuy·ªÉn d·ªØ li·ªáu gi·ªØa c√°c ph·∫ßn c·ªßa RDD tr√™n cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzOKzzog2-YI",
        "outputId": "f6195d12-6a51-43f2-9377-bc38ffb5b6a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Scalar', 'java', 'hadoop', 'spark', 'akka', 'spark and hadoop', 'pyspark', 'pyspark and spark']\n",
            "<class 'list'>\n"
          ]
        }
      ],
      "source": [
        "words = [\n",
        "    'Scalar',\n",
        "    'java',\n",
        "    'hadoop',\n",
        "    'spark',\n",
        "    'akka',\n",
        "    'spark and hadoop',\n",
        "    'pyspark',\n",
        "    'pyspark and spark'\n",
        "]\n",
        "\n",
        "print(words)\n",
        "print(type(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ym4CpQyl2-YI",
        "outputId": "bb53d409-dc49-4aca-d40e-369b97e1200f"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'sc'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[65], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m rdd_words \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallelize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(rdd_words))\n",
            "File \u001b[1;32mc:\\Users\\nam\\anaconda3\\envs\\py3_9\\lib\\site-packages\\pyspark\\context.py:783\u001b[0m, in \u001b[0;36mSparkContext.parallelize\u001b[1;34m(self, c, numSlices)\u001b[0m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparallelize\u001b[39m(\u001b[38;5;28mself\u001b[39m, c: Iterable[T], numSlices: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RDD[T]:\n\u001b[0;32m    752\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;124;03m    Distribute a local Python collection to form an RDD. Using range\u001b[39;00m\n\u001b[0;32m    754\u001b[0m \u001b[38;5;124;03m    is recommended if the input represents a range for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;124;03m    [['a'], ['b', 'c']]\u001b[39;00m\n\u001b[0;32m    782\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 783\u001b[0m     numSlices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(numSlices) \u001b[38;5;28;01mif\u001b[39;00m numSlices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefaultParallelism\u001b[49m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(c, \u001b[38;5;28mrange\u001b[39m):\n\u001b[0;32m    785\u001b[0m         size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(c)\n",
            "File \u001b[1;32mc:\\Users\\nam\\anaconda3\\envs\\py3_9\\lib\\site-packages\\pyspark\\context.py:630\u001b[0m, in \u001b[0;36mSparkContext.defaultParallelism\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefaultParallelism\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m    620\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    621\u001b[0m \u001b[38;5;124;03m    Default level of parallelism to use when not given by user (e.g. for reduce tasks)\u001b[39;00m\n\u001b[0;32m    622\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;124;03m    True\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m()\u001b[38;5;241m.\u001b[39mdefaultParallelism()\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sc'"
          ]
        }
      ],
      "source": [
        "rdd_words = sc.parallelize(words)\n",
        "print(type(rdd_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkV-sspxOyn7"
      },
      "source": [
        "**üî¥ C√°c h√†nh ƒë·ªông (Actions) tr√™n RDD**\n",
        "\n",
        "C√°c h√†nh ƒë·ªông th·ª±c thi t√≠nh to√°n tr√™n RDD v√† tr·∫£ v·ªÅ k·∫øt qu·∫£.\n",
        "M·ªôt s·ªë h√†nh ƒë·ªông ph·ªï bi·∫øn:\n",
        "\n",
        "- collect(): L·∫•y t·∫•t c·∫£ ph·∫ßn t·ª≠ t·ª´ RDD v·ªÅ driver.\n",
        "- count(): ƒê·∫øm s·ªë ph·∫ßn t·ª≠ trong RDD.\n",
        "- first(): L·∫•y ph·∫ßn t·ª≠ ƒë·∫ßu ti√™n.\n",
        "- take(n): L·∫•y n ph·∫ßn t·ª≠ ƒë·∫ßu ti√™n.\n",
        "- reduce(f): G·ªôp c√°c ph·∫ßn t·ª≠ v·ªõi m·ªôt h√†m f."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JEYDPTVkkEr"
      },
      "source": [
        "**1Ô∏è‚É£ collect()**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjsBmaJpk-S8"
      },
      "source": [
        "Thu th·∫≠p to√†n b·ªô d·ªØ li·ªáu t·ª´ c√°c ph√¢n v√πng c·ªßa RDD tr√™n c√°c worker node v√† ƒë∆∞a v·ªÅ driver d∆∞·ªõi d·∫°ng m·ªôt danh s√°ch Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S43jRCL5jtNG",
        "outputId": "1d47b779-9b06-4294-a66f-f165a2263219"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Scalar', 'java', 'hadoop', 'spark', 'akka', 'spark and hadoop', 'pyspark', 'pyspark and spark']\n",
            "<class 'list'>\n"
          ]
        }
      ],
      "source": [
        "a = rdd_words.collect()\n",
        "print(a)\n",
        "print(type(a))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2O2WgUQlt7J"
      },
      "source": [
        "Khi g·ªçi collect(), Spark s·∫Ω th·ª±c hi·ªán to√†n b·ªô c√°c ph√©p bi·∫øn ƒë·ªïi (transformation) ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a tr√™n RDD ƒë·ªÉ t·∫°o ra d·ªØ li·ªáu cu·ªëi c√πng.\n",
        "\n",
        "L∆∞u √Ω r·∫±ng n√≥ c√≥ th·ªÉ d·ªÖ d√†ng g√¢y ra v·∫•n ƒë·ªÅ b·ªô nh·ªõ n·∫øu d·ªØ li·ªáu qu√° l·ªõn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR3p-dSxoits"
      },
      "source": [
        "**2Ô∏è‚É£count()**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUIn-tAOlrwq"
      },
      "source": [
        "ƒê·∫øm s·ªë l∆∞·ª£ng ph·∫ßn t·ª≠ c√≥ trong RDD v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ d∆∞·ªõi d·∫°ng m·ªôt s·ªë nguy√™n (integer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsl6MInM2-YI",
        "outputId": "a5d9316b-08ae-4202-8e0c-7d81ac29c7f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8\n"
          ]
        }
      ],
      "source": [
        "print(rdd_words.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8veLylko0Ex"
      },
      "source": [
        "Khi g·ªçi count(), t·∫•t c·∫£ c√°c ph√©p bi·∫øn ƒë·ªïi (transformation) ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a tr√™n RDD s·∫Ω ƒë∆∞·ª£c th·ª±c hi·ªán ƒë·ªÉ t·∫°o ra d·ªØ li·ªáu cu·ªëi c√πng tr∆∞·ªõc khi ƒë·∫øm s·ªë l∆∞·ª£ng ph·∫ßn t·ª≠.\n",
        "\n",
        "Kh√¥ng gi·ªëng nh∆∞ collect, count ch·ªâ tr·∫£ v·ªÅ s·ªë ƒë·∫øm, do ƒë√≥ n√≥ kh√¥ng g√¢y ra qu√° t·∫£i b·ªô nh·ªõ tr√™n driver ngay c·∫£ khi RDD ch·ª©a r·∫•t nhi·ªÅu ph·∫ßn t·ª≠."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eY_0nm72ZTZ"
      },
      "source": [
        "**3Ô∏è‚É£first()**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nN_kEudH4O6-"
      },
      "source": [
        "Ch·ªâ l·∫•y v·ªÅ 1 ph·∫ßn t·ª≠ m√† kh√¥ng c·∫ßn chuy·ªÉn to√†n b·ªô d·ªØ li·ªáu v·ªÅ driver, gi√∫p tr√°nh qu√° t·∫£i b·ªô nh·ªõ.\n",
        "\n",
        "N·∫øu RDD r·ªóng, ph∆∞∆°ng th·ª©c n√†y s·∫Ω n√©m ra l·ªói (exception)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "iInYad2pOyUf",
        "outputId": "d05041db-9c8b-4ce8-e289-3d63ffc87a9b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Scalar'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd_words.first()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT-zKWbUhkmr"
      },
      "source": [
        "**4Ô∏è‚É£take()**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZLwx1TUmeIh"
      },
      "source": [
        "`take(n)` tr·∫£ v·ªÅ m·ªôt danh s√°ch ch·ª©a n ph·∫ßn t·ª≠ ƒë·∫ßu ti√™n c·ªßa RDD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzo1mKWuhw2r",
        "outputId": "4cafe82c-d0dc-4af5-9daf-840187ee9648"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Scalar', 'java', 'hadoop', 'spark', 'akka']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd_words.take(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnNgYwUM4LBu"
      },
      "source": [
        "Khi g·ªçi `take(n)`, Spark s·∫Ω duy·ªát qua c√°c partition c·ªßa RDD cho ƒë·∫øn khi thu th·∫≠p ƒë·ªß n ph·∫ßn t·ª≠. N·∫øu d·ªØ li·ªáu trong RDD √≠t h∆°n n, n√≥ s·∫Ω tr·∫£ v·ªÅ t·∫•t c·∫£ c√°c ph·∫ßn t·ª≠ c√≥ s·∫µn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_DgNcZ5oAQE"
      },
      "source": [
        "**üî¥C√°c ph√©p bi·∫øn ƒë·ªïi (Transformations) tr√™n RDD**\n",
        "\n",
        "C√°c ph√©p bi·∫øn ƒë·ªïi tr√™n RDD t·∫°o ra RDD m·ªõi m√† kh√¥ng thay ƒë·ªïi RDD ban ƒë·∫ßu.\n",
        "\n",
        "M·ªôt s·ªë ph√©p bi·∫øn ƒë·ªïi ph·ªï bi·∫øn:\n",
        "\n",
        "- map(f): √Åp d·ª•ng m·ªôt h√†m f l√™n t·ª´ng ph·∫ßn t·ª≠.\n",
        "- filter(f): L·ªçc c√°c ph·∫ßn t·ª≠ th·ªèa m√£n ƒëi·ªÅu ki·ªán f.\n",
        "- flatMap(f): Gi·ªëng map(), nh∆∞ng m·ªü r·ªông k·∫øt qu·∫£ th√†nh nhi·ªÅu ph·∫ßn t·ª≠.\n",
        "- groupByKey(): Nh√≥m c√°c ph·∫ßn t·ª≠ c√≥ c√πng kh√≥a (key-value RDD).\n",
        "- reduceByKey(f): K·∫øt h·ª£p c√°c gi√° tr·ªã c√≥ c√πng kh√≥a b·∫±ng m·ªôt h√†m f."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmUuvbkOoTVy"
      },
      "source": [
        "**filter()**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Wji1RB6oR3L"
      },
      "source": [
        "filter nh·∫≠n v√†o m·ªôt h√†m ƒëi·ªÅu ki·ªán (predicate) v√† √°p d·ª•ng h√†m ƒë√≥ cho t·ª´ng ph·∫ßn t·ª≠ c·ªßa RDD.\n",
        "\n",
        "Ch·ªâ nh·ªØng ph·∫ßn t·ª≠ m√† h√†m tr·∫£ v·ªÅ True s·∫Ω ƒë∆∞·ª£c gi·ªØ l·∫°i trong RDD k·∫øt qu·∫£.\n",
        "\n",
        "RDD ban ƒë·∫ßu kh√¥ng b·ªã thay ƒë·ªïi. Thay v√†o ƒë√≥, filter t·∫°o ra m·ªôt RDD m·ªõi ch·ª©a c√°c ph·∫ßn t·ª≠ ƒë√£ ƒë∆∞·ª£c l·ªçc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "H8LLtHhXmg4N",
        "outputId": "84ec774e-97ef-4f8c-b21b-79cfc0a4dee6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['hadoop', 'spark and hadoop']\n"
          ]
        }
      ],
      "source": [
        "# d√πng filter ƒë·ªÉ l·∫•y ra c√°c chu·ªói c√≥ 2 ch·ªØ 'oo'\n",
        "filtered_char = rdd_words.filter(lambda x: 'oo' in x)\n",
        "\n",
        "# Thu th·∫≠p k·∫øt qu·∫£ v√† in ra\n",
        "print(filtered_char.collect())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py3_9",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
