{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKhANDPI2-YD"
      },
      "source": [
        "# Pyspark tutorial üéØ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y9ghrep9ckQ"
      },
      "source": [
        "**Ghi ch√∫:**\n",
        "\n",
        " üîπtutorial n√†y ƒë∆∞·ª£c th·ª±c hi·ªán tr√™n google colab n√™n kh√¥ng b·ªã l·ªói khi ch·∫°y code. N·∫øu ch·∫°y tr√™n jupyter notebook ƒë∆∞·ª£c host b·ªüi local machine ch·∫Øc ch·∫Øn s·∫Ω l·ªói (ch·∫Øc v√¨ l√Ω do c·∫•u h√¨nh c√†i ƒë·∫∑t hay g√¨ ƒë√≥ :v). M·∫∑c d√π ƒë√£ m√≤ m·∫´m c·∫£ tu·∫ßn nh∆∞ng m√¨nh v·∫´n ch∆∞a c√≥ gi·∫£i ph√°p n√†o cho v·∫•n ƒë·ªÅ n√†y n√™n m√¨nh ch·∫•p nh·∫≠n th·ª±c hi·ªán n√≥ tr√™n google colab. ü§∑‚Äç‚ôÇÔ∏è\n",
        "\n",
        " üîπTh√™m n·ªØa, tutorial n√†y ƒë∆∞·ª£c x√¢y d·ª±ng d·ª±a tr√™n nhi·ªÅu ngu·ªìn, trong ƒë√≥ c√≥ c√°c blog m√† m√¨nh kham kh·∫£o. Sau ƒë√¢y xin c·∫£m ∆°n blog c·ªßa [longcnttbkhn](https://longcnttbkhn.github.io/huong-dan-spark-co-ban-cho-nguoi-moi/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXX24zUvUB_y"
      },
      "source": [
        "\n",
        "![](https://th.bing.com/th/id/OIP.I3eg_GSGbjpQ0O8GDuHVdgHaFL?rs=1&pid=ImgDetMain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-Cj9XV-2-YF"
      },
      "source": [
        "## Installation ‚¨áÔ∏è"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GFFs2FY2-YF"
      },
      "source": [
        "ƒê·ªÉ l√†m vi·ªác v·ªõi spark, ch√∫ng ta c·∫ßn c√†i ƒë·∫∑t c√°c th√†nh ph·∫ßn sau:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "477uUkwe2-YF"
      },
      "source": [
        "- üìå Java (OpenJDK 8 ho·∫∑c 11):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iqBQ9FIG4Ed"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvamacLl2-YF"
      },
      "source": [
        "- üìå Apache Spark\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rjp-b0FGrrA"
      },
      "outputs": [],
      "source": [
        "# t·∫£i g√≥i spark apache v·ªÅ\n",
        "!wget https://downloads.apache.org/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz\n",
        "\n",
        "# gi·∫£i n√©n\n",
        "!tar -xvzf /content/spark-3.5.4-bin-hadoop3.tgz\n",
        "\n",
        "# di chuy·ªÉn Spark v√†o th∆∞ m·ª•c /opt/spark (ƒë√¢y l√† quy ∆∞·ªõc, t·ª± t√¨m hi·ªÉu th√™m)\n",
        "!sudo mv /content/spark-3.5.4-bin-hadoop3 /opt/spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQP7D00oGyR6"
      },
      "source": [
        "üîß Thi·∫øt l·∫≠p bi·∫øn m√¥i tr∆∞·ªùng nh∆∞ sau:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RR3VHbS8Glrt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/opt/spark\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z26vqdxlMLxp",
        "outputId": "876abd39-8def-4ea1-d315-58f067ea5e3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/opt/spark\n",
            "/usr/lib/jvm/java-8-openjdk-amd64\n"
          ]
        }
      ],
      "source": [
        "# ki·ªÉm tra m√¥i tr∆∞·ªùng\n",
        "! echo $SPARK_HOME\n",
        "! echo $JAVA_HOME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-D7dd0y2-YG"
      },
      "source": [
        "- üìå pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHxirntQ2-YG",
        "outputId": "af1e32fb-1727-40f7-a89e-213e80e54a24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.4)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i6VQ3wE2-YG"
      },
      "source": [
        "- üìå findspark - t√πy ch·ªçn, n·∫øu ch·∫°y tr√™n Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVLN3of82-YH",
        "outputId": "942e1662-b5cf-452f-c9be-5ebf89d3aea8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: findspark in /usr/local/lib/python3.11/dist-packages (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install findspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zwpg9kB1X0nv"
      },
      "source": [
        "## Run Spark app üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfPtbeXrZXHk"
      },
      "outputs": [],
      "source": [
        "# kh·ªüi t·∫°o m·ªôt file .py\n",
        "! touch firstapp.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmXe6eUfZxUq",
        "outputId": "429b848e-0ce7-4adb-ddd0-aed23afca745"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting firstapp.py\n"
          ]
        }
      ],
      "source": [
        "# ghi n·ªôi dung v√†o file\n",
        "# m·ªõi ƒë·∫ßu ƒë·ª´ng quan t√¢m n√≥ code g√¨, ch·ªâ bi·∫øt n√≥ ch·∫°y l√† ƒë∆∞·ª£c, bi·∫øt th√¨ c√†ng t·ªët :V\n",
        "%%writefile firstapp.py\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"First Spark Application\") \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    data = [(\"Alice\", 1), (\"Bob\", 2)]\n",
        "    df = spark.createDataFrame(data, [\"Name\", \"Value\"])\n",
        "    df.show()\n",
        "\n",
        "    spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJVBudvWZe7K",
        "outputId": "4163b41c-03bd-457d-e243-82c413782928"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "from pyspark.sql import SparkSession\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    spark = SparkSession.builder \\\n",
            "        .appName(\"First Spark Application\") \\\n",
            "        .master(\"local[*]\") \\\n",
            "        .getOrCreate()\n",
            "\n",
            "    data = [(\"Alice\", 1), (\"Bob\", 2)]\n",
            "    df = spark.createDataFrame(data, [\"Name\", \"Value\"])\n",
            "    df.show()\n",
            "\n",
            "    spark.stop()\n"
          ]
        }
      ],
      "source": [
        "# ki·ªÉm tra n·ªôi dung ƒë√£ ghi v√†o\n",
        "! cat firstapp.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5qPDPiRaLdF",
        "outputId": "d06f7f83-e53a-4d81-b60d-402dc29be1c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25/02/26 09:28:11 INFO SparkContext: Running Spark version 3.5.4\n",
            "25/02/26 09:28:11 INFO SparkContext: OS info Linux, 6.1.85+, amd64\n",
            "25/02/26 09:28:11 INFO SparkContext: Java version 1.8.0_442\n",
            "25/02/26 09:28:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/02/26 09:28:11 INFO ResourceUtils: ==============================================================\n",
            "25/02/26 09:28:11 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/02/26 09:28:11 INFO ResourceUtils: ==============================================================\n",
            "25/02/26 09:28:11 INFO SparkContext: Submitted application: First Spark Application\n",
            "25/02/26 09:28:11 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/02/26 09:28:11 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/02/26 09:28:11 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/02/26 09:28:12 INFO SecurityManager: Changing view acls to: root\n",
            "25/02/26 09:28:12 INFO SecurityManager: Changing modify acls to: root\n",
            "25/02/26 09:28:12 INFO SecurityManager: Changing view acls groups to: \n",
            "25/02/26 09:28:12 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/02/26 09:28:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/02/26 09:28:13 INFO Utils: Successfully started service 'sparkDriver' on port 36777.\n",
            "25/02/26 09:28:13 INFO SparkEnv: Registering MapOutputTracker\n",
            "25/02/26 09:28:13 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/02/26 09:28:13 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/02/26 09:28:13 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/02/26 09:28:13 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/02/26 09:28:13 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8291414f-a0c9-4419-90ae-38b691fb1d3f\n",
            "25/02/26 09:28:13 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "25/02/26 09:28:13 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/02/26 09:28:14 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/02/26 09:28:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "25/02/26 09:28:14 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "25/02/26 09:28:14 INFO Executor: Starting executor ID driver on host 2ad909ab0e90\n",
            "25/02/26 09:28:14 INFO Executor: OS info Linux, 6.1.85+, amd64\n",
            "25/02/26 09:28:14 INFO Executor: Java version 1.8.0_442\n",
            "25/02/26 09:28:14 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/02/26 09:28:14 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@792e2fe4 for default.\n",
            "25/02/26 09:28:14 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39763.\n",
            "25/02/26 09:28:14 INFO NettyBlockTransferService: Server created on 2ad909ab0e90:39763\n",
            "25/02/26 09:28:14 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/02/26 09:28:14 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 2ad909ab0e90, 39763, None)\n",
            "25/02/26 09:28:14 INFO BlockManagerMasterEndpoint: Registering block manager 2ad909ab0e90:39763 with 366.3 MiB RAM, BlockManagerId(driver, 2ad909ab0e90, 39763, None)\n",
            "25/02/26 09:28:14 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 2ad909ab0e90, 39763, None)\n",
            "25/02/26 09:28:14 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 2ad909ab0e90, 39763, None)\n",
            "25/02/26 09:28:16 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/02/26 09:28:16 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/02/26 09:28:20 INFO CodeGenerator: Code generated in 502.998729 ms\n",
            "25/02/26 09:28:21 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
            "25/02/26 09:28:21 INFO DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "25/02/26 09:28:21 INFO DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)\n",
            "25/02/26 09:28:21 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/02/26 09:28:21 INFO DAGScheduler: Missing parents: List()\n",
            "25/02/26 09:28:21 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "25/02/26 09:28:21 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 12.7 KiB, free 366.3 MiB)\n",
            "25/02/26 09:28:21 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 366.3 MiB)\n",
            "25/02/26 09:28:21 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 2ad909ab0e90:39763 (size: 6.6 KiB, free: 366.3 MiB)\n",
            "25/02/26 09:28:21 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585\n",
            "25/02/26 09:28:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "25/02/26 09:28:21 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/02/26 09:28:21 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (2ad909ab0e90, executor driver, partition 0, PROCESS_LOCAL, 9016 bytes) \n",
            "25/02/26 09:28:21 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/02/26 09:28:23 INFO CodeGenerator: Code generated in 44.983168 ms\n",
            "25/02/26 09:28:23 INFO PythonRunner: Times: total = 990, boot = 811, init = 179, finish = 0\n",
            "25/02/26 09:28:23 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1971 bytes result sent to driver\n",
            "25/02/26 09:28:23 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2219 ms on 2ad909ab0e90 (executor driver) (1/1)\n",
            "25/02/26 09:28:23 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/02/26 09:28:23 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 60639\n",
            "25/02/26 09:28:23 INFO DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 2.556 s\n",
            "25/02/26 09:28:23 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/02/26 09:28:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/02/26 09:28:23 INFO DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 2.650840 s\n",
            "25/02/26 09:28:23 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
            "25/02/26 09:28:23 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "25/02/26 09:28:23 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)\n",
            "25/02/26 09:28:23 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/02/26 09:28:23 INFO DAGScheduler: Missing parents: List()\n",
            "25/02/26 09:28:23 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "25/02/26 09:28:23 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.7 KiB, free 366.3 MiB)\n",
            "25/02/26 09:28:23 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 366.3 MiB)\n",
            "25/02/26 09:28:23 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 2ad909ab0e90:39763 (size: 6.6 KiB, free: 366.3 MiB)\n",
            "25/02/26 09:28:23 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
            "25/02/26 09:28:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1))\n",
            "25/02/26 09:28:23 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/02/26 09:28:23 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (2ad909ab0e90, executor driver, partition 1, PROCESS_LOCAL, 9014 bytes) \n",
            "25/02/26 09:28:23 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/02/26 09:28:23 INFO PythonRunner: Times: total = 181, boot = -139, init = 320, finish = 0\n",
            "25/02/26 09:28:23 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1883 bytes result sent to driver\n",
            "25/02/26 09:28:23 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 219 ms on 2ad909ab0e90 (executor driver) (1/1)\n",
            "25/02/26 09:28:23 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.240 s\n",
            "25/02/26 09:28:23 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/02/26 09:28:23 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/02/26 09:28:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/02/26 09:28:23 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.251319 s\n",
            "25/02/26 09:28:26 INFO CodeGenerator: Code generated in 59.110357 ms\n",
            "+-----+-----+\n",
            "| Name|Value|\n",
            "+-----+-----+\n",
            "|Alice|    1|\n",
            "|  Bob|    2|\n",
            "+-----+-----+\n",
            "\n",
            "25/02/26 09:28:26 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/02/26 09:28:26 INFO SparkUI: Stopped Spark web UI at http://2ad909ab0e90:4041\n",
            "25/02/26 09:28:26 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/02/26 09:28:26 INFO MemoryStore: MemoryStore cleared\n",
            "25/02/26 09:28:26 INFO BlockManager: BlockManager stopped\n",
            "25/02/26 09:28:26 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/02/26 09:28:26 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/02/26 09:28:26 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/02/26 09:28:27 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/02/26 09:28:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-84a5e045-f5d8-4da4-b0e7-8bb846892fda/pyspark-29d40d6b-7090-43be-84f1-a45363b6c44b\n",
            "25/02/26 09:28:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-f079293f-8a21-475a-b2ca-0ddf09b04455\n",
            "25/02/26 09:28:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-84a5e045-f5d8-4da4-b0e7-8bb846892fda\n"
          ]
        }
      ],
      "source": [
        "# ch·∫°y file n√†y\n",
        "! $SPARK_HOME/bin/spark-submit firstapp.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c4nXT46ah8F"
      },
      "source": [
        "ƒê√£ c√≥ m·ªôt b·∫£ng v·ªõi 2 c·ªôt (Name, Value) c√πng v·ªõi d√≤ng l√† (Alice, 1) v√† (Bob, 2). V·∫≠y l√† ·ª©ng d·ª•ng ƒë√£ ch·∫°y th√†nh c√¥ng !\n",
        "\n",
        "N·∫øu ch·∫°y file .py theo c√°ch b√¨nh th∆∞·ªùng th√¨ m·ªçi t√†i nguy√™n v√† t√≠nh to√°n s·∫Ω ƒë∆∞·ª£c th·ª±c hi·ªán tr√™n local machine (trong tr∆∞·ªùng h·ª£p n√†y l√† colab), nh∆∞ng l·∫°i b·ªã gi·ªõi h·∫°n v·ªÅ kh·∫£ nƒÉng x·ª≠ l√Ω ph√¢n t√°n.\n",
        "\n",
        "L·ªánh `spark-submit` cho ph√©p ta g·ª° b·ªè ƒëi·ªÅu ƒë√≥ b·∫±ng c√°ch \"submit\"  file .py c·ªßa b·∫°n ƒë·∫øn cluster v√†  ph√¢n ph·ªëi c√¥ng vi·ªác cho c√°c node trong cluster ƒë√≥.\n",
        "\n",
        "Gi·ªù th√¨ ta s·∫Ω ƒëi s√¢u h∆°n v√†o t·ª´ng ph·∫ßn c·ªßa m√£ ngu·ªìn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8Jo6rOf2-YH"
      },
      "source": [
        "## SparkSession üî•"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6VlbveA2-YH"
      },
      "source": [
        "Apache Spark l√† m·ªôt h·ªá th·ªëng ph√¢n t√°n, ch·∫°y tr√™n nhi·ªÅu node. ƒê·ªÉ t∆∞∆°ng t√°c v·ªõi spark, ch√∫ng ta c·∫ßn m·ªôt th·ª© g·ªçi l√† \"ƒëi·ªÉm kh·ªüi ƒë·∫ßu\" (Entry point). Trong tr∆∞·ªùng h·ª£p n√†y ch√≠nh l√† SparkSession. SparkSession gi·ªëng nh∆∞ c·ª≠a ch√≠nh c·ªßa m·ªôt t√≤a nh√†, gi√∫p b·∫°n truy c·∫≠p v√†o c√°c th√†nh ph·∫ßn b√™n trong nh∆∞ SparkContext, StreamContext, SQLContext,... (m·∫•y c√°i n√†y b√†n sau)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64OXTUScUG-i"
      },
      "source": [
        "![](https://abhishekbaranwal10.files.wordpress.com/2018/09/introduction-to-apache-spark-20-12-638.jpg?resize=638%2C479&is-pending-load=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YmADrJF2-YH"
      },
      "outputs": [],
      "source": [
        "# kh·ªüi t·∫°o SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Datacamp Pyspark Tutorial\") \\\n",
        "    .config(\"spark.memory.offHeap.enabled\",\"true\") \\\n",
        "    .config(\"spark.memory.offHeap.size\",\"5g\").getOrCreate()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP4IdQzRCM60"
      },
      "source": [
        "D·ª´ng SparkSession khi ƒë√£ ho√†n th√†nh t·∫•t c·∫£ c√°c t√°c v·ª• x·ª≠ l√Ω d·ªØ li·ªáu, nh·∫±m gi·∫£i ph√≥ng t√†i nguy√™n (nh∆∞ b·ªô nh·ªõ, CPU v√† k·∫øt n·ªëi ƒë·∫øn cluster)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbkhNJQ52-YH"
      },
      "outputs": [],
      "source": [
        "# spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tnpHHwMAGS4"
      },
      "source": [
        "## SparkConf üî•"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-inNEMrWAMzn"
      },
      "source": [
        "SparkConf l√† m·ªôt class ƒë·∫∑c bi·ªát gi√∫p ch√∫ng ta thay ƒë·ªïi l·∫°i c√°ch kh·ªüi t·∫°o session\n",
        "\n",
        "·ªû ph·∫ßn tr√™n, ch√∫ng ta kh·ªüi t·∫°o m·ªôt session th√¥ng qua m·ªôt builder c√≥ s·∫µn v√† n√≥ kh√° ƒë∆°n gi·∫£n. Nh∆∞ng n·∫øu mu·ªën t√πy ch·ªânh c·∫•u h√¨nh chi ti·∫øt (nh∆∞ b·ªô nh·ªõ, s·ªë l∆∞·ª£ng core, v.v.) th√¨ ta c√≥ c√°ch sau:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0x72-FNBUE5",
        "outputId": "ab259ba9-547c-4097-9e1f-64e9682ebda3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "some-value\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkConf\n",
        "\n",
        "\n",
        "# T·∫°o SparkConf v·ªõi c√°c c·∫•u h√¨nh c·∫ßn thi·∫øt\n",
        "conf = SparkConf() \\\n",
        "    .setAppName(\"MyApp\") \\\n",
        "    .setMaster(\"local[*]\") \\\n",
        "    .set(\"spark.some.config.option\", \"some-value\")\n",
        "\n",
        "# T·∫°o SparkSession b·∫±ng c√°ch truy·ªÅn SparkConf\n",
        "spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .config(conf=conf) \\\n",
        "        .getOrCreate()\n",
        "\n",
        "# Ki·ªÉm tra c·∫•u h√¨nh t·ª´ SparkSession\n",
        "print(spark.conf.get(\"spark.some.config.option\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZistYlEcBieI"
      },
      "source": [
        "ƒê·ªëi v·ªõi ƒëa s·ªë ·ª©ng d·ª•ng, vi·ªác s·ª≠ d·ª•ng tr·ª±c ti·∫øp builder c·ªßa SparkSession ƒë√£ ƒë·ªß n√™n c√≥ th·ªÉ b·ªè qua b∆∞·ªõc n√†y."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VvnNMdM2-YH"
      },
      "source": [
        "## SparkContext üî•"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAuKglvr2-YH"
      },
      "source": [
        "üîπSparkContext l√† class c·ªët l√µi trong Apache Spark, gi√∫p ·ª©ng d·ª•ng k·∫øt n·ªëi v·ªõi cluster manager - tr√¨nh qu·∫£n l√Ω c√°c t√°c v·ª• t√≠nh to√°n ph√¢n t√°n.\n",
        "\n",
        "üîπSparkContext ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ kh·ªüi t·∫°o c√°c c·∫•u tr√∫c d·ªØ li·ªáu ƒë·∫∑c bi·ªát nh∆∞ RDD, accumulator, v√† broadcast variable (c√°i n√†y s·∫Ω n√≥i sau)\n",
        "\n",
        "üîπTr∆∞·ªõc Spark phi√™n b·∫£n 2.x, SparkContext l√† entry point ch√≠nh.\n",
        "\n",
        "üîπT·ª´ Spark 2.x tr·ªü ƒëi, SparkSession thay th·∫ø SparkContext, nh∆∞ng SparkContext v·∫´n t·ªìn t·∫°i b√™n trong SparkSession.\n",
        "\n",
        "ƒê·ªÉ hi·ªÉu s√¢u h∆°n, h√£y nh√¨n s∆° ƒë·ªì b√™n d∆∞·ªõi:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ido4zEM78zm"
      },
      "source": [
        "![](https://sparkbyexamples.com/wp-content/uploads/2022/05/image04.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PFbmL-hbF4h"
      },
      "source": [
        "Nh∆∞ ƒë√£ n√≥i, khi ch·∫°y m·ªôt Spark app, ch∆∞∆°ng tr√¨nh s·∫Ω ƒë∆∞·ª£c \"submit\" t·ªõi cluster v√† ƒë∆∞·ª£c ƒë∆∞a ƒë·∫øn c√°c node ƒëang c√≥ d·ªØ li·ªáu. Trong ƒë√≥, m·ªôt node s·∫Ω ƒë∆∞·ª£c ch·ªçn l√†m **Master node** ƒë·ªÉ ch·∫°y m·ªôt ch∆∞∆°ng tr√¨nh g·ªçi l√† **Driver Program**, c√°c node c√≤n l·∫°i s·∫Ω ƒë√≥ng vai tr√≤ l√† **Worker**.\n",
        "\n",
        "T·∫°i **Driver Program**, m·ªôt ƒë·ªëi t∆∞·ª£ng **SparkContext** s·∫Ω ƒë∆∞·ª£c kh·ªüi t·∫°o gi√∫p ·ª©ng d·ª•ng Spark giao ti·∫øp v·ªõi **Cluster Manager** ƒë·ªÉ y√™u c·∫ßu t√†i nguy√™n. Sau khi t√†i nguy√™n ƒë∆∞·ª£c ph√¢n b·ªï cho c√°c node, **Cluster Manager** s·∫Ω kh·ªüi ƒë·ªông c√°c **Executor**.\n",
        "\n",
        "**Executor** l√† nh·ªØng ti·∫øn tr√¨nh ch·∫°y tr√™n c√°c **Worker** v√† ch√∫ng s·∫Ω x·ª≠ l√Ω c√°c **Task** ƒë∆∞·ª£c giao b·ªüi **Driver Program**.\n",
        "\n",
        "**Driver Program** s·∫Ω t·∫°o task v√† ph√¢n chia cho c√°c **Worker** theo nguy√™n t·∫Øc x·ª≠ l√Ω c·ª•c b·ªô, t·ª©c l√† t√†i nguy√™n tr√™n node n√†o s·∫Ω ƒë∆∞·ª£c x·ª≠ l√Ω b·ªüi **Executor** tr√™n node ƒë√≥."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2vZTRc4fcMq"
      },
      "source": [
        "N√≥i t·ªõi ƒë√¢y th√¨ c√≥ l·∫Ω ch√∫ng ta ƒë√£ ƒë·ªãnh h√¨nh ƒë∆∞·ª£c vai tr√≤ c·ªßa SparkContext r·ªìi. H√£y nh·ªõ, b·∫•t c·ª© khi n√†o l√†m vi·ªác v·ªõi Spark, lu√¥n kh·ªüi t·∫°o SparkContext sau khi ƒë√£ c√≥ SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iS3Alc0L2-YH",
        "outputId": "cce42a49-ff69-4485-ae83-673ec408921b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SparkContext ƒë√£ t·ªìn t·∫°i, ch·ªâ kh·ªüi t·∫°o m·ªôt l·∫ßn. ƒê·ªçc th√™m ·ªü b√™n d∆∞·ªõi ƒë·ªÉ bi·∫øt th√™m chi ti·∫øt\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Kh·ªüi t·∫°o sparkContext\n",
        "\n",
        "try:\n",
        "  sc = SparkContext(\"local\", \"MyApp\")\n",
        "  print(sc)\n",
        "except:\n",
        "  print(\"SparkContext ƒë√£ t·ªìn t·∫°i, ch·ªâ kh·ªüi t·∫°o m·ªôt l·∫ßn. ƒê·ªçc th√™m ·ªü b√™n d∆∞·ªõi ƒë·ªÉ bi·∫øt th√™m chi ti·∫øt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "4gq70-rV2-YI",
        "outputId": "1eea7ba4-8c78-4462-dcb7-e6fbfed761dc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://2ad909ab0e90:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.4</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Datacamp Pyspark Tutorial</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ],
            "text/plain": [
              "<SparkContext master=local[*] appName=Datacamp Pyspark Tutorial>"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ki·ªÉm tra c√≥ SparkContext n√†o ƒëang ch·∫°y kh√¥ng\n",
        "SparkContext._active_spark_context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS9Ns5_l2-YI"
      },
      "source": [
        "üîπL∆∞u √Ω: sparkContext ch·ªâ kh·ªüi t·∫°o m·ªôt l·∫ßn\n",
        "\n",
        "üîπCh·∫°y l·∫ßn 2 b·ªã b√°o l·ªói `Cannot run multiple SparkContexts at once; existing SparkContext`\n",
        "\n",
        "üîπC√≥ th·ªÉ kh·∫Øc ph·ª•c b·∫±ng 2 c√°ch:\n",
        "    üî∏D·ª´ng sparkContext,\n",
        "    üî∏L·∫•y sparkContext hi·ªán c√≥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUeyajYs2-YI",
        "outputId": "811175d0-3722-4dc9-a6e1-17ab489d2ef5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "L∆∞u √Ω: Kh√¥ng g·ªçi sc.stop() n·∫øu bi·∫øn sc ch∆∞a t·ªìn t·∫°i. L√∫c ƒë√≥ ph·∫£i ch·∫°y l·ªánh b√™n d∆∞·ªõi\n"
          ]
        }
      ],
      "source": [
        "# D·ª´ng SparkContext\n",
        "\n",
        "try:\n",
        "  # sc.stop()\n",
        "  # sc = SparkContext(\"local\", \"MyApp\")\n",
        "  print(sc)\n",
        "except:\n",
        "  print('L∆∞u √Ω: Kh√¥ng g·ªçi sc.stop() n·∫øu bi·∫øn sc ch∆∞a t·ªìn t·∫°i. L√∫c ƒë√≥ ph·∫£i ch·∫°y l·ªánh b√™n d∆∞·ªõi')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Y93vaZH2-YI",
        "outputId": "ceadcb04-c400-4d97-e2e4-6dd7c287bdf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<SparkContext master=local[*] appName=Datacamp Pyspark Tutorial>\n"
          ]
        }
      ],
      "source": [
        "# L·∫•y SparkContext hi·ªán c√≥ t·ª´ SparkSession\n",
        "\n",
        "try:\n",
        "  sc = spark.sparkContext\n",
        "  print(sc)\n",
        "except:\n",
        "  print('Ch∆∞a import spark ph·∫£i kh√¥ng ? nh·ªõ import nha :v')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rFZdy_I2-YI"
      },
      "source": [
        "## RDD üî•\n",
        "\n",
        "RDD (Resilient Distributed Dataset) l√† c·∫•u tr√∫c d·ªØ li·ªáu c·ªët l√µi c·ªßa Apache Spark, cho ph√©p x·ª≠ l√Ω d·ªØ li·ªáu ph√¢n t√°n tr√™n c√°c cluster m·ªôt c√°ch linh ho·∫°t v√† hi·ªáu qu·∫£."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OdUmVugmOk3"
      },
      "source": [
        "![](https://images.viblo.asia/2cd88166-3c16-4cdc-9298-ce9900ac1288.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmpiYtGcmU6z"
      },
      "source": [
        "üî¥ **C√°c t√≠nh ch·∫•t c·ªßa RDD:**\n",
        "\n",
        "- T√≠nh Ph√¢n t√°n (Distributed): M·ªói RDD ƒë∆∞·ª£c chia th√†nh c√°c ph·∫ßn nh·ªè g·ªçi l√† partitions, m·ªói partition c√≥ th·ªÉ ƒë∆∞·ª£c x·ª≠ l√Ω ƒë·ªôc l·∫≠p tr√™n c√°c node kh√°c nhau trong 1 cluster\n",
        "\n",
        "- T√≠nh Linh ho·∫°t (Resilient): RDD c√≥ th·ªÉ t·ª± ƒë·ªông ph·ª•c h·ªìi sau khi m·ªôt ph·∫ßn c·ªßa d·ªØ li·ªáu ho·∫∑c m·ªôt ph·∫ßn c·ªßa cluster b·ªã l·ªói.\n",
        "\n",
        "- T√≠nh B·∫•t bi·∫øn : Sau khi ƒë∆∞·ª£c t·∫°o, m·ªôt RDD kh√¥ng th·ªÉ thay ƒë·ªïi.\n",
        "\n",
        "- ƒê√°nh gi√° l∆∞·ªùi (Lazy Evaluation): C√°c ph√©p bi·∫øn ƒë·ªïi tr√™n RDD kh√¥ng ƒë∆∞·ª£c th·ª±c hi·ªán ngay l·∫≠p t·ª©c m√† ch·ªâ khi c√≥ h√†nh ƒë·ªông (action) ƒë∆∞·ª£c g·ªçi.\n",
        "\n",
        "- T√≠nh t·ªëi ∆∞u h√≥a (Optimized): RDDs c√≥ th·ªÉ t·ªëi ∆∞u h√≥a ƒë·ªÉ t·∫≠n d·ª•ng c√°c ho·∫°t ƒë·ªông in-memory, gi·∫£m thi·ªÉu vi·ªác truy c·∫≠p d·ªØ li·ªáu t·ª´ ƒëƒ©a v√† t·ªëi ∆∞u h√≥a vi·ªác chuy·ªÉn d·ªØ li·ªáu gi·ªØa c√°c ph·∫ßn c·ªßa RDD tr√™n cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzOKzzog2-YI",
        "outputId": "f6195d12-6a51-43f2-9377-bc38ffb5b6a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Scalar', 'java', 'hadoop', 'spark', 'akka', 'spark and hadoop', 'pyspark', 'pyspark and spark']\n",
            "<class 'list'>\n"
          ]
        }
      ],
      "source": [
        "words = [\n",
        "    'Scalar',\n",
        "    'java',\n",
        "    'hadoop',\n",
        "    'spark',\n",
        "    'akka',\n",
        "    'spark and hadoop',\n",
        "    'pyspark',\n",
        "    'pyspark and spark'\n",
        "]\n",
        "\n",
        "print(words)\n",
        "print(type(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ym4CpQyl2-YI",
        "outputId": "bb53d409-dc49-4aca-d40e-369b97e1200f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pyspark.rdd.RDD'>\n"
          ]
        }
      ],
      "source": [
        "rdd_words = sc.parallelize(words)\n",
        "print(type(rdd_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkV-sspxOyn7"
      },
      "source": [
        "**üî¥ C√°c h√†nh ƒë·ªông (Actions) tr√™n RDD**\n",
        "\n",
        "C√°c h√†nh ƒë·ªông th·ª±c thi t√≠nh to√°n tr√™n RDD v√† tr·∫£ v·ªÅ k·∫øt qu·∫£.\n",
        "M·ªôt s·ªë h√†nh ƒë·ªông ph·ªï bi·∫øn:\n",
        "\n",
        "- collect(): L·∫•y t·∫•t c·∫£ ph·∫ßn t·ª≠ t·ª´ RDD v·ªÅ driver.\n",
        "- count(): ƒê·∫øm s·ªë ph·∫ßn t·ª≠ trong RDD.\n",
        "- first(): L·∫•y ph·∫ßn t·ª≠ ƒë·∫ßu ti√™n.\n",
        "- take(n): L·∫•y n ph·∫ßn t·ª≠ ƒë·∫ßu ti√™n.\n",
        "- reduce(f): G·ªôp c√°c ph·∫ßn t·ª≠ v·ªõi m·ªôt h√†m f."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JEYDPTVkkEr"
      },
      "source": [
        "**1Ô∏è‚É£ collect()**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjsBmaJpk-S8"
      },
      "source": [
        "Thu th·∫≠p to√†n b·ªô d·ªØ li·ªáu t·ª´ c√°c ph√¢n v√πng c·ªßa RDD tr√™n c√°c worker node v√† ƒë∆∞a v·ªÅ driver d∆∞·ªõi d·∫°ng m·ªôt danh s√°ch Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S43jRCL5jtNG",
        "outputId": "1d47b779-9b06-4294-a66f-f165a2263219"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Scalar', 'java', 'hadoop', 'spark', 'akka', 'spark and hadoop', 'pyspark', 'pyspark and spark']\n",
            "<class 'list'>\n"
          ]
        }
      ],
      "source": [
        "a = rdd_words.collect()\n",
        "print(a)\n",
        "print(type(a))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2O2WgUQlt7J"
      },
      "source": [
        "Khi g·ªçi collect(), Spark s·∫Ω th·ª±c hi·ªán to√†n b·ªô c√°c ph√©p bi·∫øn ƒë·ªïi (transformation) ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a tr√™n RDD ƒë·ªÉ t·∫°o ra d·ªØ li·ªáu cu·ªëi c√πng.\n",
        "\n",
        "L∆∞u √Ω r·∫±ng n√≥ c√≥ th·ªÉ d·ªÖ d√†ng g√¢y ra v·∫•n ƒë·ªÅ b·ªô nh·ªõ n·∫øu d·ªØ li·ªáu qu√° l·ªõn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR3p-dSxoits"
      },
      "source": [
        "**2Ô∏è‚É£count()**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUIn-tAOlrwq"
      },
      "source": [
        "ƒê·∫øm s·ªë l∆∞·ª£ng ph·∫ßn t·ª≠ c√≥ trong RDD v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ d∆∞·ªõi d·∫°ng m·ªôt s·ªë nguy√™n (integer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsl6MInM2-YI",
        "outputId": "a5d9316b-08ae-4202-8e0c-7d81ac29c7f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8\n"
          ]
        }
      ],
      "source": [
        "print(rdd_words.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8veLylko0Ex"
      },
      "source": [
        "Khi g·ªçi count(), t·∫•t c·∫£ c√°c ph√©p bi·∫øn ƒë·ªïi (transformation) ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a tr√™n RDD s·∫Ω ƒë∆∞·ª£c th·ª±c hi·ªán ƒë·ªÉ t·∫°o ra d·ªØ li·ªáu cu·ªëi c√πng tr∆∞·ªõc khi ƒë·∫øm s·ªë l∆∞·ª£ng ph·∫ßn t·ª≠.\n",
        "\n",
        "Kh√¥ng gi·ªëng nh∆∞ collect, count ch·ªâ tr·∫£ v·ªÅ s·ªë ƒë·∫øm, do ƒë√≥ n√≥ kh√¥ng g√¢y ra qu√° t·∫£i b·ªô nh·ªõ tr√™n driver ngay c·∫£ khi RDD ch·ª©a r·∫•t nhi·ªÅu ph·∫ßn t·ª≠."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eY_0nm72ZTZ"
      },
      "source": [
        "**3Ô∏è‚É£first()**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nN_kEudH4O6-"
      },
      "source": [
        "Ch·ªâ l·∫•y v·ªÅ 1 ph·∫ßn t·ª≠ m√† kh√¥ng c·∫ßn chuy·ªÉn to√†n b·ªô d·ªØ li·ªáu v·ªÅ driver, gi√∫p tr√°nh qu√° t·∫£i b·ªô nh·ªõ.\n",
        "\n",
        "N·∫øu RDD r·ªóng, ph∆∞∆°ng th·ª©c n√†y s·∫Ω n√©m ra l·ªói (exception)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "iInYad2pOyUf",
        "outputId": "d05041db-9c8b-4ce8-e289-3d63ffc87a9b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Scalar'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd_words.first()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT-zKWbUhkmr"
      },
      "source": [
        "**4Ô∏è‚É£take()**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZLwx1TUmeIh"
      },
      "source": [
        "`take(n)` tr·∫£ v·ªÅ m·ªôt danh s√°ch ch·ª©a n ph·∫ßn t·ª≠ ƒë·∫ßu ti√™n c·ªßa RDD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzo1mKWuhw2r",
        "outputId": "4cafe82c-d0dc-4af5-9daf-840187ee9648"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Scalar', 'java', 'hadoop', 'spark', 'akka']"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd_words.take(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnNgYwUM4LBu"
      },
      "source": [
        "Khi g·ªçi `take(n)`, Spark s·∫Ω duy·ªát qua c√°c partition c·ªßa RDD cho ƒë·∫øn khi thu th·∫≠p ƒë·ªß n ph·∫ßn t·ª≠. N·∫øu d·ªØ li·ªáu trong RDD √≠t h∆°n n, n√≥ s·∫Ω tr·∫£ v·ªÅ t·∫•t c·∫£ c√°c ph·∫ßn t·ª≠ c√≥ s·∫µn."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üî¥C√°c ph√©p bi·∫øn ƒë·ªïi (Transformations) tr√™n RDD**\n",
        "\n",
        "C√°c ph√©p bi·∫øn ƒë·ªïi tr√™n RDD t·∫°o ra RDD m·ªõi m√† kh√¥ng thay ƒë·ªïi RDD ban ƒë·∫ßu.\n",
        "\n",
        "M·ªôt s·ªë ph√©p bi·∫øn ƒë·ªïi ph·ªï bi·∫øn:\n",
        "\n",
        "- map(f): √Åp d·ª•ng m·ªôt h√†m f l√™n t·ª´ng ph·∫ßn t·ª≠.\n",
        "- filter(f): L·ªçc c√°c ph·∫ßn t·ª≠ th·ªèa m√£n ƒëi·ªÅu ki·ªán f.\n",
        "- flatMap(f): Gi·ªëng map(), nh∆∞ng m·ªü r·ªông k·∫øt qu·∫£ th√†nh nhi·ªÅu ph·∫ßn t·ª≠.\n",
        "- groupByKey(): Nh√≥m c√°c ph·∫ßn t·ª≠ c√≥ c√πng kh√≥a (key-value RDD).\n",
        "- reduceByKey(f): K·∫øt h·ª£p c√°c gi√° tr·ªã c√≥ c√πng kh√≥a b·∫±ng m·ªôt h√†m f."
      ],
      "metadata": {
        "id": "q_DgNcZ5oAQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**filter()**"
      ],
      "metadata": {
        "id": "fmUuvbkOoTVy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "filter nh·∫≠n v√†o m·ªôt h√†m ƒëi·ªÅu ki·ªán (predicate) v√† √°p d·ª•ng h√†m ƒë√≥ cho t·ª´ng ph·∫ßn t·ª≠ c·ªßa RDD.\n",
        "\n",
        "Ch·ªâ nh·ªØng ph·∫ßn t·ª≠ m√† h√†m tr·∫£ v·ªÅ True s·∫Ω ƒë∆∞·ª£c gi·ªØ l·∫°i trong RDD k·∫øt qu·∫£.\n",
        "\n",
        "RDD ban ƒë·∫ßu kh√¥ng b·ªã thay ƒë·ªïi. Thay v√†o ƒë√≥, filter t·∫°o ra m·ªôt RDD m·ªõi ch·ª©a c√°c ph·∫ßn t·ª≠ ƒë√£ ƒë∆∞·ª£c l·ªçc."
      ],
      "metadata": {
        "id": "0Wji1RB6oR3L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "H8LLtHhXmg4N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "84ec774e-97ef-4f8c-b21b-79cfc0a4dee6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'sc' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9bcac31d30d1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# T·∫°o RDD t·ª´ m·ªôt danh s√°ch c√°c s·ªë t·ª´ 1 ƒë·∫øn 10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnumbers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# S·ª≠ d·ª•ng filter ƒë·ªÉ ch·ªâ l·∫•y ra c√°c s·ªë ch·∫µn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0meven_numbers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
          ]
        }
      ],
      "source": [
        "# T·∫°o RDD t·ª´ m·ªôt danh s√°ch c√°c s·ªë t·ª´ 1 ƒë·∫øn 10\n",
        "numbers = sc.parallelize(range(1, 11))\n",
        "\n",
        "# S·ª≠ d·ª•ng filter ƒë·ªÉ ch·ªâ l·∫•y ra c√°c s·ªë ch·∫µn\n",
        "even_numbers = numbers.filter(lambda x: x % 2 == 0)\n",
        "\n",
        "# Thu th·∫≠p k·∫øt qu·∫£ v√† in ra\n",
        "print(even_numbers.collect())  # K·∫øt qu·∫£: [2, 4, 6, 8, 10]\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py3_9",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}