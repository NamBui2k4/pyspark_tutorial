{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKhANDPI2-YD"
      },
      "source": [
        "# Pyspark tutorial 🎯"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y9ghrep9ckQ"
      },
      "source": [
        "**Ghi chú: tutorial này được thực hiện trên google colab nên không bị lỗi khi chạy code. Nếu chạy trên jupyter notebook được host bởi local machine chắc chắn sẽ lỗi (chắc vì lý do cấu hình cài đặt hay gì đó :v). Mặc dù đã mò mẫm cả tuần nhưng mình vẫn chưa có giải pháp nào cho vấn đề này nên mình chấp nhận thực hiện nó trên google colab. 🤷‍♂️**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXX24zUvUB_y"
      },
      "source": [
        "\n",
        "![](https://th.bing.com/th/id/OIP.I3eg_GSGbjpQ0O8GDuHVdgHaFL?rs=1&pid=ImgDetMain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-Cj9XV-2-YF"
      },
      "source": [
        "## Installation ⬇️"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GFFs2FY2-YF"
      },
      "source": [
        "Để làm việc với spark, chúng ta cần cài đặt các thành phần sau:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "477uUkwe2-YF"
      },
      "source": [
        "- 📌 Java (OpenJDK 8 hoặc 11):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6iqBQ9FIG4Ed"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvamacLl2-YF"
      },
      "source": [
        "- 📌 Apache Spark\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rjp-b0FGrrA"
      },
      "outputs": [],
      "source": [
        "# tải gói spark apache về\n",
        "!wget https://downloads.apache.org/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz\n",
        "\n",
        "# giải nén\n",
        "!tar -xvzf /content/spark-3.5.4-bin-hadoop3.tgz\n",
        "\n",
        "# di chuyển Spark vào thư mục /opt/spark (đây là quy ước, tự tìm hiểu thêm)\n",
        "!sudo mv /content/spark-3.5.4-bin-hadoop3 /opt/spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQP7D00oGyR6"
      },
      "source": [
        "🔧 Thiết lập biến môi trường như sau:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RR3VHbS8Glrt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/opt/spark/spark-3.5.4-bin-hadoop3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z26vqdxlMLxp",
        "outputId": "42e55428-f9ce-4c7b-ea74-ade5459c76e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/opt/spark/spark-3.5.4-bin-hadoop3\n",
            "/usr/lib/jvm/java-8-openjdk-amd64\n"
          ]
        }
      ],
      "source": [
        "! echo $SPARK_HOME\n",
        "! echo $JAVA_HOME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-D7dd0y2-YG"
      },
      "source": [
        "- 📌 pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHxirntQ2-YG",
        "outputId": "ee4f6507-0d96-4944-930d-2dd8805f97c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.4)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i6VQ3wE2-YG"
      },
      "source": [
        "- 📌 findspark - tùy chọn, nếu chạy trên Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVLN3of82-YH",
        "outputId": "8fbb5bcd-f82d-4503-9e03-67c94e001210"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: findspark in /usr/local/lib/python3.11/dist-packages (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install findspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zwpg9kB1X0nv"
      },
      "source": [
        "## Run Spark app 🚀"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "IfPtbeXrZXHk"
      },
      "outputs": [],
      "source": [
        "# khởi tạo một file .py\n",
        "! touch firstapp.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmXe6eUfZxUq",
        "outputId": "6bd0b7f2-1929-476e-eeec-1ed2953329c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting firstapp.py\n"
          ]
        }
      ],
      "source": [
        "# ghi nội dung vào file\n",
        "# mới đầu đừng quan tâm nó code gì, chỉ biết nó chạy là được, biết thì càng tốt :V\n",
        "%%writefile firstapp.py\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"First Spark Application\") \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    data = [(\"Alice\", 1), (\"Bob\", 2)]\n",
        "    df = spark.createDataFrame(data, [\"Name\", \"Value\"])\n",
        "    df.show()\n",
        "\n",
        "    spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJVBudvWZe7K",
        "outputId": "ef7e9dd3-72b2-40c4-bcc4-0cc3ced80a25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "from pyspark.sql import SparkSession\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    spark = SparkSession.builder \\\n",
            "        .appName(\"First Spark Application\") \\\n",
            "        .master(\"local[*]\") \\\n",
            "        .getOrCreate()\n",
            "    \n",
            "    data = [(\"Alice\", 1), (\"Bob\", 2)]\n",
            "    df = spark.createDataFrame(data, [\"Name\", \"Value\"])\n",
            "    df.show()\n",
            "    \n",
            "    spark.stop()\n"
          ]
        }
      ],
      "source": [
        "# kiểm tra nội dung đã ghi vào\n",
        "! cat firstapp.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5qPDPiRaLdF",
        "outputId": "a645541f-83dd-4420-8b32-5cb18be80ab7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25/02/26 03:59:43 INFO SparkContext: Running Spark version 3.5.4\n",
            "25/02/26 03:59:43 INFO SparkContext: OS info Linux, 6.1.85+, amd64\n",
            "25/02/26 03:59:43 INFO SparkContext: Java version 1.8.0_442\n",
            "25/02/26 03:59:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/02/26 03:59:43 INFO ResourceUtils: ==============================================================\n",
            "25/02/26 03:59:43 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/02/26 03:59:43 INFO ResourceUtils: ==============================================================\n",
            "25/02/26 03:59:43 INFO SparkContext: Submitted application: First Spark Application\n",
            "25/02/26 03:59:43 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/02/26 03:59:43 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/02/26 03:59:43 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/02/26 03:59:43 INFO SecurityManager: Changing view acls to: root\n",
            "25/02/26 03:59:43 INFO SecurityManager: Changing modify acls to: root\n",
            "25/02/26 03:59:43 INFO SecurityManager: Changing view acls groups to: \n",
            "25/02/26 03:59:43 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/02/26 03:59:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/02/26 03:59:44 INFO Utils: Successfully started service 'sparkDriver' on port 33237.\n",
            "25/02/26 03:59:44 INFO SparkEnv: Registering MapOutputTracker\n",
            "25/02/26 03:59:44 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/02/26 03:59:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/02/26 03:59:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/02/26 03:59:44 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/02/26 03:59:44 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c2a396ca-3ee9-4cd5-ba30-482953232a63\n",
            "25/02/26 03:59:44 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "25/02/26 03:59:44 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/02/26 03:59:44 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/02/26 03:59:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "25/02/26 03:59:44 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "25/02/26 03:59:45 INFO Executor: Starting executor ID driver on host f304f0394c0e\n",
            "25/02/26 03:59:45 INFO Executor: OS info Linux, 6.1.85+, amd64\n",
            "25/02/26 03:59:45 INFO Executor: Java version 1.8.0_442\n",
            "25/02/26 03:59:45 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/02/26 03:59:45 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@46cd2861 for default.\n",
            "25/02/26 03:59:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37407.\n",
            "25/02/26 03:59:45 INFO NettyBlockTransferService: Server created on f304f0394c0e:37407\n",
            "25/02/26 03:59:45 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/02/26 03:59:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f304f0394c0e, 37407, None)\n",
            "25/02/26 03:59:45 INFO BlockManagerMasterEndpoint: Registering block manager f304f0394c0e:37407 with 366.3 MiB RAM, BlockManagerId(driver, f304f0394c0e, 37407, None)\n",
            "25/02/26 03:59:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f304f0394c0e, 37407, None)\n",
            "25/02/26 03:59:45 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f304f0394c0e, 37407, None)\n",
            "25/02/26 03:59:47 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/02/26 03:59:47 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/02/26 03:59:54 INFO CodeGenerator: Code generated in 448.415461 ms\n",
            "25/02/26 03:59:54 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
            "25/02/26 03:59:54 INFO DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "25/02/26 03:59:54 INFO DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)\n",
            "25/02/26 03:59:54 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/02/26 03:59:54 INFO DAGScheduler: Missing parents: List()\n",
            "25/02/26 03:59:54 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "25/02/26 03:59:55 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 12.7 KiB, free 366.3 MiB)\n",
            "25/02/26 03:59:55 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 366.3 MiB)\n",
            "25/02/26 03:59:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f304f0394c0e:37407 (size: 6.7 KiB, free: 366.3 MiB)\n",
            "25/02/26 03:59:55 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585\n",
            "25/02/26 03:59:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "25/02/26 03:59:55 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/02/26 03:59:55 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (f304f0394c0e, executor driver, partition 0, PROCESS_LOCAL, 9016 bytes) \n",
            "25/02/26 03:59:55 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/02/26 03:59:57 INFO CodeGenerator: Code generated in 31.744896 ms\n",
            "25/02/26 03:59:57 INFO PythonRunner: Times: total = 1191, boot = 980, init = 210, finish = 1\n",
            "25/02/26 03:59:57 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1971 bytes result sent to driver\n",
            "25/02/26 03:59:57 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2217 ms on f304f0394c0e (executor driver) (1/1)\n",
            "25/02/26 03:59:57 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 55067\n",
            "25/02/26 03:59:57 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/02/26 03:59:57 INFO DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 2.629 s\n",
            "25/02/26 03:59:57 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/02/26 03:59:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/02/26 03:59:57 INFO DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 2.771752 s\n",
            "25/02/26 03:59:57 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
            "25/02/26 03:59:57 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "25/02/26 03:59:57 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)\n",
            "25/02/26 03:59:57 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/02/26 03:59:57 INFO DAGScheduler: Missing parents: List()\n",
            "25/02/26 03:59:57 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "25/02/26 03:59:57 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.7 KiB, free 366.3 MiB)\n",
            "25/02/26 03:59:57 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 366.3 MiB)\n",
            "25/02/26 03:59:57 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f304f0394c0e:37407 (size: 6.7 KiB, free: 366.3 MiB)\n",
            "25/02/26 03:59:57 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
            "25/02/26 03:59:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1))\n",
            "25/02/26 03:59:57 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/02/26 03:59:57 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (f304f0394c0e, executor driver, partition 1, PROCESS_LOCAL, 9014 bytes) \n",
            "25/02/26 03:59:57 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/02/26 03:59:57 INFO PythonRunner: Times: total = 118, boot = -195, init = 313, finish = 0\n",
            "25/02/26 03:59:57 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1883 bytes result sent to driver\n",
            "25/02/26 03:59:57 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 153 ms on f304f0394c0e (executor driver) (1/1)\n",
            "25/02/26 03:59:57 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/02/26 03:59:57 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.175 s\n",
            "25/02/26 03:59:57 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/02/26 03:59:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/02/26 03:59:57 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.190003 s\n",
            "25/02/26 03:59:59 INFO CodeGenerator: Code generated in 36.980145 ms\n",
            "+-----+-----+\n",
            "| Name|Value|\n",
            "+-----+-----+\n",
            "|Alice|    1|\n",
            "|  Bob|    2|\n",
            "+-----+-----+\n",
            "\n",
            "25/02/26 03:59:59 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/02/26 03:59:59 INFO SparkUI: Stopped Spark web UI at http://f304f0394c0e:4041\n",
            "25/02/26 03:59:59 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/02/26 03:59:59 INFO MemoryStore: MemoryStore cleared\n",
            "25/02/26 03:59:59 INFO BlockManager: BlockManager stopped\n",
            "25/02/26 03:59:59 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/02/26 03:59:59 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/02/26 03:59:59 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/02/26 03:59:59 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/02/26 03:59:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-244ff210-2ed8-4fc1-81ea-17bc3268b330\n",
            "25/02/26 03:59:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-244ff210-2ed8-4fc1-81ea-17bc3268b330/pyspark-33ac738f-621e-46da-b03d-0ec7006b3021\n",
            "25/02/26 03:59:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-fc863893-7f92-4237-bf59-b693596749b6\n"
          ]
        }
      ],
      "source": [
        "# chạy file này\n",
        "! $SPARK_HOME/bin/spark-submit firstapp.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c4nXT46ah8F"
      },
      "source": [
        "Đã có một bảng với 2 cột (Name, Value) cùng với dòng là (Alice, 1) và (Bob, 2). Vậy là ứng dụng đã chạy thành công !\n",
        "\n",
        "Nếu chạy file .py theo cách bình thường thì mọi tài nguyên và tính toán sẽ được thực hiện trên local machine (trong trường hợp này là colab), nhưng lại bị giới hạn về khả năng xử lý phân tán.\n",
        "\n",
        "Lệnh `spark-submit` cho phép ta gỡ bỏ điều đó bằng cách \"submit\"  file .py của bạn đến cluster và  phân phối công việc cho các node trong cluster đó.\n",
        "\n",
        "Giờ thì ta sẽ đi sâu hơn vào từng phần của mã nguồn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8Jo6rOf2-YH"
      },
      "source": [
        "## SparkSession 🔥"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6VlbveA2-YH"
      },
      "source": [
        "Apache Spark là một hệ thống phân tán, chạy trên nhiều node. Để tương tác với spark, chúng ta cần một thứ gọi là \"điểm khởi đầu\" (Entry point). Trong trường hợp này chính là SparkSession. SparkSession giống như cửa chính của một tòa nhà, giúp bạn truy cập vào các thành phần bên trong như SparkContext, StreamContext, SQLContext,... (mấy cái này bàn sau)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64OXTUScUG-i"
      },
      "source": [
        "![](https://abhishekbaranwal10.files.wordpress.com/2018/09/introduction-to-apache-spark-20-12-638.jpg?resize=638%2C479&is-pending-load=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7YmADrJF2-YH"
      },
      "outputs": [],
      "source": [
        "# khởi tạo SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Datacamp Pyspark Tutorial\") \\\n",
        "    .config(\"spark.memory.offHeap.enabled\",\"true\") \\\n",
        "    .config(\"spark.memory.offHeap.size\",\"5g\").getOrCreate()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP4IdQzRCM60"
      },
      "source": [
        "Dừng SparkSession khi đã hoàn thành tất cả các tác vụ xử lý dữ liệu, nhằm giải phóng tài nguyên (như bộ nhớ, CPU và kết nối đến cluster)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CbkhNJQ52-YH"
      },
      "outputs": [],
      "source": [
        "# spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tnpHHwMAGS4"
      },
      "source": [
        "## SparkConf 🔥"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-inNEMrWAMzn"
      },
      "source": [
        "SparkConf là một class đặc biệt giúp chúng ta thay đổi lại cách khởi tạo session\n",
        "\n",
        "Ở phần trên, chúng ta khởi tạo một session thông qua một builder có sẵn và nó khá đơn giản. Nhưng nếu muốn tùy chỉnh cấu hình chi tiết (như bộ nhớ, số lượng core, v.v.) thì ta có cách sau:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0x72-FNBUE5",
        "outputId": "3d049e9f-fa43-4b17-9e21-064409c4af78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "some-value\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkConf\n",
        "\n",
        "\n",
        "# Tạo SparkConf với các cấu hình cần thiết\n",
        "conf = SparkConf() \\\n",
        "    .setAppName(\"MyApp\") \\\n",
        "    .setMaster(\"local[*]\") \\\n",
        "    .set(\"spark.some.config.option\", \"some-value\")\n",
        "\n",
        "# Tạo SparkSession bằng cách truyền SparkConf\n",
        "spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .config(conf=conf) \\\n",
        "        .getOrCreate()\n",
        "\n",
        "# Kiểm tra cấu hình từ SparkSession\n",
        "print(spark.conf.get(\"spark.some.config.option\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZistYlEcBieI"
      },
      "source": [
        "Đối với đa số ứng dụng, việc sử dụng trực tiếp builder của SparkSession đã đủ nên có thể bỏ qua bước này."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VvnNMdM2-YH"
      },
      "source": [
        "## SparkContext 🔥"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAuKglvr2-YH"
      },
      "source": [
        "🔹SparkContext là class cốt lõi trong Apache Spark, giúp ứng dụng kết nối với trình quản lý cluster - nơi quản lý các tác vụ tính toán của node.\n",
        "\n",
        "🔹SparkContext được sử dụng để khởi tạo các cấu trúc dữ liệu đặc biệt như RDD, accumulator, và broadcast variable (cái này sẽ nói sau)\n",
        "\n",
        "🔹Trước Spark phiên bản 2.x, SparkContext là entry point chính.\n",
        "\n",
        "🔹Từ Spark 2.x trở đi, SparkSession thay thế SparkContext, nhưng SparkContext vẫn tồn tại bên trong SparkSession."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ido4zEM78zm"
      },
      "source": [
        "![](https://sparkbyexamples.com/wp-content/uploads/2022/05/image04.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iS3Alc0L2-YH",
        "outputId": "f979b629-9041-44e3-8d73-87a94df3af3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SparkContext đã tồn tại, chỉ khởi tạo một lần. Đọc thêm ở bên dưới để biết thêm chi tiết\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Khởi tạo sparkContext\n",
        "\n",
        "try:\n",
        "  sc = SparkContext(\"local\", \"MyApp\")\n",
        "  print(sc)\n",
        "except:\n",
        "  print(\"SparkContext đã tồn tại, chỉ khởi tạo một lần. Đọc thêm ở bên dưới để biết thêm chi tiết\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "4gq70-rV2-YI",
        "outputId": "71dccf75-547a-40b7-fa84-fd46bb110f37"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://f304f0394c0e:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.4</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Datacamp Pyspark Tutorial</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ],
            "text/plain": [
              "<SparkContext master=local[*] appName=Datacamp Pyspark Tutorial>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# kiểm tra có SparkContext nào đang chạy không\n",
        "SparkContext._active_spark_context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS9Ns5_l2-YI"
      },
      "source": [
        "🔹Lưu ý: sparkContext chỉ khởi tạo một lần\n",
        "\n",
        "🔹Chạy lần 2 bị báo lỗi `Cannot run multiple SparkContexts at once; existing SparkContext`\n",
        "\n",
        "🔹Có thể khắc phục bằng 2 cách:\n",
        "    🔸Dừng sparkContext,\n",
        "    🔸Lấy sparkContext hiện có"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "PUeyajYs2-YI"
      },
      "outputs": [],
      "source": [
        "# Dừng SparkContext\n",
        "\n",
        "try:\n",
        "  # sc.stop()\n",
        "  # sc = SparkContext(\"local\", \"MyApp\")\n",
        "  print(sc)\n",
        "except:\n",
        "  print('Lưu ý: Không gọi sc.stop() nếu biến sc chưa tồn tại. Lúc đó phải chạy lệnh bên dưới')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Y93vaZH2-YI",
        "outputId": "97676211-e17d-48ca-c2b3-0407d0580198"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<SparkContext master=local[*] appName=Datacamp Pyspark Tutorial>\n"
          ]
        }
      ],
      "source": [
        "# Lấy SparkContext hiện có từ SparkSession\n",
        "\n",
        "try:\n",
        "  sc = spark.sparkContext\n",
        "  print(sc)\n",
        "except:\n",
        "  print('Chưa import spark phải không ? nhớ import nha :v')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rFZdy_I2-YI"
      },
      "source": [
        "## RDD 🔥\n",
        "\n",
        "RDD (Resilient Distributed Dataset) là cấu trúc dữ liệu cốt lõi của Apache Spark, cho phép xử lý dữ liệu phân tán trên các cluster một cách linh hoạt và hiệu quả."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OdUmVugmOk3"
      },
      "source": [
        "![](https://images.viblo.asia/2cd88166-3c16-4cdc-9298-ce9900ac1288.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmpiYtGcmU6z"
      },
      "source": [
        "🔴 **Các tính chất của RDD:**\n",
        "\n",
        "- Tính Phân tán (Distributed): Mỗi RDD được chia thành các phần nhỏ gọi là partitions, mỗi partition có thể được xử lý độc lập trên các node khác nhau trong 1 cluster\n",
        "\n",
        "- Tính Linh hoạt (Resilient): RDD có thể tự động phục hồi sau khi một phần của dữ liệu hoặc một phần của cluster bị lỗi.\n",
        "\n",
        "- Tính Bất biến : Sau khi được tạo, một RDD không thể thay đổi.\n",
        "\n",
        "- Đánh giá lười (Lazy Evaluation): Các phép biến đổi trên RDD không được thực hiện ngay lập tức mà chỉ khi có hành động (action) được gọi.\n",
        "\n",
        "- Tính tối ưu hóa (Optimized): RDDs có thể tối ưu hóa để tận dụng các hoạt động in-memory, giảm thiểu việc truy cập dữ liệu từ đĩa và tối ưu hóa việc chuyển dữ liệu giữa các phần của RDD trên cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzOKzzog2-YI"
      },
      "outputs": [],
      "source": [
        "words = [\n",
        "    'Scalar',\n",
        "    'java',\n",
        "    'hadoop',\n",
        "    'spark',\n",
        "    'akka',\n",
        "    'spark and hadoop',\n",
        "    'pyspark',\n",
        "    'pyspark and spark'\n",
        "]\n",
        "\n",
        "print(words)\n",
        "print(type(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ym4CpQyl2-YI"
      },
      "outputs": [],
      "source": [
        "rdd_words = sc.parallelize(words)\n",
        "print(type(rdd_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkV-sspxOyn7"
      },
      "source": [
        "**🔴 Các hành động (Actions) trên RDD**\n",
        "\n",
        "Các hành động thực thi tính toán trên RDD và trả về kết quả.\n",
        "Một số hành động phổ biến:\n",
        "\n",
        "- collect(): Lấy tất cả phần tử từ RDD về driver.\n",
        "- count(): Đếm số phần tử trong RDD.\n",
        "- first(): Lấy phần tử đầu tiên.\n",
        "- take(n): Lấy n phần tử đầu tiên.\n",
        "- reduce(f): Gộp các phần tử với một hàm f."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JEYDPTVkkEr"
      },
      "source": [
        "**1️⃣ collect()**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjsBmaJpk-S8"
      },
      "source": [
        "Thu thập toàn bộ dữ liệu từ các phân vùng của RDD trên các worker node và đưa về driver dưới dạng một danh sách Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S43jRCL5jtNG"
      },
      "outputs": [],
      "source": [
        "a = rdd_words.collect()\n",
        "print(a)\n",
        "print(type(a))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2O2WgUQlt7J"
      },
      "source": [
        "Khi gọi collect(), Spark sẽ thực hiện toàn bộ các phép biến đổi (transformation) đã được định nghĩa trên RDD để tạo ra dữ liệu cuối cùng.\n",
        "\n",
        "Lưu ý rằng nó có thể dễ dàng gây ra vấn đề bộ nhớ nếu dữ liệu quá lớn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR3p-dSxoits"
      },
      "source": [
        "**2️⃣count()**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUIn-tAOlrwq"
      },
      "source": [
        "Đếm số lượng phần tử có trong RDD và trả về kết quả dưới dạng một số nguyên (integer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsl6MInM2-YI"
      },
      "outputs": [],
      "source": [
        "print(rdd_words.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8veLylko0Ex"
      },
      "source": [
        "Khi gọi count(), tất cả các phép biến đổi (transformation) đã được định nghĩa trên RDD sẽ được thực hiện để tạo ra dữ liệu cuối cùng trước khi đếm số lượng phần tử.\n",
        "\n",
        "Không giống như collect, count chỉ trả về số đếm, do đó nó không gây ra quá tải bộ nhớ trên driver ngay cả khi RDD chứa rất nhiều phần tử."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eY_0nm72ZTZ"
      },
      "source": [
        "**3️⃣first()**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nN_kEudH4O6-"
      },
      "source": [
        "Chỉ lấy về 1 phần tử mà không cần chuyển toàn bộ dữ liệu về driver, giúp tránh quá tải bộ nhớ.\n",
        "\n",
        "Nếu RDD rỗng, phương thức này sẽ ném ra lỗi (exception)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iInYad2pOyUf"
      },
      "outputs": [],
      "source": [
        "rdd_words.first()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnNgYwUM4LBu"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "py3_9",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
