{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKhANDPI2-YD"
      },
      "source": [
        "# Pyspark tutorial üéØ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y9ghrep9ckQ"
      },
      "source": [
        "**Ghi ch√∫: tutorial n√†y ƒë∆∞·ª£c th·ª±c hi·ªán tr√™n google colab n√™n kh√¥ng b·ªã l·ªói khi ch·∫°y code. N·∫øu ch·∫°y tr√™n jupyter notebook ƒë∆∞·ª£c host b·ªüi local machine ch·∫Øc ch·∫Øn s·∫Ω l·ªói (ch·∫Øc v√¨ l√Ω do c·∫•u h√¨nh c√†i ƒë·∫∑t hay g√¨ ƒë√≥ :v). M·∫∑c d√π ƒë√£ m√≤ m·∫´m c·∫£ tu·∫ßn nh∆∞ng m√¨nh v·∫´n ch∆∞a c√≥ gi·∫£i ph√°p n√†o cho v·∫•n ƒë·ªÅ n√†y n√™n m√¨nh ch·∫•p nh·∫≠n th·ª±c hi·ªán n√≥ tr√™n google colab. ü§∑‚Äç‚ôÇÔ∏è**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXX24zUvUB_y"
      },
      "source": [
        "\n",
        "![](https://th.bing.com/th/id/OIP.I3eg_GSGbjpQ0O8GDuHVdgHaFL?rs=1&pid=ImgDetMain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-Cj9XV-2-YF"
      },
      "source": [
        "## Installation ‚¨áÔ∏è"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GFFs2FY2-YF"
      },
      "source": [
        "ƒê·ªÉ l√†m vi·ªác v·ªõi spark, ch√∫ng ta c·∫ßn c√†i ƒë·∫∑t c√°c th√†nh ph·∫ßn sau:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "477uUkwe2-YF"
      },
      "source": [
        "- üìå Java (OpenJDK 8 ho·∫∑c 11):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6iqBQ9FIG4Ed"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvamacLl2-YF"
      },
      "source": [
        "- üìå Apache Spark\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rjp-b0FGrrA"
      },
      "outputs": [],
      "source": [
        "# t·∫£i g√≥i spark apache v·ªÅ\n",
        "!wget https://downloads.apache.org/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz\n",
        "\n",
        "# gi·∫£i n√©n\n",
        "!tar -xvzf /content/spark-3.5.4-bin-hadoop3.tgz\n",
        "\n",
        "# di chuy·ªÉn Spark v√†o th∆∞ m·ª•c /opt/spark (ƒë√¢y l√† quy ∆∞·ªõc, t·ª± t√¨m hi·ªÉu th√™m)\n",
        "!sudo mv /content/spark-3.5.4-bin-hadoop3 /opt/spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQP7D00oGyR6"
      },
      "source": [
        "üîß Thi·∫øt l·∫≠p bi·∫øn m√¥i tr∆∞·ªùng nh∆∞ sau:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RR3VHbS8Glrt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/opt/spark/spark-3.5.4-bin-hadoop3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z26vqdxlMLxp",
        "outputId": "42e55428-f9ce-4c7b-ea74-ade5459c76e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/opt/spark/spark-3.5.4-bin-hadoop3\n",
            "/usr/lib/jvm/java-8-openjdk-amd64\n"
          ]
        }
      ],
      "source": [
        "! echo $SPARK_HOME\n",
        "! echo $JAVA_HOME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-D7dd0y2-YG"
      },
      "source": [
        "- üìå pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHxirntQ2-YG",
        "outputId": "ee4f6507-0d96-4944-930d-2dd8805f97c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.4)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i6VQ3wE2-YG"
      },
      "source": [
        "- üìå findspark - t√πy ch·ªçn, n·∫øu ch·∫°y tr√™n Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVLN3of82-YH",
        "outputId": "8fbb5bcd-f82d-4503-9e03-67c94e001210"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: findspark in /usr/local/lib/python3.11/dist-packages (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install findspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zwpg9kB1X0nv"
      },
      "source": [
        "## Run Spark app üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "IfPtbeXrZXHk"
      },
      "outputs": [],
      "source": [
        "# kh·ªüi t·∫°o m·ªôt file .py\n",
        "! touch firstapp.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmXe6eUfZxUq",
        "outputId": "6bd0b7f2-1929-476e-eeec-1ed2953329c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting firstapp.py\n"
          ]
        }
      ],
      "source": [
        "# ghi n·ªôi dung v√†o file\n",
        "# m·ªõi ƒë·∫ßu ƒë·ª´ng quan t√¢m n√≥ code g√¨, ch·ªâ bi·∫øt n√≥ ch·∫°y l√† ƒë∆∞·ª£c, bi·∫øt th√¨ c√†ng t·ªët :V\n",
        "%%writefile firstapp.py\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"First Spark Application\") \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    data = [(\"Alice\", 1), (\"Bob\", 2)]\n",
        "    df = spark.createDataFrame(data, [\"Name\", \"Value\"])\n",
        "    df.show()\n",
        "\n",
        "    spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJVBudvWZe7K",
        "outputId": "ef7e9dd3-72b2-40c4-bcc4-0cc3ced80a25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "from pyspark.sql import SparkSession\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    spark = SparkSession.builder \\\n",
            "        .appName(\"First Spark Application\") \\\n",
            "        .master(\"local[*]\") \\\n",
            "        .getOrCreate()\n",
            "    \n",
            "    data = [(\"Alice\", 1), (\"Bob\", 2)]\n",
            "    df = spark.createDataFrame(data, [\"Name\", \"Value\"])\n",
            "    df.show()\n",
            "    \n",
            "    spark.stop()\n"
          ]
        }
      ],
      "source": [
        "# ki·ªÉm tra n·ªôi dung ƒë√£ ghi v√†o\n",
        "! cat firstapp.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5qPDPiRaLdF",
        "outputId": "a645541f-83dd-4420-8b32-5cb18be80ab7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25/02/26 03:59:43 INFO SparkContext: Running Spark version 3.5.4\n",
            "25/02/26 03:59:43 INFO SparkContext: OS info Linux, 6.1.85+, amd64\n",
            "25/02/26 03:59:43 INFO SparkContext: Java version 1.8.0_442\n",
            "25/02/26 03:59:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "25/02/26 03:59:43 INFO ResourceUtils: ==============================================================\n",
            "25/02/26 03:59:43 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "25/02/26 03:59:43 INFO ResourceUtils: ==============================================================\n",
            "25/02/26 03:59:43 INFO SparkContext: Submitted application: First Spark Application\n",
            "25/02/26 03:59:43 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "25/02/26 03:59:43 INFO ResourceProfile: Limiting resource is cpu\n",
            "25/02/26 03:59:43 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "25/02/26 03:59:43 INFO SecurityManager: Changing view acls to: root\n",
            "25/02/26 03:59:43 INFO SecurityManager: Changing modify acls to: root\n",
            "25/02/26 03:59:43 INFO SecurityManager: Changing view acls groups to: \n",
            "25/02/26 03:59:43 INFO SecurityManager: Changing modify acls groups to: \n",
            "25/02/26 03:59:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "25/02/26 03:59:44 INFO Utils: Successfully started service 'sparkDriver' on port 33237.\n",
            "25/02/26 03:59:44 INFO SparkEnv: Registering MapOutputTracker\n",
            "25/02/26 03:59:44 INFO SparkEnv: Registering BlockManagerMaster\n",
            "25/02/26 03:59:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "25/02/26 03:59:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "25/02/26 03:59:44 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "25/02/26 03:59:44 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c2a396ca-3ee9-4cd5-ba30-482953232a63\n",
            "25/02/26 03:59:44 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
            "25/02/26 03:59:44 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "25/02/26 03:59:44 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "25/02/26 03:59:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "25/02/26 03:59:44 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
            "25/02/26 03:59:45 INFO Executor: Starting executor ID driver on host f304f0394c0e\n",
            "25/02/26 03:59:45 INFO Executor: OS info Linux, 6.1.85+, amd64\n",
            "25/02/26 03:59:45 INFO Executor: Java version 1.8.0_442\n",
            "25/02/26 03:59:45 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "25/02/26 03:59:45 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@46cd2861 for default.\n",
            "25/02/26 03:59:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37407.\n",
            "25/02/26 03:59:45 INFO NettyBlockTransferService: Server created on f304f0394c0e:37407\n",
            "25/02/26 03:59:45 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "25/02/26 03:59:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f304f0394c0e, 37407, None)\n",
            "25/02/26 03:59:45 INFO BlockManagerMasterEndpoint: Registering block manager f304f0394c0e:37407 with 366.3 MiB RAM, BlockManagerId(driver, f304f0394c0e, 37407, None)\n",
            "25/02/26 03:59:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f304f0394c0e, 37407, None)\n",
            "25/02/26 03:59:45 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f304f0394c0e, 37407, None)\n",
            "25/02/26 03:59:47 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "25/02/26 03:59:47 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "25/02/26 03:59:54 INFO CodeGenerator: Code generated in 448.415461 ms\n",
            "25/02/26 03:59:54 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
            "25/02/26 03:59:54 INFO DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "25/02/26 03:59:54 INFO DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)\n",
            "25/02/26 03:59:54 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/02/26 03:59:54 INFO DAGScheduler: Missing parents: List()\n",
            "25/02/26 03:59:54 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "25/02/26 03:59:55 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 12.7 KiB, free 366.3 MiB)\n",
            "25/02/26 03:59:55 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 366.3 MiB)\n",
            "25/02/26 03:59:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f304f0394c0e:37407 (size: 6.7 KiB, free: 366.3 MiB)\n",
            "25/02/26 03:59:55 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585\n",
            "25/02/26 03:59:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "25/02/26 03:59:55 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "25/02/26 03:59:55 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (f304f0394c0e, executor driver, partition 0, PROCESS_LOCAL, 9016 bytes) \n",
            "25/02/26 03:59:55 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "25/02/26 03:59:57 INFO CodeGenerator: Code generated in 31.744896 ms\n",
            "25/02/26 03:59:57 INFO PythonRunner: Times: total = 1191, boot = 980, init = 210, finish = 1\n",
            "25/02/26 03:59:57 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1971 bytes result sent to driver\n",
            "25/02/26 03:59:57 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2217 ms on f304f0394c0e (executor driver) (1/1)\n",
            "25/02/26 03:59:57 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 55067\n",
            "25/02/26 03:59:57 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "25/02/26 03:59:57 INFO DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 2.629 s\n",
            "25/02/26 03:59:57 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/02/26 03:59:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "25/02/26 03:59:57 INFO DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 2.771752 s\n",
            "25/02/26 03:59:57 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
            "25/02/26 03:59:57 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "25/02/26 03:59:57 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)\n",
            "25/02/26 03:59:57 INFO DAGScheduler: Parents of final stage: List()\n",
            "25/02/26 03:59:57 INFO DAGScheduler: Missing parents: List()\n",
            "25/02/26 03:59:57 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "25/02/26 03:59:57 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.7 KiB, free 366.3 MiB)\n",
            "25/02/26 03:59:57 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 366.3 MiB)\n",
            "25/02/26 03:59:57 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f304f0394c0e:37407 (size: 6.7 KiB, free: 366.3 MiB)\n",
            "25/02/26 03:59:57 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
            "25/02/26 03:59:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1))\n",
            "25/02/26 03:59:57 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
            "25/02/26 03:59:57 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (f304f0394c0e, executor driver, partition 1, PROCESS_LOCAL, 9014 bytes) \n",
            "25/02/26 03:59:57 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
            "25/02/26 03:59:57 INFO PythonRunner: Times: total = 118, boot = -195, init = 313, finish = 0\n",
            "25/02/26 03:59:57 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1883 bytes result sent to driver\n",
            "25/02/26 03:59:57 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 153 ms on f304f0394c0e (executor driver) (1/1)\n",
            "25/02/26 03:59:57 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
            "25/02/26 03:59:57 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.175 s\n",
            "25/02/26 03:59:57 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "25/02/26 03:59:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
            "25/02/26 03:59:57 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.190003 s\n",
            "25/02/26 03:59:59 INFO CodeGenerator: Code generated in 36.980145 ms\n",
            "+-----+-----+\n",
            "| Name|Value|\n",
            "+-----+-----+\n",
            "|Alice|    1|\n",
            "|  Bob|    2|\n",
            "+-----+-----+\n",
            "\n",
            "25/02/26 03:59:59 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "25/02/26 03:59:59 INFO SparkUI: Stopped Spark web UI at http://f304f0394c0e:4041\n",
            "25/02/26 03:59:59 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "25/02/26 03:59:59 INFO MemoryStore: MemoryStore cleared\n",
            "25/02/26 03:59:59 INFO BlockManager: BlockManager stopped\n",
            "25/02/26 03:59:59 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "25/02/26 03:59:59 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "25/02/26 03:59:59 INFO SparkContext: Successfully stopped SparkContext\n",
            "25/02/26 03:59:59 INFO ShutdownHookManager: Shutdown hook called\n",
            "25/02/26 03:59:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-244ff210-2ed8-4fc1-81ea-17bc3268b330\n",
            "25/02/26 03:59:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-244ff210-2ed8-4fc1-81ea-17bc3268b330/pyspark-33ac738f-621e-46da-b03d-0ec7006b3021\n",
            "25/02/26 03:59:59 INFO ShutdownHookManager: Deleting directory /tmp/spark-fc863893-7f92-4237-bf59-b693596749b6\n"
          ]
        }
      ],
      "source": [
        "# ch·∫°y file n√†y\n",
        "! $SPARK_HOME/bin/spark-submit firstapp.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c4nXT46ah8F"
      },
      "source": [
        "ƒê√£ c√≥ m·ªôt b·∫£ng v·ªõi 2 c·ªôt (Name, Value) c√πng v·ªõi d√≤ng l√† (Alice, 1) v√† (Bob, 2). V·∫≠y l√† ·ª©ng d·ª•ng ƒë√£ ch·∫°y th√†nh c√¥ng !\n",
        "\n",
        "N·∫øu ch·∫°y file .py theo c√°ch b√¨nh th∆∞·ªùng th√¨ m·ªçi t√†i nguy√™n v√† t√≠nh to√°n s·∫Ω ƒë∆∞·ª£c th·ª±c hi·ªán tr√™n local machine (trong tr∆∞·ªùng h·ª£p n√†y l√† colab), nh∆∞ng l·∫°i b·ªã gi·ªõi h·∫°n v·ªÅ kh·∫£ nƒÉng x·ª≠ l√Ω ph√¢n t√°n.\n",
        "\n",
        "L·ªánh `spark-submit` cho ph√©p ta g·ª° b·ªè ƒëi·ªÅu ƒë√≥ b·∫±ng c√°ch \"submit\"  file .py c·ªßa b·∫°n ƒë·∫øn cluster v√†  ph√¢n ph·ªëi c√¥ng vi·ªác cho c√°c node trong cluster ƒë√≥.\n",
        "\n",
        "Gi·ªù th√¨ ta s·∫Ω ƒëi s√¢u h∆°n v√†o t·ª´ng ph·∫ßn c·ªßa m√£ ngu·ªìn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8Jo6rOf2-YH"
      },
      "source": [
        "## SparkSession üî•"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6VlbveA2-YH"
      },
      "source": [
        "Apache Spark l√† m·ªôt h·ªá th·ªëng ph√¢n t√°n, ch·∫°y tr√™n nhi·ªÅu node. ƒê·ªÉ t∆∞∆°ng t√°c v·ªõi spark, ch√∫ng ta c·∫ßn m·ªôt th·ª© g·ªçi l√† \"ƒëi·ªÉm kh·ªüi ƒë·∫ßu\" (Entry point). Trong tr∆∞·ªùng h·ª£p n√†y ch√≠nh l√† SparkSession. SparkSession gi·ªëng nh∆∞ c·ª≠a ch√≠nh c·ªßa m·ªôt t√≤a nh√†, gi√∫p b·∫°n truy c·∫≠p v√†o c√°c th√†nh ph·∫ßn b√™n trong nh∆∞ SparkContext, StreamContext, SQLContext,... (m·∫•y c√°i n√†y b√†n sau)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64OXTUScUG-i"
      },
      "source": [
        "![](https://abhishekbaranwal10.files.wordpress.com/2018/09/introduction-to-apache-spark-20-12-638.jpg?resize=638%2C479&is-pending-load=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7YmADrJF2-YH"
      },
      "outputs": [],
      "source": [
        "# kh·ªüi t·∫°o SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Datacamp Pyspark Tutorial\") \\\n",
        "    .config(\"spark.memory.offHeap.enabled\",\"true\") \\\n",
        "    .config(\"spark.memory.offHeap.size\",\"5g\").getOrCreate()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP4IdQzRCM60"
      },
      "source": [
        "D·ª´ng SparkSession khi ƒë√£ ho√†n th√†nh t·∫•t c·∫£ c√°c t√°c v·ª• x·ª≠ l√Ω d·ªØ li·ªáu, nh·∫±m gi·∫£i ph√≥ng t√†i nguy√™n (nh∆∞ b·ªô nh·ªõ, CPU v√† k·∫øt n·ªëi ƒë·∫øn cluster)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CbkhNJQ52-YH"
      },
      "outputs": [],
      "source": [
        "# spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tnpHHwMAGS4"
      },
      "source": [
        "## SparkConf üî•"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-inNEMrWAMzn"
      },
      "source": [
        "SparkConf l√† m·ªôt class ƒë·∫∑c bi·ªát gi√∫p ch√∫ng ta thay ƒë·ªïi l·∫°i c√°ch kh·ªüi t·∫°o session\n",
        "\n",
        "·ªû ph·∫ßn tr√™n, ch√∫ng ta kh·ªüi t·∫°o m·ªôt session th√¥ng qua m·ªôt builder c√≥ s·∫µn v√† n√≥ kh√° ƒë∆°n gi·∫£n. Nh∆∞ng n·∫øu mu·ªën t√πy ch·ªânh c·∫•u h√¨nh chi ti·∫øt (nh∆∞ b·ªô nh·ªõ, s·ªë l∆∞·ª£ng core, v.v.) th√¨ ta c√≥ c√°ch sau:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0x72-FNBUE5",
        "outputId": "3d049e9f-fa43-4b17-9e21-064409c4af78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "some-value\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkConf\n",
        "\n",
        "\n",
        "# T·∫°o SparkConf v·ªõi c√°c c·∫•u h√¨nh c·∫ßn thi·∫øt\n",
        "conf = SparkConf() \\\n",
        "    .setAppName(\"MyApp\") \\\n",
        "    .setMaster(\"local[*]\") \\\n",
        "    .set(\"spark.some.config.option\", \"some-value\")\n",
        "\n",
        "# T·∫°o SparkSession b·∫±ng c√°ch truy·ªÅn SparkConf\n",
        "spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .config(conf=conf) \\\n",
        "        .getOrCreate()\n",
        "\n",
        "# Ki·ªÉm tra c·∫•u h√¨nh t·ª´ SparkSession\n",
        "print(spark.conf.get(\"spark.some.config.option\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZistYlEcBieI"
      },
      "source": [
        "ƒê·ªëi v·ªõi ƒëa s·ªë ·ª©ng d·ª•ng, vi·ªác s·ª≠ d·ª•ng tr·ª±c ti·∫øp builder c·ªßa SparkSession ƒë√£ ƒë·ªß n√™n c√≥ th·ªÉ b·ªè qua b∆∞·ªõc n√†y."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VvnNMdM2-YH"
      },
      "source": [
        "## SparkContext üî•"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAuKglvr2-YH"
      },
      "source": [
        "üîπSparkContext l√† class c·ªët l√µi trong Apache Spark, gi√∫p ·ª©ng d·ª•ng k·∫øt n·ªëi v·ªõi tr√¨nh qu·∫£n l√Ω cluster - n∆°i qu·∫£n l√Ω c√°c t√°c v·ª• t√≠nh to√°n c·ªßa node.\n",
        "\n",
        "üîπSparkContext ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ kh·ªüi t·∫°o c√°c c·∫•u tr√∫c d·ªØ li·ªáu ƒë·∫∑c bi·ªát nh∆∞ RDD, accumulator, v√† broadcast variable (c√°i n√†y s·∫Ω n√≥i sau)\n",
        "\n",
        "üîπTr∆∞·ªõc Spark phi√™n b·∫£n 2.x, SparkContext l√† entry point ch√≠nh.\n",
        "\n",
        "üîπT·ª´ Spark 2.x tr·ªü ƒëi, SparkSession thay th·∫ø SparkContext, nh∆∞ng SparkContext v·∫´n t·ªìn t·∫°i b√™n trong SparkSession."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ido4zEM78zm"
      },
      "source": [
        "![](https://sparkbyexamples.com/wp-content/uploads/2022/05/image04.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iS3Alc0L2-YH",
        "outputId": "f979b629-9041-44e3-8d73-87a94df3af3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SparkContext ƒë√£ t·ªìn t·∫°i, ch·ªâ kh·ªüi t·∫°o m·ªôt l·∫ßn. ƒê·ªçc th√™m ·ªü b√™n d∆∞·ªõi ƒë·ªÉ bi·∫øt th√™m chi ti·∫øt\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Kh·ªüi t·∫°o sparkContext\n",
        "\n",
        "try:\n",
        "  sc = SparkContext(\"local\", \"MyApp\")\n",
        "  print(sc)\n",
        "except:\n",
        "  print(\"SparkContext ƒë√£ t·ªìn t·∫°i, ch·ªâ kh·ªüi t·∫°o m·ªôt l·∫ßn. ƒê·ªçc th√™m ·ªü b√™n d∆∞·ªõi ƒë·ªÉ bi·∫øt th√™m chi ti·∫øt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "4gq70-rV2-YI",
        "outputId": "71dccf75-547a-40b7-fa84-fd46bb110f37"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://f304f0394c0e:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.4</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Datacamp Pyspark Tutorial</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ],
            "text/plain": [
              "<SparkContext master=local[*] appName=Datacamp Pyspark Tutorial>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ki·ªÉm tra c√≥ SparkContext n√†o ƒëang ch·∫°y kh√¥ng\n",
        "SparkContext._active_spark_context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS9Ns5_l2-YI"
      },
      "source": [
        "üîπL∆∞u √Ω: sparkContext ch·ªâ kh·ªüi t·∫°o m·ªôt l·∫ßn\n",
        "\n",
        "üîπCh·∫°y l·∫ßn 2 b·ªã b√°o l·ªói `Cannot run multiple SparkContexts at once; existing SparkContext`\n",
        "\n",
        "üîπC√≥ th·ªÉ kh·∫Øc ph·ª•c b·∫±ng 2 c√°ch:\n",
        "    üî∏D·ª´ng sparkContext,\n",
        "    üî∏L·∫•y sparkContext hi·ªán c√≥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "PUeyajYs2-YI"
      },
      "outputs": [],
      "source": [
        "# D·ª´ng SparkContext\n",
        "\n",
        "try:\n",
        "  # sc.stop()\n",
        "  # sc = SparkContext(\"local\", \"MyApp\")\n",
        "  print(sc)\n",
        "except:\n",
        "  print('L∆∞u √Ω: Kh√¥ng g·ªçi sc.stop() n·∫øu bi·∫øn sc ch∆∞a t·ªìn t·∫°i. L√∫c ƒë√≥ ph·∫£i ch·∫°y l·ªánh b√™n d∆∞·ªõi')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Y93vaZH2-YI",
        "outputId": "97676211-e17d-48ca-c2b3-0407d0580198"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<SparkContext master=local[*] appName=Datacamp Pyspark Tutorial>\n"
          ]
        }
      ],
      "source": [
        "# L·∫•y SparkContext hi·ªán c√≥ t·ª´ SparkSession\n",
        "\n",
        "try:\n",
        "  sc = spark.sparkContext\n",
        "  print(sc)\n",
        "except:\n",
        "  print('Ch∆∞a import spark ph·∫£i kh√¥ng ? nh·ªõ import nha :v')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rFZdy_I2-YI"
      },
      "source": [
        "## RDD üî•\n",
        "\n",
        "RDD (Resilient Distributed Dataset) l√† c·∫•u tr√∫c d·ªØ li·ªáu c·ªët l√µi c·ªßa Apache Spark, cho ph√©p x·ª≠ l√Ω d·ªØ li·ªáu ph√¢n t√°n tr√™n c√°c cluster m·ªôt c√°ch linh ho·∫°t v√† hi·ªáu qu·∫£."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OdUmVugmOk3"
      },
      "source": [
        "![](https://images.viblo.asia/2cd88166-3c16-4cdc-9298-ce9900ac1288.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmpiYtGcmU6z"
      },
      "source": [
        "üî¥ **C√°c t√≠nh ch·∫•t c·ªßa RDD:**\n",
        "\n",
        "- T√≠nh Ph√¢n t√°n (Distributed): M·ªói RDD ƒë∆∞·ª£c chia th√†nh c√°c ph·∫ßn nh·ªè g·ªçi l√† partitions, m·ªói partition c√≥ th·ªÉ ƒë∆∞·ª£c x·ª≠ l√Ω ƒë·ªôc l·∫≠p tr√™n c√°c node kh√°c nhau trong 1 cluster\n",
        "\n",
        "- T√≠nh Linh ho·∫°t (Resilient): RDD c√≥ th·ªÉ t·ª± ƒë·ªông ph·ª•c h·ªìi sau khi m·ªôt ph·∫ßn c·ªßa d·ªØ li·ªáu ho·∫∑c m·ªôt ph·∫ßn c·ªßa cluster b·ªã l·ªói.\n",
        "\n",
        "- T√≠nh B·∫•t bi·∫øn : Sau khi ƒë∆∞·ª£c t·∫°o, m·ªôt RDD kh√¥ng th·ªÉ thay ƒë·ªïi.\n",
        "\n",
        "- ƒê√°nh gi√° l∆∞·ªùi (Lazy Evaluation): C√°c ph√©p bi·∫øn ƒë·ªïi tr√™n RDD kh√¥ng ƒë∆∞·ª£c th·ª±c hi·ªán ngay l·∫≠p t·ª©c m√† ch·ªâ khi c√≥ h√†nh ƒë·ªông (action) ƒë∆∞·ª£c g·ªçi.\n",
        "\n",
        "- T√≠nh t·ªëi ∆∞u h√≥a (Optimized): RDDs c√≥ th·ªÉ t·ªëi ∆∞u h√≥a ƒë·ªÉ t·∫≠n d·ª•ng c√°c ho·∫°t ƒë·ªông in-memory, gi·∫£m thi·ªÉu vi·ªác truy c·∫≠p d·ªØ li·ªáu t·ª´ ƒëƒ©a v√† t·ªëi ∆∞u h√≥a vi·ªác chuy·ªÉn d·ªØ li·ªáu gi·ªØa c√°c ph·∫ßn c·ªßa RDD tr√™n cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzOKzzog2-YI"
      },
      "outputs": [],
      "source": [
        "words = [\n",
        "    'Scalar',\n",
        "    'java',\n",
        "    'hadoop',\n",
        "    'spark',\n",
        "    'akka',\n",
        "    'spark and hadoop',\n",
        "    'pyspark',\n",
        "    'pyspark and spark'\n",
        "]\n",
        "\n",
        "print(words)\n",
        "print(type(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ym4CpQyl2-YI"
      },
      "outputs": [],
      "source": [
        "rdd_words = sc.parallelize(words)\n",
        "print(type(rdd_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkV-sspxOyn7"
      },
      "source": [
        "**üî¥ C√°c h√†nh ƒë·ªông (Actions) tr√™n RDD**\n",
        "\n",
        "C√°c h√†nh ƒë·ªông th·ª±c thi t√≠nh to√°n tr√™n RDD v√† tr·∫£ v·ªÅ k·∫øt qu·∫£.\n",
        "M·ªôt s·ªë h√†nh ƒë·ªông ph·ªï bi·∫øn:\n",
        "\n",
        "- collect(): L·∫•y t·∫•t c·∫£ ph·∫ßn t·ª≠ t·ª´ RDD v·ªÅ driver.\n",
        "- count(): ƒê·∫øm s·ªë ph·∫ßn t·ª≠ trong RDD.\n",
        "- first(): L·∫•y ph·∫ßn t·ª≠ ƒë·∫ßu ti√™n.\n",
        "- take(n): L·∫•y n ph·∫ßn t·ª≠ ƒë·∫ßu ti√™n.\n",
        "- reduce(f): G·ªôp c√°c ph·∫ßn t·ª≠ v·ªõi m·ªôt h√†m f."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JEYDPTVkkEr"
      },
      "source": [
        "**1Ô∏è‚É£ collect()**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjsBmaJpk-S8"
      },
      "source": [
        "Thu th·∫≠p to√†n b·ªô d·ªØ li·ªáu t·ª´ c√°c ph√¢n v√πng c·ªßa RDD tr√™n c√°c worker node v√† ƒë∆∞a v·ªÅ driver d∆∞·ªõi d·∫°ng m·ªôt danh s√°ch Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S43jRCL5jtNG"
      },
      "outputs": [],
      "source": [
        "a = rdd_words.collect()\n",
        "print(a)\n",
        "print(type(a))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2O2WgUQlt7J"
      },
      "source": [
        "Khi g·ªçi collect(), Spark s·∫Ω th·ª±c hi·ªán to√†n b·ªô c√°c ph√©p bi·∫øn ƒë·ªïi (transformation) ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a tr√™n RDD ƒë·ªÉ t·∫°o ra d·ªØ li·ªáu cu·ªëi c√πng.\n",
        "\n",
        "L∆∞u √Ω r·∫±ng n√≥ c√≥ th·ªÉ d·ªÖ d√†ng g√¢y ra v·∫•n ƒë·ªÅ b·ªô nh·ªõ n·∫øu d·ªØ li·ªáu qu√° l·ªõn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR3p-dSxoits"
      },
      "source": [
        "**2Ô∏è‚É£count()**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUIn-tAOlrwq"
      },
      "source": [
        "ƒê·∫øm s·ªë l∆∞·ª£ng ph·∫ßn t·ª≠ c√≥ trong RDD v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ d∆∞·ªõi d·∫°ng m·ªôt s·ªë nguy√™n (integer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsl6MInM2-YI"
      },
      "outputs": [],
      "source": [
        "print(rdd_words.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8veLylko0Ex"
      },
      "source": [
        "Khi g·ªçi count(), t·∫•t c·∫£ c√°c ph√©p bi·∫øn ƒë·ªïi (transformation) ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a tr√™n RDD s·∫Ω ƒë∆∞·ª£c th·ª±c hi·ªán ƒë·ªÉ t·∫°o ra d·ªØ li·ªáu cu·ªëi c√πng tr∆∞·ªõc khi ƒë·∫øm s·ªë l∆∞·ª£ng ph·∫ßn t·ª≠.\n",
        "\n",
        "Kh√¥ng gi·ªëng nh∆∞ collect, count ch·ªâ tr·∫£ v·ªÅ s·ªë ƒë·∫øm, do ƒë√≥ n√≥ kh√¥ng g√¢y ra qu√° t·∫£i b·ªô nh·ªõ tr√™n driver ngay c·∫£ khi RDD ch·ª©a r·∫•t nhi·ªÅu ph·∫ßn t·ª≠."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eY_0nm72ZTZ"
      },
      "source": [
        "**3Ô∏è‚É£first()**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nN_kEudH4O6-"
      },
      "source": [
        "Ch·ªâ l·∫•y v·ªÅ 1 ph·∫ßn t·ª≠ m√† kh√¥ng c·∫ßn chuy·ªÉn to√†n b·ªô d·ªØ li·ªáu v·ªÅ driver, gi√∫p tr√°nh qu√° t·∫£i b·ªô nh·ªõ.\n",
        "\n",
        "N·∫øu RDD r·ªóng, ph∆∞∆°ng th·ª©c n√†y s·∫Ω n√©m ra l·ªói (exception)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iInYad2pOyUf"
      },
      "outputs": [],
      "source": [
        "rdd_words.first()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnNgYwUM4LBu"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "py3_9",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
