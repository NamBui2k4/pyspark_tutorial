{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKhANDPI2-YD"
      },
      "source": [
        "# Pyspark tutorial 🎯"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y9ghrep9ckQ"
      },
      "source": [
        "**Ghi chú:**\n",
        " \n",
        " 🔹Tutorial này được thực hiện trên google colab nên ít bị lỗi. Còn mà chạy trên jupyter notebook được host bởi local machine thì sẽ lỗi tùm lum ( mình đã từng bị rồi 🥲) nên mình khuyến khích mọi người dùng colab. \n",
        "\n",
        " 🔹Khuyến khích thêm là nhớ tải về vscode rồi xem lại nhé, github hiển thị giao diện xấu lắm.\n",
        "\n",
        " 🔹Thêm nữa, mình làm tutorial này dựa trên nhiều nguồn, trong đó có các blog mà mình kham khảo. Sau đây xin cảm ơn blog của [longcnttbkhn](https://longcnttbkhn.github.io/huong-dan-spark-co-ban-cho-nguoi-moi/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXX24zUvUB_y"
      },
      "source": [
        "\n",
        "![](https://th.bing.com/th/id/OIP.I3eg_GSGbjpQ0O8GDuHVdgHaFL?rs=1&pid=ImgDetMain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-Cj9XV-2-YF"
      },
      "source": [
        "## Installation ⬇️"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GFFs2FY2-YF"
      },
      "source": [
        "Để làm việc với spark, chúng ta cần cài đặt các thành phần sau:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "477uUkwe2-YF"
      },
      "source": [
        "- 📌 Java (OpenJDK 8 hoặc 11):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6iqBQ9FIG4Ed"
      },
      "outputs": [],
      "source": [
        "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvamacLl2-YF"
      },
      "source": [
        "- 📌 Apache Spark\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-rjp-b0FGrrA"
      },
      "outputs": [],
      "source": [
        "# # tải gói spark apache về\n",
        "# !wget https://dlcdn.apache.org/spark/spark-3.5.5/spark-3.5.5-bin-hadoop3.tgz\n",
        "\n",
        "# # giải nén\n",
        "# !tar -xvzf /content/spark-3.5.5-bin-hadoop3.tgz\n",
        "\n",
        "# # di chuyển Spark vào thư mục /opt/spark (đây là quy ước, tự tìm hiểu thêm)\n",
        "# !sudo mv /content/spark-3.5.5-bin-hadoop3 /opt/spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQP7D00oGyR6"
      },
      "source": [
        "🔧 Thiết lập biến môi trường như sau:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RR3VHbS8Glrt"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/opt/spark/spark-3.5.5-bin-hadoop3\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-D7dd0y2-YG"
      },
      "source": [
        "- 📌 pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHxirntQ2-YG",
        "outputId": "af1e32fb-1727-40f7-a89e-213e80e54a24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in c:\\users\\nam\\anaconda3\\envs\\py3_9\\lib\\site-packages (3.5.4)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\nam\\anaconda3\\envs\\py3_9\\lib\\site-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i6VQ3wE2-YG"
      },
      "source": [
        "- 📌 findspark - tùy chọn, nếu chạy trên Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVLN3of82-YH",
        "outputId": "942e1662-b5cf-452f-c9be-5ebf89d3aea8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: findspark in c:\\users\\nam\\anaconda3\\envs\\py3_9\\lib\\site-packages (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install findspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "  <details><summary>🌊 Kham khảo thêm cách cài đặt trên local machine: </summary><br>\n",
        "\n",
        "  <details><summary> 🔹 Với ubuntu: </summary>\n",
        "    \n",
        "  Ghi chú: Ở đây mình xài python 3.9 của anaconda nên đường dẫn cài đặt sẽ khác so với python tải từ web truyền thống (lưu ý này dành cho mấy bạn mới đụng tới python 😅) .\n",
        "  \n",
        "  Tên environment của conda sẽ là `py3_9`, còn tên user trong hdh ubuntu thì sẽ đặt là `nam`. <br>\n",
        "\n",
        "  Chạy lệnh sau:\n",
        "\n",
        "    👉 sudo apt update\n",
        " \n",
        "    👉 sudo apt install openjdk-11-jdk -y\n",
        "\n",
        "    👉 java -version # kiểm tra thử sau khi cài\n",
        "\n",
        "  Cài biến môi trường:\n",
        "\n",
        "    👉 echo 'export SPARK_HOME=/opt/spark' >> ~/.bashrc\n",
        "\n",
        "    👉 echo 'export PATH=$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH' >> ~/.bashrc\n",
        "\n",
        "    👉 echo 'export PYSPARK_PYTHON=/home/nam/anaconda3/envs/py3_9/bin/python'\n",
        "    >> ~/.bashrc\n",
        "\n",
        "    👉 source ~/.bashrc\n",
        "\n",
        "  <br>\n",
        "  </details>\n",
        "\n",
        "  <details><summary>🔹 Với window: </summary>\n",
        "  \n",
        "  Đối với window thì hơi thủ công xíu\n",
        "\n",
        "\n",
        "  👉 Tải JDK 11 từ [Oracle](https://www.oracle.com/java/technologies/javase/jdk11-archive-downloads.html) hoặc dùng OpenJDK từ Adoptium.\n",
        "\n",
        "  👉 Sau khi tải về, cài đặt và kiểm tra phiên bản bằng lệnh: ```java -version```\n",
        "\n",
        "  👉 Tải Apache Spark phiên bản mới nhất ![](https://spark.apache.org/downloads.html`)\n",
        "\n",
        "  👉 Giải nén file .tgz vào thư mục C:\\spark (hoặc nơi khác tùy ý).\n",
        "\n",
        "  ❗❗❗ Có một lưu ý nhỏ là code mình vẫn chạy trên jupyter notebook, do đó cách cài biến môi trường gần giống như colab. Chỉ có một thay đổi nhỏ ở đoạn mã sau:\n",
        "\n",
        "  ```python\n",
        "    import os\n",
        "    os.environ[\"JAVA_HOME\"] = \"C:\\\\Program Files\\\\Java\\\\jdk-11\"\n",
        "    os.environ[\"SPARK_HOME\"] = \"C:\\\\spark\\\\spark-3.5.4-bin-hadoop3\"\n",
        "    os.environ[\"PYSPARK_PYTHON\"] = \"C:\\\\Users\\\\nam\\\\anaconda3\\\\envs\\\\py3_9\\\\python.exe\"\n",
        "\n",
        "  Trong đó lần lượt từ trên xuống dưới là đường dẫn tới chương trình java, spark , và python tương ứng trên máy của mình. Có thể bạn đọc sẽ gặp khó khăn khi mới đầu cài đặt spark trên local machine nhưng chịu khó vọc vạch và hỏi chatgpt thì mình tin là sẽ giải quyết được. Chúc bạn thành công 😉\n",
        "  ```\n",
        "  </details>\n",
        "  </details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zwpg9kB1X0nv"
      },
      "source": [
        "## Run Spark app 🚀"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IfPtbeXrZXHk"
      },
      "outputs": [],
      "source": [
        "# khởi tạo một file .py\n",
        "# ! touch firstapp.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmXe6eUfZxUq",
        "outputId": "429b848e-0ce7-4adb-ddd0-aed23afca745"
      },
      "outputs": [],
      "source": [
        "# # ghi nội dung vào file\n",
        "# # mới đầu đừng quan tâm nó code gì, chỉ biết nó chạy là được, biết thì càng tốt :V\n",
        "\n",
        "# %%writefile firstapp.py\n",
        "\n",
        "# from pyspark.sql import SparkSession\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     spark = SparkSession.builder \\\n",
        "#         .appName(\"First Spark Application\") \\\n",
        "#         .master(\"local[*]\") \\\n",
        "#         .getOrCreate()\n",
        "\n",
        "#     data = [(\"Alice\", 1), (\"Bob\", 2)]\n",
        "#     df = spark.createDataFrame(data, [\"Name\", \"Value\"])\n",
        "#     df.show()\n",
        "\n",
        "#     spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJVBudvWZe7K",
        "outputId": "4163b41c-03bd-457d-e243-82c413782928"
      },
      "outputs": [],
      "source": [
        "# kiểm tra nội dung đã ghi vào\n",
        "# ! cat firstapp.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5qPDPiRaLdF",
        "outputId": "d06f7f83-e53a-4d81-b60d-402dc29be1c1"
      },
      "outputs": [],
      "source": [
        "# chạy file này\n",
        "# ! $SPARK_HOME/bin/spark-submit firstapp.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c4nXT46ah8F"
      },
      "source": [
        "Đã có một bảng với 2 cột (Name, Value) cùng với dòng là (Alice, 1) và (Bob, 2). Vậy là ứng dụng đã chạy thành công !\n",
        "\n",
        "Nếu chạy file .py theo cách bình thường thì mọi tài nguyên và tính toán sẽ được thực hiện trên local machine (trong trường hợp này là colab), nhưng lại bị giới hạn về khả năng xử lý phân tán.\n",
        "\n",
        "Lệnh `spark-submit` cho phép ta gỡ bỏ điều đó bằng cách \"submit\"  file .py của bạn đến cluster và  phân phối công việc cho các node trong cluster đó.\n",
        "\n",
        "Giờ thì ta sẽ đi sâu hơn vào từng phần của mã nguồn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8Jo6rOf2-YH"
      },
      "source": [
        "## SparkSession 🔥"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6VlbveA2-YH"
      },
      "source": [
        "Apache Spark là một hệ thống phân tán, chạy trên nhiều node. Để tương tác với spark, chúng ta cần một thứ gọi là \"điểm khởi đầu\" (Entry point). Trong trường hợp này chính là SparkSession. SparkSession giống như cửa chính của một tòa nhà, giúp bạn truy cập vào các thành phần bên trong như SparkContext, StreamContext, SQLContext,... (mấy cái này bàn sau)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64OXTUScUG-i"
      },
      "source": [
        "![](https://abhishekbaranwal10.files.wordpress.com/2018/09/introduction-to-apache-spark-20-12-638.jpg?resize=638%2C479&is-pending-load=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7YmADrJF2-YH"
      },
      "outputs": [],
      "source": [
        "# khởi tạo SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Datacamp Pyspark Tutorial\") \\\n",
        "    .config(\"spark.memory.offHeap.enabled\",\"true\") \\\n",
        "    .config(\"spark.memory.offHeap.size\",\"5g\").getOrCreate()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP4IdQzRCM60"
      },
      "source": [
        "Dừng SparkSession khi đã hoàn thành tất cả các tác vụ xử lý dữ liệu, nhằm giải phóng tài nguyên (như bộ nhớ, CPU và kết nối đến cluster)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbkhNJQ52-YH"
      },
      "outputs": [],
      "source": [
        "# spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tnpHHwMAGS4"
      },
      "source": [
        "## SparkConf 🔥"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-inNEMrWAMzn"
      },
      "source": [
        "SparkConf là một class đặc biệt giúp chúng ta thay đổi lại cách khởi tạo session\n",
        "\n",
        "Ở phần trên, chúng ta khởi tạo một session thông qua một builder có sẵn và nó khá đơn giản. Nhưng nếu muốn tùy chỉnh cấu hình chi tiết (như bộ nhớ, số lượng core, v.v.) thì ta có cách sau:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0x72-FNBUE5",
        "outputId": "ab259ba9-547c-4097-9e1f-64e9682ebda3"
      },
      "source": [
        "```python\n",
        "    from pyspark import SparkConf\n",
        "\n",
        "    # Tạo SparkConf với các cấu hình cần thiết\n",
        "    conf = SparkConf() \\\n",
        "        .setAppName(\"MyApp\") \\\n",
        "        .setMaster(\"local[*]\") \\\n",
        "        .set(\"spark.some.config.option\", \"some-value\")\n",
        "\n",
        "    # Tạo SparkSession bằng cách truyền SparkConf\n",
        "    spark = SparkSession \\\n",
        "            .builder \\\n",
        "            .config(conf=conf) \\\n",
        "            .getOrCreate()\n",
        "\n",
        "    # Kiểm tra cấu hình từ SparkSession\n",
        "    print(spark.conf.get(\"spark.some.config.option\"))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZistYlEcBieI"
      },
      "source": [
        "Đối với đa số ứng dụng, việc sử dụng trực tiếp builder của SparkSession đã đủ nên có thể bỏ qua bước này."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VvnNMdM2-YH"
      },
      "source": [
        "## SparkContext 🔥"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAuKglvr2-YH"
      },
      "source": [
        "🔹SparkContext là class cốt lõi trong Apache Spark, giúp ứng dụng kết nối với cluster manager - trình quản lý các tác vụ tính toán phân tán.\n",
        "\n",
        "🔹SparkContext được sử dụng để khởi tạo các cấu trúc dữ liệu đặc biệt như RDD, accumulator, và broadcast variable (cái này sẽ nói sau)\n",
        "\n",
        "🔹Trước Spark phiên bản 2.x, SparkContext là entry point chính.\n",
        "\n",
        "🔹Từ Spark 2.x trở đi, SparkSession thay thế SparkContext, nhưng SparkContext vẫn tồn tại bên trong SparkSession.\n",
        "\n",
        "Để hiểu sâu hơn, hãy nhìn sơ đồ bên dưới:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ido4zEM78zm"
      },
      "source": [
        "![](https://sparkbyexamples.com/wp-content/uploads/2022/05/image04.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PFbmL-hbF4h"
      },
      "source": [
        "Như đã nói, khi chạy một Spark app, chương trình sẽ được \"submit\" tới cluster và được đưa đến các node đang có dữ liệu. Trong đó, một node sẽ được chọn làm **Master node** để chạy một chương trình gọi là **Driver Program**, các node còn lại sẽ đóng vai trò là **Worker**.\n",
        "\n",
        "Tại **Driver Program**, một đối tượng **SparkContext** sẽ được khởi tạo giúp ứng dụng Spark giao tiếp với **Cluster Manager** để yêu cầu tài nguyên. Sau khi tài nguyên được phân bổ cho các node, **Cluster Manager** sẽ khởi động các **Executor**.\n",
        "\n",
        "**Executor** là những tiến trình chạy trên các **Worker** và chúng sẽ xử lý các **Task** được giao bởi **Driver Program**.\n",
        "\n",
        "**Driver Program** sẽ tạo task và phân chia cho các **Worker** theo nguyên tắc xử lý cục bộ, tức là tài nguyên trên node nào sẽ được xử lý bởi **Executor** trên node đó."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2vZTRc4fcMq"
      },
      "source": [
        "Nói tới đây thì có lẽ chúng ta đã định hình được vai trò của SparkContext rồi. Hãy nhớ, bất cứ khi nào làm việc với Spark, luôn khởi tạo SparkContext sau khi đã có SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Y93vaZH2-YI",
        "outputId": "ceadcb04-c400-4d97-e2e4-6dd7c287bdf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<SparkContext master=local[*] appName=Datacamp Pyspark Tutorial>\n"
          ]
        }
      ],
      "source": [
        "# Lấy SparkContext hiện có từ SparkSession\n",
        "\n",
        "try:\n",
        "  sc = spark.sparkContext\n",
        "  print(sc)\n",
        "except:\n",
        "  print('Chưa import spark phải không ? nhớ import nha :v')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "4gq70-rV2-YI",
        "outputId": "1eea7ba4-8c78-4462-dcb7-e6fbfed761dc"
      },
      "outputs": [],
      "source": [
        "# kiểm tra có SparkContext nào đang chạy không\n",
        "from pyspark import SparkContext\n",
        "SparkContext._active_spark_context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Một cách khác để khởi tạo spark context mà không cần spark session:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iS3Alc0L2-YH",
        "outputId": "cce42a49-ff69-4485-ae83-673ec408921b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SparkContext đã tồn tại, chỉ khởi tạo một lần. Đọc thêm ở bên dưới để biết thêm chi tiết\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "# Khởi tạo sparkContext\n",
        "\n",
        "try:\n",
        "  sc = SparkContext(\"local\", \"newAppName\")\n",
        "  print(sc)\n",
        "except:\n",
        "  print(\"SparkContext đã tồn tại, chỉ khởi tạo một lần. Đọc thêm ở bên dưới để biết thêm chi tiết\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS9Ns5_l2-YI"
      },
      "source": [
        "🔹Lưu ý: sparkContext chỉ khởi tạo một lần\n",
        "\n",
        "🔹Chạy lần 2 bị báo lỗi `Cannot run multiple SparkContexts at once; existing SparkContext`\n",
        "\n",
        "🔹Do đó, không cần phải khởi tạo lần nữa, và nhảy sang phần tiếp theo: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nếu muốn dừng spark sesson:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<SparkContext master=local appName=newAppName>\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkContext\n",
        "try:\n",
        "  # sc.stop()\n",
        "  # sc = SparkContext(\"local\", \"newAppName\") # tạo lại cái mới\n",
        "  print(sc)\n",
        "except:\n",
        "  print('Bị lỗi vì không thể gọi đến sc.stop(). Có thể biến sc chưa được khởi tạo trước đó. ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rFZdy_I2-YI"
      },
      "source": [
        "## RDD 🔥\n",
        "\n",
        "RDD (Resilient Distributed Dataset) là cấu trúc dữ liệu cốt lõi của Apache Spark, cho phép xử lý dữ liệu phân tán trên các cluster một cách linh hoạt và hiệu quả."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OdUmVugmOk3"
      },
      "source": [
        "![](https://images.viblo.asia/2cd88166-3c16-4cdc-9298-ce9900ac1288.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmpiYtGcmU6z"
      },
      "source": [
        "🔴 **Các tính chất của RDD:**\n",
        "\n",
        "- Tính Phân tán (Distributed): Mỗi RDD được chia thành các phần nhỏ gọi là partitions, mỗi partition có thể được xử lý độc lập trên các node khác nhau trong 1 cluster\n",
        "\n",
        "- Tính Linh hoạt (Resilient): RDD có thể tự động phục hồi sau khi một phần của dữ liệu hoặc một phần của cluster bị lỗi.\n",
        "\n",
        "- Tính Bất biến : Sau khi được tạo, một RDD không thể thay đổi.\n",
        "\n",
        "- Đánh giá lười (Lazy Evaluation): Các phép biến đổi trên RDD không được thực hiện ngay lập tức mà chỉ khi có hành động (action) được gọi.\n",
        "\n",
        "- Tính tối ưu hóa (Optimized): RDDs có thể tối ưu hóa để tận dụng các hoạt động in-memory, giảm thiểu việc truy cập dữ liệu từ đĩa và tối ưu hóa việc chuyển dữ liệu giữa các phần của RDD trên cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzOKzzog2-YI",
        "outputId": "f6195d12-6a51-43f2-9377-bc38ffb5b6a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Scalar', 'java', 'hadoop', 'spark', 'akka', 'spark and hadoop', 'pyspark', 'pyspark and spark']\n",
            "<class 'list'>\n"
          ]
        }
      ],
      "source": [
        "words = [\n",
        "    'Scalar',\n",
        "    'java',\n",
        "    'hadoop',\n",
        "    'spark',\n",
        "    'akka',\n",
        "    'spark and hadoop',\n",
        "    'pyspark',\n",
        "    'pyspark and spark'\n",
        "]\n",
        "\n",
        "print(words)\n",
        "print(type(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ym4CpQyl2-YI",
        "outputId": "bb53d409-dc49-4aca-d40e-369b97e1200f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pyspark.rdd.RDD'>\n"
          ]
        }
      ],
      "source": [
        "rdd_words = sc.parallelize(words)\n",
        "print(type(rdd_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Actions 🔴"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkV-sspxOyn7"
      },
      "source": [
        "Các hành động thực thi tính toán trên RDD và trả về kết quả.\n",
        "Một số hành động phổ biến:\n",
        "\n",
        "- collect(): Lấy tất cả phần tử từ RDD về driver.\n",
        "- count(): Đếm số phần tử trong RDD.\n",
        "- first(): Lấy phần tử đầu tiên.\n",
        "- take(n): Lấy n phần tử đầu tiên.\n",
        "- reduce(f): Gộp các phần tử với một hàm f."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JEYDPTVkkEr"
      },
      "source": [
        "**1️⃣ collect()**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjsBmaJpk-S8"
      },
      "source": [
        "Thu thập toàn bộ dữ liệu từ các phân vùng của RDD trên các worker node và đưa về driver dưới dạng một danh sách Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S43jRCL5jtNG",
        "outputId": "1d47b779-9b06-4294-a66f-f165a2263219"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Scalar', 'java', 'hadoop', 'spark', 'akka', 'spark and hadoop', 'pyspark', 'pyspark and spark']\n",
            "<class 'list'>\n"
          ]
        }
      ],
      "source": [
        "a = rdd_words.collect()\n",
        "print(a)\n",
        "print(type(a))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2O2WgUQlt7J"
      },
      "source": [
        "Khi gọi collect(), Spark sẽ thực hiện toàn bộ các phép biến đổi (transformation) đã được định nghĩa trên RDD để tạo ra dữ liệu cuối cùng.\n",
        "\n",
        "Lưu ý rằng nó có thể dễ dàng gây ra vấn đề bộ nhớ nếu dữ liệu quá lớn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR3p-dSxoits"
      },
      "source": [
        "**2️⃣count()**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUIn-tAOlrwq"
      },
      "source": [
        "Đếm số lượng phần tử có trong RDD và trả về kết quả dưới dạng một số nguyên (integer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsl6MInM2-YI",
        "outputId": "a5d9316b-08ae-4202-8e0c-7d81ac29c7f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8\n"
          ]
        }
      ],
      "source": [
        "print(rdd_words.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8veLylko0Ex"
      },
      "source": [
        "Khi gọi count(), tất cả các phép biến đổi (transformation) đã được định nghĩa trên RDD sẽ được thực hiện để tạo ra dữ liệu cuối cùng trước khi đếm số lượng phần tử.\n",
        "\n",
        "Không giống như collect, count chỉ trả về số đếm, do đó nó không gây ra quá tải bộ nhớ trên driver ngay cả khi RDD chứa rất nhiều phần tử."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eY_0nm72ZTZ"
      },
      "source": [
        "**3️⃣first()**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nN_kEudH4O6-"
      },
      "source": [
        "Chỉ lấy về 1 phần tử mà không cần chuyển toàn bộ dữ liệu về driver, giúp tránh quá tải bộ nhớ.\n",
        "\n",
        "Nếu RDD rỗng, phương thức này sẽ ném ra lỗi (exception)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "iInYad2pOyUf",
        "outputId": "d05041db-9c8b-4ce8-e289-3d63ffc87a9b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Scalar'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd_words.first()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT-zKWbUhkmr"
      },
      "source": [
        "**4️⃣take()**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZLwx1TUmeIh"
      },
      "source": [
        "`take(n)` trả về một danh sách chứa n phần tử đầu tiên của RDD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzo1mKWuhw2r",
        "outputId": "4cafe82c-d0dc-4af5-9daf-840187ee9648"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Scalar', 'java', 'hadoop', 'spark', 'akka']"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rdd_words.take(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnNgYwUM4LBu"
      },
      "source": [
        "Khi gọi `take(n)`, Spark sẽ duyệt qua các partition của RDD cho đến khi thu thập đủ n phần tử. Nếu dữ liệu trong RDD ít hơn n, nó sẽ trả về tất cả các phần tử có sẵn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_DgNcZ5oAQE"
      },
      "source": [
        "**🔴Các phép biến đổi (Transformations) trên RDD**\n",
        "\n",
        "Các phép biến đổi trên RDD tạo ra RDD mới mà không thay đổi RDD ban đầu.\n",
        "\n",
        "Một số phép biến đổi phổ biến:\n",
        "\n",
        "- map(f): Áp dụng một hàm f lên từng phần tử.\n",
        "- filter(f): Lọc các phần tử thỏa mãn điều kiện f.\n",
        "- flatMap(f): Giống map(), nhưng mở rộng kết quả thành nhiều phần tử.\n",
        "- groupByKey(): Nhóm các phần tử có cùng khóa (key-value RDD).\n",
        "- reduceByKey(f): Kết hợp các giá trị có cùng khóa bằng một hàm f."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmUuvbkOoTVy"
      },
      "source": [
        "**filter()**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Wji1RB6oR3L"
      },
      "source": [
        "filter nhận vào một hàm điều kiện (predicate) và áp dụng hàm đó cho từng phần tử của RDD.\n",
        "\n",
        "Chỉ những phần tử mà hàm trả về True sẽ được giữ lại trong RDD kết quả.\n",
        "\n",
        "RDD ban đầu không bị thay đổi. Thay vào đó, filter tạo ra một RDD mới chứa các phần tử đã được lọc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "H8LLtHhXmg4N",
        "outputId": "84ec774e-97ef-4f8c-b21b-79cfc0a4dee6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['hadoop', 'spark and hadoop']\n"
          ]
        }
      ],
      "source": [
        "# dùng filter để lấy ra các chuỗi có 2 chữ 'oo'\n",
        "filtered_char = rdd_words.filter(lambda x: 'oo' in x)\n",
        "\n",
        "# Thu thập kết quả và in ra\n",
        "print(filtered_char.collect())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py3_9",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
