{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle exception in Pyspark üõ†Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**L·ªùi n√≥i ƒë·∫ßu:**\n",
    "\n",
    "üîπ Khi l√†m vi·ªác v·ªõi pyspark cho t√°c v·ª• x·ª≠ l√Ω d·ªØ li·ªáu l·ªõn, m√¨nh ƒë√£ g·∫∑p kh√¥ng √≠t l·ªói v·ªÅ ph·∫ßn c√†i ƒë·∫∑t c≈©ng nh∆∞ c·∫•u h√¨nh framework, nh·∫•t l√† khi l√†m tr√™n local machine, (remote th√¨ kh·ªèi n√≥i, n√≥ support t·∫≠n rƒÉng, ch·∫≥ng h·∫°n nh∆∞ colab üôÑ). ƒê√¢y l√† s·∫Ω tutorial ƒë√∫c k·∫øt ra nhi·ªÅu kinh nghi·ªám x·ª≠ l√Ω l·ªói sau nhi·ªÅu ng√†y v·∫≠t l·ªôn v·ªõi c√°i pyspark n√†y.\n",
    "\n",
    "üîπ L·ªói (error) v√† ngo·∫°i l·ªá (exception) trong PySpark r·∫•t d√†i. L√†m vi·ªác v·ªõi PySpark gi·ªëng nh∆∞ ki·ªÉu b·∫°n ph·∫£i l√†m vi·ªác v·ªõi combo Python error + exception Java v·∫≠y, m√† ph·∫ßn l·ªõn l·∫°i li√™n quan t·ªõi spark session v√† spark context ( m√≤ l√¢u l·∫Øm m·ªõi t√¨m ra ƒë∆∞·ª£c nguy√™n nh√¢n ü•¥ )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case List üìáüìáüìá"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D∆∞·ªõi ƒë√¢y l√† m·ªôt v√†i t√¨nh hu·ªëng m√† m√¨nh g·∫∑p ph·∫£i (ƒë√∫ng h∆°n l√† t·ª± t·∫°o ra üò¢) khi l√†m vi·ªác v·ªõi pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üî¥ T√πy √Ω stop v√† kh·ªüi t·∫°o l·∫°i session v√† context**\n",
    "\n",
    "X√©t m√£ ngu·ªìn b√™n d∆∞·ªõi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Scalar',\n",
       " 'java',\n",
       " 'hadoop',\n",
       " 'spark',\n",
       " 'akka',\n",
       " 'spark and hadoop',\n",
       " 'pyspark',\n",
       " 'pyspark and spark']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Kh·ªüi t·∫°o m·ªôt session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Datacamp Pyspark Tutorial\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\",\"true\") \\\n",
    "    .config(\"spark.memory.offHeap.size\",\"5g\").getOrCreate()\n",
    "\n",
    "# L·∫•y Context t·ª´ session\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# t·∫°o m·ªôt rdd\n",
    "rdd_words = sc.parallelize(\n",
    "    [\n",
    "    'Scalar',\n",
    "    'java',\n",
    "    'hadoop',\n",
    "    'spark',\n",
    "    'akka',\n",
    "    'spark and hadoop',\n",
    "    'pyspark',\n",
    "    'pyspark and spark'\n",
    "    ]\n",
    ")\n",
    "\n",
    "# show k·∫øt qu·∫£\n",
    "rdd_words.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "M·ªçi th·ª© di·ªÖn ra kh√° b√¨nh th∆∞·ªùng. B√¢y gi·ªù m√¨nh s·∫Ω th·ª≠ stop context v√† kh·ªüi t·∫°o l·∫°i m·ªôt c√°i m·ªõi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
