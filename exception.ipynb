{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle exception in Pyspark 🛠️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lời nói đầu:**\n",
    "\n",
    "🔹 Khi làm việc với pyspark cho tác vụ xử lý dữ liệu lớn, mình đã gặp không ít lỗi về phần cài đặt cũng như cấu hình framework, nhất là khi làm trên local machine, (remote thì khỏi nói, nó support tận răng, chẳng hạn như colab 🙄). Đây là sẽ tutorial đúc kết ra nhiều kiến thức xử lý lỗi sau nhiều ngày vật lộn với cái pyspark này.\n",
    "\n",
    "🔹 Lỗi (error) và ngoại lệ (exception) trong PySpark rất dài. Làm việc với PySpark giống như kiểu bạn phải làm việc với combo Python error + exception Java vậy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://th.bing.com/th/id/OIP.vTD1E70SWGFDQEGOJHHAyAHaC9?rs=1&pid=ImgDetMain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small Cases 📇📇📇"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dưới đây là một vài lỗi nho nhỏ mà mình gặp phải ở lần đầu tiên làm việc với pyspark (đúng hơn là tự mình tạo ra 😢) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**🚫 Cài đặt thiếu biến môi trường trên local**\n",
    "\n",
    "Yeah lỗi này cũng là do lần đầu tiên \"ngựa ngựa\" cài đặt trên local machine, trong khi người người nhà nhà thì toàn làm trên google colab 😅.\n",
    "\n",
    "Lỗi này rất dài và sẽ thông báo như sau:\n",
    "\n",
    "```Error\n",
    "Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
    ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 1.0 failed 1 times, most recent failure: Lost task 7.0 in stage 1.0 (TID 19) (host.docker.internal executor driver): java.io.IOException: Cannot run program \"\": CreateProcess error=87, The parameter is incorrect\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lỗi này xuất hiện khi mình thao tác với action count() trên rdd của pyspark. Nó xuất phát từ `Py4J`, một thư viện giúp Python giao tiếp với JVM (Java Virtual Machine). Khi chạy một action như count(), PySpark sẽ submit lệnh từ Python sang Java (JVM). Nếu có lỗi xảy ra trong Java, nó sẽ được báo lại dưới dạng `Py4JJavaError` trong Python. \n",
    "\n",
    "Ở case của mình, nguyên nhân chủ yếu là mình chạy trên jupyter notebook, mặc dù đã cài biến môi trường `JAVA_HOME`, `SPARK_HOME`, và `PYSPARK_PYTHON` đầy đủ trên máy tính nhưng jupyter notebook vẫn không nhận diện được đường dẫn khiến cho một vài task bị fail.\n",
    "\n",
    "Cách giải quyết:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"C:\\\\Program Files\\\\Java\\\\jdk-11\"\n",
    "os.environ[\"SPARK_HOME\"] = \"C:\\\\spark\\\\spark-3.5.4-bin-hadoop3\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] =\"C:\\\\Users\\\\nam\\\\anaconda3\\\\envs\\\\py3_9\\\\python.exe\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❗❗❗ Lưu ý, tùy thuộc vào phiên bản python mà bạn đang cài đặt nhưng nhìn chung thì bạn sẽ phải mò mẫm xem cái  file python.exe nằm ở đâu. Còn với java, nếu bạn cài đặt phiên bản thấp hơn chẳng hạn java 8 thì chỉ cần đổi lại thành jdk-8 là được"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**🚫 Vô ý stop một context và thực thi với rdd**\n",
    "\n",
    "Lỗi này khá cơ bản, mình nghĩ rằng mình và cũng như nhiều beginer khi làm việc với các hệ thống xử lý dữ liệu lớn như spark sẽ gặp phải. Nhiều trường hợp mà một cách thần kỳ nào đó ta lại vô thức dừng session để rồi gặp phải thông báo sau:\n",
    "\n",
    "```Error\n",
    "AttributeError: 'NoneType' object has no attribute 'sc'\n",
    "```\n",
    "Ví dụ:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**✅ Code không lỗi:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Khởi tạo một session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Datacamp Pyspark Tutorial\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\",\"true\") \\\n",
    "    .config(\"spark.memory.offHeap.size\",\"5g\").getOrCreate()\n",
    "\n",
    "# Lấy Context từ session\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# tạo một rdd\n",
    "rdd_words = sc.parallelize(\n",
    "    [\n",
    "    'Scalar',\n",
    "    'java',\n",
    "    'hadoop',\n",
    "    'spark',\n",
    "    'akka',\n",
    "    'spark and hadoop',\n",
    "    'pyspark',\n",
    "    'pyspark and spark'\n",
    "    ]\n",
    ")\n",
    "\n",
    "# show kết quả\n",
    "rdd_words.collect()\n",
    "rdd_words.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**❌ Code bị lỗi:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'sc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m sc\u001b[38;5;241m.\u001b[39mstop()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# tạo một rdd\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m rdd_words \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallelize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mScalar\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjava\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhadoop\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspark\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43makka\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspark and hadoop\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpyspark\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpyspark and spark\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# show kết quả\u001b[39;00m\n\u001b[0;32m     20\u001b[0m rdd_words\u001b[38;5;241m.\u001b[39mcount()\n",
      "File \u001b[1;32mc:\\Users\\nam\\anaconda3\\envs\\py3_9\\lib\\site-packages\\pyspark\\context.py:783\u001b[0m, in \u001b[0;36mSparkContext.parallelize\u001b[1;34m(self, c, numSlices)\u001b[0m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparallelize\u001b[39m(\u001b[38;5;28mself\u001b[39m, c: Iterable[T], numSlices: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RDD[T]:\n\u001b[0;32m    752\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;124;03m    Distribute a local Python collection to form an RDD. Using range\u001b[39;00m\n\u001b[0;32m    754\u001b[0m \u001b[38;5;124;03m    is recommended if the input represents a range for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;124;03m    [['a'], ['b', 'c']]\u001b[39;00m\n\u001b[0;32m    782\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 783\u001b[0m     numSlices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(numSlices) \u001b[38;5;28;01mif\u001b[39;00m numSlices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefaultParallelism\u001b[49m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(c, \u001b[38;5;28mrange\u001b[39m):\n\u001b[0;32m    785\u001b[0m         size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(c)\n",
      "File \u001b[1;32mc:\\Users\\nam\\anaconda3\\envs\\py3_9\\lib\\site-packages\\pyspark\\context.py:630\u001b[0m, in \u001b[0;36mSparkContext.defaultParallelism\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefaultParallelism\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m    620\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    621\u001b[0m \u001b[38;5;124;03m    Default level of parallelism to use when not given by user (e.g. for reduce tasks)\u001b[39;00m\n\u001b[0;32m    622\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;124;03m    True\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m()\u001b[38;5;241m.\u001b[39mdefaultParallelism()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sc'"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc.stop()\n",
    "\n",
    "# tạo một rdd\n",
    "rdd_words = sc.parallelize(\n",
    "    [\n",
    "    'Scalar',\n",
    "    'java',\n",
    "    'hadoop',\n",
    "    'spark',\n",
    "    'akka',\n",
    "    'spark and hadoop',\n",
    "    'pyspark',\n",
    "    'pyspark and spark'\n",
    "    ]\n",
    ")\n",
    "\n",
    "# show kết quả\n",
    "rdd_words.count()\n",
    "\n",
    "# AttributeError: 'NoneType' object has no attribute 'sc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
