{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle exception in Pyspark ðŸ› ï¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lá»i nÃ³i Ä‘áº§u:**\n",
    "\n",
    "ðŸ”¹ Khi lÃ m viá»‡c vá»›i pyspark cho tÃ¡c vá»¥ xá»­ lÃ½ dá»¯ liá»‡u lá»›n, mÃ¬nh Ä‘Ã£ gáº·p khÃ´ng Ã­t lá»—i vá» pháº§n cÃ i Ä‘áº·t cÅ©ng nhÆ° cáº¥u hÃ¬nh framework, nháº¥t lÃ  khi lÃ m trÃªn local machine, (remote thÃ¬ khá»i nÃ³i, nÃ³ support táº­n rÄƒng, cháº³ng háº¡n nhÆ° colab ðŸ™„). ÄÃ¢y lÃ  sáº½ tutorial Ä‘Ãºc káº¿t ra nhiá»u kiáº¿n thá»©c xá»­ lÃ½ lá»—i sau nhiá»u ngÃ y váº­t lá»™n vá»›i cÃ¡i pyspark nÃ y.\n",
    "\n",
    "ðŸ”¹ Lá»—i (error) vÃ  ngoáº¡i lá»‡ (exception) trong PySpark ráº¥t dÃ i. LÃ m viá»‡c vá»›i PySpark giá»‘ng nhÆ° kiá»ƒu báº¡n pháº£i lÃ m viá»‡c vá»›i combo Python error + exception Java váº­y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://th.bing.com/th/id/OIP.vTD1E70SWGFDQEGOJHHAyAHaC9?rs=1&pid=ImgDetMain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small Cases ðŸ“‡ðŸ“‡ðŸ“‡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DÆ°á»›i Ä‘Ã¢y lÃ  má»™t vÃ i lá»—i nho nhá» mÃ  mÃ¬nh gáº·p pháº£i á»Ÿ láº§n Ä‘áº§u tiÃªn lÃ m viá»‡c vá»›i pyspark (Ä‘Ãºng hÆ¡n lÃ  tá»± mÃ¬nh táº¡o ra ðŸ˜¢) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ðŸš« CÃ i Ä‘áº·t thiáº¿u biáº¿n mÃ´i trÆ°á»ng trÃªn local**\n",
    "\n",
    "Yeah lá»—i nÃ y cÅ©ng lÃ  do láº§n Ä‘áº§u tiÃªn \"ngá»±a ngá»±a\" cÃ i Ä‘áº·t trÃªn local machine, trong khi ngÆ°á»i ngÆ°á»i nhÃ  nhÃ  thÃ¬ toÃ n lÃ m trÃªn google colab ðŸ˜….\n",
    "\n",
    "Lá»—i nÃ y ráº¥t dÃ i vÃ  sáº½ thÃ´ng bÃ¡o nhÆ° sau:\n",
    "\n",
    "```Error\n",
    "Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
    ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 1.0 failed 1 times, most recent failure: Lost task 7.0 in stage 1.0 (TID 19) (host.docker.internal executor driver): java.io.IOException: Cannot run program \"\": CreateProcess error=87, The parameter is incorrect\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lá»—i nÃ y xuáº¥t hiá»‡n khi mÃ¬nh thao tÃ¡c vá»›i action count() trÃªn rdd cá»§a pyspark. NÃ³ xuáº¥t phÃ¡t tá»« `Py4J`, má»™t thÆ° viá»‡n giÃºp Python giao tiáº¿p vá»›i JVM (Java Virtual Machine). Khi cháº¡y má»™t action nhÆ° count(), PySpark sáº½ submit lá»‡nh tá»« Python sang Java (JVM). Náº¿u cÃ³ lá»—i xáº£y ra trong Java, nÃ³ sáº½ Ä‘Æ°á»£c bÃ¡o láº¡i dÆ°á»›i dáº¡ng `Py4JJavaError` trong Python. \n",
    "\n",
    "á»ž case cá»§a mÃ¬nh, nguyÃªn nhÃ¢n chá»§ yáº¿u lÃ  mÃ¬nh cháº¡y trÃªn jupyter notebook, máº·c dÃ¹ Ä‘Ã£ cÃ i biáº¿n mÃ´i trÆ°á»ng `JAVA_HOME`, `SPARK_HOME`, vÃ  `PYSPARK_PYTHON` Ä‘áº§y Ä‘á»§ trÃªn mÃ¡y tÃ­nh nhÆ°ng jupyter notebook váº«n khÃ´ng nháº­n diá»‡n Ä‘Æ°á»£c Ä‘Æ°á»ng dáº«n khiáº¿n cho má»™t vÃ i task bá»‹ fail.\n",
    "\n",
    "CÃ¡ch giáº£i quyáº¿t:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"C:\\\\Program Files\\\\Java\\\\jdk-11\"\n",
    "os.environ[\"SPARK_HOME\"] = \"C:\\\\spark\\\\spark-3.5.4-bin-hadoop3\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] =\"C:\\\\Users\\\\nam\\\\anaconda3\\\\envs\\\\py3_9\\\\python.exe\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â—â—â— LÆ°u Ã½, tÃ¹y thuá»™c vÃ o phiÃªn báº£n python mÃ  báº¡n Ä‘ang cÃ i Ä‘áº·t nhÆ°ng nhÃ¬n chung thÃ¬ báº¡n sáº½ pháº£i mÃ² máº«m xem cÃ¡i  file python.exe náº±m á»Ÿ Ä‘Ã¢u. CÃ²n vá»›i java, náº¿u báº¡n cÃ i Ä‘áº·t phiÃªn báº£n tháº¥p hÆ¡n cháº³ng háº¡n java 8 thÃ¬ chá»‰ cáº§n Ä‘á»•i láº¡i thÃ nh jdk-8 lÃ  Ä‘Æ°á»£c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ðŸš« VÃ´ Ã½ stop má»™t context vÃ  thá»±c thi vá»›i rdd**\n",
    "\n",
    "Lá»—i nÃ y khÃ¡ cÆ¡ báº£n, mÃ¬nh nghÄ© ráº±ng mÃ¬nh vÃ  cÅ©ng nhÆ° nhiá»u beginer khi lÃ m viá»‡c vá»›i cÃ¡c há»‡ thá»‘ng xá»­ lÃ½ dá»¯ liá»‡u lá»›n nhÆ° spark sáº½ gáº·p pháº£i. Nhiá»u trÆ°á»ng há»£p mÃ  má»™t cÃ¡ch tháº§n ká»³ nÃ o Ä‘Ã³ ta láº¡i vÃ´ thá»©c dá»«ng session Ä‘á»ƒ rá»“i gáº·p pháº£i thÃ´ng bÃ¡o sau:\n",
    "\n",
    "```Error\n",
    "AttributeError: 'NoneType' object has no attribute 'sc'\n",
    "```\n",
    "VÃ­ dá»¥:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**âœ… Code khÃ´ng lá»—i:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Khá»Ÿi táº¡o má»™t session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Datacamp Pyspark Tutorial\") \\\n",
    "    .config(\"spark.memory.offHeap.enabled\",\"true\") \\\n",
    "    .config(\"spark.memory.offHeap.size\",\"5g\").getOrCreate()\n",
    "\n",
    "# Láº¥y Context tá»« session\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# táº¡o má»™t rdd\n",
    "rdd_words = sc.parallelize(\n",
    "    [\n",
    "    'Scalar',\n",
    "    'java',\n",
    "    'hadoop',\n",
    "    'spark',\n",
    "    'akka',\n",
    "    'spark and hadoop',\n",
    "    'pyspark',\n",
    "    'pyspark and spark'\n",
    "    ]\n",
    ")\n",
    "\n",
    "# show káº¿t quáº£\n",
    "rdd_words.collect()\n",
    "rdd_words.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**âŒ Code bá»‹ lá»—i:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'sc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m sc\u001b[38;5;241m.\u001b[39mstop()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# táº¡o má»™t rdd\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m rdd_words \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallelize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mScalar\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjava\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhadoop\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspark\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43makka\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspark and hadoop\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpyspark\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpyspark and spark\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# show káº¿t quáº£\u001b[39;00m\n\u001b[0;32m     20\u001b[0m rdd_words\u001b[38;5;241m.\u001b[39mcount()\n",
      "File \u001b[1;32mc:\\Users\\nam\\anaconda3\\envs\\py3_9\\lib\\site-packages\\pyspark\\context.py:783\u001b[0m, in \u001b[0;36mSparkContext.parallelize\u001b[1;34m(self, c, numSlices)\u001b[0m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparallelize\u001b[39m(\u001b[38;5;28mself\u001b[39m, c: Iterable[T], numSlices: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RDD[T]:\n\u001b[0;32m    752\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;124;03m    Distribute a local Python collection to form an RDD. Using range\u001b[39;00m\n\u001b[0;32m    754\u001b[0m \u001b[38;5;124;03m    is recommended if the input represents a range for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;124;03m    [['a'], ['b', 'c']]\u001b[39;00m\n\u001b[0;32m    782\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 783\u001b[0m     numSlices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(numSlices) \u001b[38;5;28;01mif\u001b[39;00m numSlices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefaultParallelism\u001b[49m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(c, \u001b[38;5;28mrange\u001b[39m):\n\u001b[0;32m    785\u001b[0m         size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(c)\n",
      "File \u001b[1;32mc:\\Users\\nam\\anaconda3\\envs\\py3_9\\lib\\site-packages\\pyspark\\context.py:630\u001b[0m, in \u001b[0;36mSparkContext.defaultParallelism\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefaultParallelism\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m    620\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    621\u001b[0m \u001b[38;5;124;03m    Default level of parallelism to use when not given by user (e.g. for reduce tasks)\u001b[39;00m\n\u001b[0;32m    622\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;124;03m    True\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m()\u001b[38;5;241m.\u001b[39mdefaultParallelism()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'sc'"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc.stop()\n",
    "\n",
    "# táº¡o má»™t rdd\n",
    "rdd_words = sc.parallelize(\n",
    "    [\n",
    "    'Scalar',\n",
    "    'java',\n",
    "    'hadoop',\n",
    "    'spark',\n",
    "    'akka',\n",
    "    'spark and hadoop',\n",
    "    'pyspark',\n",
    "    'pyspark and spark'\n",
    "    ]\n",
    ")\n",
    "\n",
    "# show káº¿t quáº£\n",
    "rdd_words.count()\n",
    "\n",
    "# AttributeError: 'NoneType' object has no attribute 'sc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
