{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems in pyspark üìùüîç"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark l√† m·ªôt c√¥ng c·ª• m·∫°nh m·∫Ω ƒë·ªÉ x·ª≠ l√Ω c√°c b√†i to√°n d·ªØ li·ªáu l·ªõn. Sau ƒë√¢y l√† m·ªôt v√†i b√†i to√°n kinh ƒëi·ªÉn:\n",
    "\n",
    "- wordcount\n",
    "- ASEAN countries count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCount üìä"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cho m·ªôt chu·ªói, y√™u c·∫ßu ƒë·∫øm t·ª´ng k√Ω t·ª± trong chu·ªói ƒë√≥."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Apache', 2),\n",
       " ('implicit', 1),\n",
       " ('developed', 1),\n",
       " ('of', 1),\n",
       " ('codebase', 1),\n",
       " ('and', 1),\n",
       " ('later', 1),\n",
       " ('to', 1),\n",
       " ('Software', 1),\n",
       " ('has', 1),\n",
       " ('open-source', 1),\n",
       " ('unified', 1),\n",
       " ('clusters', 1),\n",
       " ('at', 1),\n",
       " ('University', 1),\n",
       " (\"Berkeley's\", 1),\n",
       " ('for', 2),\n",
       " ('processing.', 1),\n",
       " ('with', 1),\n",
       " ('California,', 1),\n",
       " ('it', 1),\n",
       " ('since.', 1),\n",
       " ('analytics', 1),\n",
       " ('engine', 1),\n",
       " ('fault', 1),\n",
       " ('large-scale', 1),\n",
       " ('parallelism', 1),\n",
       " ('tolerance.', 1),\n",
       " ('maintained', 1),\n",
       " ('Spark', 3),\n",
       " ('programming', 1),\n",
       " ('data', 2),\n",
       " ('Originally', 1),\n",
       " ('donated', 1),\n",
       " ('Foundation,', 1),\n",
       " ('an', 2),\n",
       " ('provides', 1),\n",
       " ('was', 1),\n",
       " ('the', 3),\n",
       " ('which', 1),\n",
       " ('is', 1),\n",
       " ('AMPLab,', 1),\n",
       " ('interface', 1)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = \"Apache Spark is an open-source unified analytics engine for large-scale data processing. Spark provides an interface for programming clusters with implicit data parallelism and fault tolerance. Originally developed at the University of California, Berkeley's AMPLab, the Spark codebase was later donated to the Apache Software Foundation, which has maintained it since.\"\n",
    "\n",
    "result = sc.parallelize([doc]) \\\n",
    "    .flatMap(lambda x : x.split(\" \")) \\\n",
    "    .map(lambda x: (x, 1)) \\\n",
    "    .reduceByKey(lambda x, y : x + y) \\\n",
    "    .collect()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gi·∫£i th√≠ch chi ti·∫øt:\n",
    "\n",
    "- `sc.parallelize` s·∫Ω bao b·ªçc `doc` trong `list`, ƒë·∫£m b·∫£o r·∫±ng RDD c√≥ m·ªôt ph·∫ßn t·ª≠ l√† to√†n b·ªô chu·ªói.\n",
    "- √°p d·ª•ng `flatMap` l√™n chu·ªói ƒë√≥, ƒëi k√®m `split` ƒë·ªÉ t√°ch th√†nh m·ªôt `list` c√°c t·ª´, sau ƒë√≥ ‚Äúl√†m ph·∫≥ng‚Äù `list` n√†y th√†nh m·ªôt RDD ch·ª©a c√°c t·ª´.\n",
    "  \n",
    "  V√≠ d·ª•, khi ch·∫°y th√¨ k·∫øt qu·∫£ s·∫Ω l√†: \n",
    "  ```python\n",
    "  [\"Apache\", \"Spark\", \"is\", \"an\", \"open-source\", \"unified\", \"analytics\", \"engine\", \"for\", \"large-scale\", \"data\", \"processing.\", \"Spark\", \"provides\", \"an\", \"interface\", \"for\", \"programming\", \"clusters\", \"with\", \"implicit\", \"data\", \"parallelism\", \"and\", \"fault\", \"tolerance.\"]\n",
    "  ```\n",
    "\n",
    "\n",
    "- `map` chuy·ªÉn ƒë·ªïi m·ªói t·ª´ trong RDD th√†nh m·ªôt tuple c√≥ ƒë·ªãnh d·∫°ng `word, count)`\n",
    "\n",
    "    V√≠ d·ª•:\n",
    "\n",
    "    ```python\n",
    "    [(\"Apache\", 1), (\"Spark\", 1), (\"is\", 1), (\"an\", 1),\n",
    "    (\"open-source\", 1), (\"unified\", 1), (\"analytics\", 1), \n",
    "    (\"engine\", 1), (\"for\", 1), (\"large-scale\", 1), (\"data\", 1), \n",
    "    (\"processing.\", 1), (\"Spark\", 1), (\"provides\", 1), \n",
    "    (\"an\", 1), (\"interface\", 1), (\"for\", 1), (\"programming\", 1),\n",
    "    (\"clusters\", 1), (\"with\", 1), (\"implicit\", 1), (\"data\", 1),\n",
    "    (\"parallelism\", 1), (\"and\", 1), (\"fault\", 1), (\"tolerance.\", 1)]\n",
    "\n",
    "    ```\n",
    "- Sau ƒë√≥, `reduceByKey` nh√≥m c√°c tuple theo key word v√† √°p d·ª•ng h√†m lambda ƒë·ªÉ c·ªông d·ªìn count l·∫°i cho m·ªói t·ª´\n",
    "\n",
    "    Qu√° tr√¨nh x·ª≠ l√Ω nh∆∞ sau:\n",
    "\n",
    "    + Nh√≥m l·∫°i:\n",
    "    \n",
    "        ```python\n",
    "        {\n",
    "        \"Spark\": [1, 1],\n",
    "        \"is\": [1, 1],\n",
    "        \"an\": [1, 1],\n",
    "        \"open-source\": [1]\n",
    "        ....................\n",
    "        }\n",
    "        ```\n",
    "    + √Åp d·ª•ng h√†m lambda x, y: x + y cho m·ªói nh√≥m:\n",
    "    \n",
    "        ```python\n",
    "        \"Spark\": 1 + 1 = 2\n",
    "        \"is\": 1 + 1 = 2\n",
    "        \"an\": 1 + 1 = 2\n",
    "        \"open-source\": 1 (kh√¥ng thay ƒë·ªïi v√¨ ch·ªâ c√≥ m·ªôt ph·∫ßn t·ª≠)\n",
    "        ......................\n",
    "        ```\n",
    "\n",
    "K·∫øt qu·∫£ l√† ch√∫ng ta c√≥ t·∫ßn xu·∫•t v·ªõi t·ª´ng t·ª´.\n",
    "\n",
    "ƒê·∫∑t v·∫•n ƒë·ªÅ: ·ªû b∆∞·ªõc reduce, ta th·∫•y m·ªôt v√†i word c√≥ count = 1 v√† xu·∫•t hi·ªán 2 l·∫ßn n√™n ta hi·ªÉu r·∫±ng x + y ch√≠nh l√† 1 + 1. B·ªüi v√¨ 2 ch√≠nh l√† s·ªë l∆∞·ª£ng ph·∫ßn t·ª≠ c√πng key. V·∫≠y th√¨ n·∫øu s·ªë l∆∞·ª£ng n√†y l·ªõn h∆°n 2 th√¨ sao ? Ch·∫≥ng h·∫°n nh∆∞:\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"Spark\": [1, 1, 1],\n",
    "    \"is\": [1, 1],\n",
    "    \"an\": [1, 1]\n",
    "}\n",
    "```\n",
    "N·∫øu s·ªë l∆∞·ª£ng ph·∫ßn t·ª≠ c√πng key l·ªõn h∆°n 2, PySpark s·∫Ω t·ª± ƒë·ªông √°p d·ª•ng ph√©p to√°n nhi·ªÅu l·∫ßn ƒë·ªÉ g·ªôp t·∫•t c·∫£ gi√° tr·ªã v·ªÅ m·ªôt k·∫øt qu·∫£ duy nh·∫•t.\n",
    "\n",
    "Nh∆∞ v·∫≠y, vi·ªác √°p d·ª•ng lambda x, y: x + y cho t·ª´ng nh√≥m s·∫Ω ƒë∆∞·ª£c h√¨nh dung nh∆∞ sau:\n",
    "\n",
    "```python\n",
    "\"Spark\":\n",
    "    1 + 1 = 2\n",
    "    2 + 1 = 3\n",
    "\n",
    "\"is\": \n",
    "    1 + 1 = 2\n",
    "    \n",
    "\"an\":\n",
    "    1 + 1 = 2\n",
    "```\n",
    "\n",
    "K·∫øt qu·∫£ cu·ªëi c√πng: ```[(\"Spark\", 3), (\"is\", 2), (\"an\", 2)]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASEAN countries count üìä"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cho m·ªôt file d·ªØ li·ªáu `WHO-COVID-19-20210601-213841.tsv` ch·ª©a c√°c tr∆∞·ªùng sau:\n",
    "- Name \n",
    "- WHO Region \n",
    "- Cases - cumulative total \n",
    "- Cases - cumulative total per 100000 population \n",
    "- Cases - newly reported in last 7 days \n",
    "- Cases - newly reported in last 7 days per 100000 population \n",
    "- Cases - newly reported in last 24 hours \n",
    "- Deaths - cumulative total \n",
    "- Deaths - cumulative total per 100000 population \n",
    "- Deaths - newly reported in last 7 days \n",
    "- Deaths - newly reported in last 7 days per 100000 population \n",
    "- Deaths - newly reported in last 24 hours \n",
    "- Transmission Classification\n",
    "\n",
    "Bi·∫øt r·∫±ng c·ªôt `Cases - cumulative total` cho ta bi·∫øt t·ªïng s·ªë ca m·∫Øc covid c·ªßa t·ª´ng qu·ªëc gia ·ªü m·ªôt s·ªë khu v·ª±c \n",
    "\n",
    "D·ª±a v√†o c·ªôt `Cases - cumulative total` , y√™u c·∫ßu: \n",
    "- ƒê·∫øm xem c√≥ bao nhi√™u qu·ªëc gia ·ªü khu v·ª±c Ch√¢u √Å b·ªã nhi·ªÖm Covid\n",
    "- Ch·ªâ ra qu·ªëc gia c√≥ t·ªïng s·ªë ca l·ªõn nh·∫•t ·ªü khu v·ª±c n√†y\n",
    "- L·∫•y ra 3 qu·ªëc gia c√≥ t·ªïng s·ªë ca nh·ªè nh·∫•t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nam\\anaconda3\\envs\\py3_9\\lib\\site-packages\\pyspark\\sql\\context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Spark Asean count\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlc.read.csv(\"WHO-COVID-19-20210601-213841.tsv\",\n",
    "                   header=True,\n",
    "                   sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asianCount = df.where(df['WHO Region'] == 'South-East Asia') \\\n",
    "    .select(\"Cases - cumulative total\") \\\n",
    "    .rdd.count()\n",
    "     \n",
    "asianCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Cases - cumulative total='800,540.000')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxAsian = df.where(df['WHO Region'] == 'South-East Asia') \\\n",
    "    .select(\"Cases - cumulative total\") \\\n",
    "    .rdd \\\n",
    "    .max()\n",
    "    \n",
    "maxAsian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Bangladesh', Cases - cumulative total='800,540.000'),\n",
       " Row(Name='Bhutan', Cases - cumulative total='1,620.000'),\n",
       " Row(Name=\"Democratic People's Republic of Korea\", Cases - cumulative total='0.000')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_min = df.where(df['WHO Region'] == 'South-East Asia') \\\n",
    "    .select(\"Name\",'Cases - cumulative total') \\\n",
    "    .rdd \\\n",
    "    .takeOrdered(3, key=lambda x: x[0])\n",
    "    \n",
    "three_min "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
