{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems in pyspark üìùüîç"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark l√† m·ªôt c√¥ng c·ª• m·∫°nh m·∫Ω ƒë·ªÉ x·ª≠ l√Ω c√°c b√†i to√°n d·ªØ li·ªáu l·ªõn. Sau ƒë√¢y l√† m·ªôt v√†i b√†i to√°n kinh ƒëi·ªÉn:\n",
    "\n",
    "- WordCount\n",
    "- ASEAN countries count\n",
    "- House price predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chu·∫©n b·ªã m√¥i tr∆∞·ªùng cho vi·ªác ch·∫°y spark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Spark Asean count\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCount üìä <a id=\"wordcount\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B√†i to√°n c·∫•p ƒë·ªô \"hello word\" v·ªõi d√†nh cho c√°c beginner h·ªçc big data. Ta s·∫Ω gi·∫£i quy·∫øt b·∫±ng ph∆∞∆°ng ph√°p mapReduce.\n",
    "\n",
    "Cho m·ªôt chu·ªói, y√™u c·∫ßu ƒë·∫øm t·ª´ng k√Ω t·ª± trong chu·ªói ƒë√≥.\n",
    "\n",
    "- Input: \n",
    "```\n",
    "\"Apache Spark is an open-source unified analytics engine for large-scale data processing. Spark provides an interface for programming clusters with implicit data parallelism and fault tolerance. Originally developed at the University of California, Berkeley's AMPLab, the Spark codebase was later donated to the Apache Software Foundation, which has maintained it since.\"\n",
    "```\n",
    "\n",
    "- Output: \n",
    "    \n",
    "    danh s√°ch c√°c c·∫∑p `(word, count)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Apache', 2),\n",
       " ('implicit', 1),\n",
       " ('developed', 1),\n",
       " ('of', 1),\n",
       " ('codebase', 1),\n",
       " ('and', 1),\n",
       " ('later', 1),\n",
       " ('to', 1),\n",
       " ('Software', 1),\n",
       " ('has', 1),\n",
       " ('open-source', 1),\n",
       " ('unified', 1),\n",
       " ('clusters', 1),\n",
       " ('at', 1),\n",
       " ('University', 1),\n",
       " (\"Berkeley's\", 1),\n",
       " ('for', 2),\n",
       " ('processing.', 1),\n",
       " ('with', 1),\n",
       " ('California,', 1),\n",
       " ('it', 1),\n",
       " ('since.', 1),\n",
       " ('analytics', 1),\n",
       " ('engine', 1),\n",
       " ('fault', 1),\n",
       " ('large-scale', 1),\n",
       " ('parallelism', 1),\n",
       " ('tolerance.', 1),\n",
       " ('maintained', 1),\n",
       " ('Spark', 3),\n",
       " ('programming', 1),\n",
       " ('data', 2),\n",
       " ('Originally', 1),\n",
       " ('donated', 1),\n",
       " ('Foundation,', 1),\n",
       " ('an', 2),\n",
       " ('provides', 1),\n",
       " ('was', 1),\n",
       " ('the', 3),\n",
       " ('which', 1),\n",
       " ('is', 1),\n",
       " ('AMPLab,', 1),\n",
       " ('interface', 1)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = \"Apache Spark is an open-source unified analytics engine for large-scale data processing. Spark provides an interface for programming clusters with implicit data parallelism and fault tolerance. Originally developed at the University of California, Berkeley's AMPLab, the Spark codebase was later donated to the Apache Software Foundation, which has maintained it since.\"\n",
    "\n",
    "result = sc.parallelize([doc]) \\\n",
    "    .flatMap(lambda x : x.split(\" \")) \\\n",
    "    .map(lambda x: (x, 1)) \\\n",
    "    .reduceByKey(lambda x, y : x + y) \\\n",
    "    .collect()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gi·∫£i th√≠ch chi ti·∫øt:\n",
    "\n",
    "- `sc.parallelize` s·∫Ω bao b·ªçc `doc` trong `list`, ƒë·∫£m b·∫£o r·∫±ng RDD c√≥ m·ªôt ph·∫ßn t·ª≠ l√† to√†n b·ªô chu·ªói.\n",
    "- √Åp d·ª•ng `flatMap` l√™n chu·ªói ƒë√≥, ƒëi k√®m `split` ƒë·ªÉ t√°ch th√†nh m·ªôt `list` c√°c t·ª´, sau ƒë√≥ ‚Äúl√†m ph·∫≥ng‚Äù `list` n√†y th√†nh m·ªôt RDD ch·ª©a c√°c t·ª´.\n",
    "  \n",
    "  V√≠ d·ª•, khi ch·∫°y th√¨ k·∫øt qu·∫£ s·∫Ω l√†: \n",
    "  ```python\n",
    "  [\"Apache\", \"Spark\", \"is\", \"an\", \"open-source\", \"unified\", \"analytics\", \"engine\", \"for\", \"large-scale\", \"data\", \"processing.\", \"Spark\", \"provides\", \"an\", \"interface\", \"for\", \"programming\", \"clusters\", \"with\", \"implicit\", \"data\", \"parallelism\", \"and\", \"fault\", \"tolerance.\"]\n",
    "  ```\n",
    "\n",
    "\n",
    "- `map` chuy·ªÉn ƒë·ªïi m·ªói t·ª´ trong RDD th√†nh m·ªôt tuple c√≥ ƒë·ªãnh d·∫°ng `(word, count)`\n",
    "\n",
    "    V√≠ d·ª•:\n",
    "\n",
    "    ```python\n",
    "    [(\"Apache\", 1), (\"Spark\", 1), (\"is\", 1), (\"an\", 1),\n",
    "    (\"open-source\", 1), (\"unified\", 1), (\"analytics\", 1), \n",
    "    (\"engine\", 1), (\"for\", 1), (\"large-scale\", 1), (\"data\", 1), \n",
    "    (\"processing.\", 1), (\"Spark\", 1), (\"provides\", 1), \n",
    "    (\"an\", 1), (\"interface\", 1), (\"for\", 1), (\"programming\", 1),\n",
    "    (\"clusters\", 1), (\"with\", 1), (\"implicit\", 1), (\"data\", 1),\n",
    "    (\"parallelism\", 1), (\"and\", 1), (\"fault\", 1), (\"tolerance.\", 1)]\n",
    "\n",
    "    ```\n",
    "- Sau ƒë√≥, `reduceByKey` nh√≥m c√°c tuple theo word v√† √°p d·ª•ng h√†m lambda ƒë·ªÉ c·ªông d·ªìn count l·∫°i cho m·ªói t·ª´\n",
    "\n",
    "    Qu√° tr√¨nh x·ª≠ l√Ω nh∆∞ sau:\n",
    "\n",
    "    + Nh√≥m l·∫°i:\n",
    "    \n",
    "        ```python\n",
    "        \n",
    "        \"Spark\": [1, 1],\n",
    "        \"is\": [1, 1],\n",
    "        \"an\": [1, 1],\n",
    "        \"open-source\": [1]\n",
    "        \n",
    "        ```\n",
    "    + √Åp d·ª•ng h√†m `lambda x, y: x + y` cho m·ªói nh√≥m:\n",
    "    \n",
    "        ```python\n",
    "        \"Spark\": 1 + 1 = 2\n",
    "        \"is\": 1 + 1 = 2\n",
    "        \"an\": 1 + 1 = 2\n",
    "        \"open-source\": 1 (kh√¥ng thay ƒë·ªïi v√¨ ch·ªâ c√≥ m·ªôt ph·∫ßn t·ª≠)\n",
    "        \\\n",
    "        ```\n",
    "\n",
    "K·∫øt qu·∫£ l√† ch√∫ng ta c√≥ t·∫ßn xu·∫•t v·ªõi t·ª´ng t·ª´.\n",
    "\n",
    "ƒê·∫∑t v·∫•n ƒë·ªÅ: ·ªû b∆∞·ªõc reduce, ta th·∫•y m·ªôt v√†i word xu·∫•t hi·ªán 2 l·∫ßn (m·ªói l·∫ßn ƒë∆∞·ª£c ƒë·∫øm v·ªõi count = 1) n√™n ta hi·ªÉu r·∫±ng lambda `x + y` t∆∞∆°ng ƒë∆∞∆°ng `1 + 1` . V·∫≠y th√¨ n·∫øu ch√∫ng xu·∫•t hi·ªán nhi·ªÅu h∆°n 2 l·∫ßn th√¨ sao ? Ch·∫≥ng h·∫°n nh∆∞:\n",
    "\n",
    "```python\n",
    "\n",
    "    \"Spark\": [1, 1, 1],\n",
    "    \"is\": [1, 1],\n",
    "    \"an\": [1, 1]\n",
    "\n",
    "```\n",
    "N·∫øu s·ªë l∆∞·ª£ng ph·∫ßn t·ª≠ c√πng key l·ªõn h∆°n 2, PySpark s·∫Ω t·ª± ƒë·ªông √°p d·ª•ng ph√©p to√°n nhi·ªÅu l·∫ßn ƒë·ªÉ g·ªôp t·∫•t c·∫£ gi√° tr·ªã v·ªÅ m·ªôt k·∫øt qu·∫£ duy nh·∫•t.\n",
    "\n",
    "Nh∆∞ v·∫≠y, c√°ch x·ª≠ l√Ω cho t·ª´ng nh√≥m s·∫Ω ƒë∆∞·ª£c h√¨nh dung nh∆∞ sau:\n",
    "\n",
    "```python\n",
    "\"Spark\":\n",
    "    1 + 1 = 2\n",
    "    2 + 1 = 3\n",
    "\n",
    "\"is\": \n",
    "    1 + 1 = 2\n",
    "    \n",
    "\"an\":\n",
    "    1 + 1 = 2\n",
    "```\n",
    "\n",
    "K·∫øt qu·∫£ cu·ªëi c√πng: ```[(\"Spark\", 3), (\"is\", 2), (\"an\", 2)]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASEAN countries count üìä"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B√†i to√°n truy v·∫•n d·ªØ li·ªáu t·ª´ b·∫£ng d·ªØ li·ªáu t∆∞∆°ng t·ª± nh∆∞ c√°c c√¢u truy v·∫•n trong sql. \n",
    "\n",
    "\n",
    "Cho m·ªôt file d·ªØ li·ªáu `WHO-COVID-19-20210601-213841.tsv` ch·ª©a c√°c tr∆∞·ªùng sau:\n",
    "\n",
    "- Name \n",
    "- WHO Region \n",
    "- Cases - cumulative total \n",
    "- Cases - cumulative total per 100000 population \n",
    "- Cases - newly reported in last 7 days \n",
    "- Cases - newly reported in last 7 days per 100000 population \n",
    "- Cases - newly reported in last 24 hours \n",
    "- Deaths - cumulative total \n",
    "- Deaths - cumulative total per 100000 population \n",
    "- Deaths - newly reported in last 7 days \n",
    "- Deaths - newly reported in last 7 days per 100000 population \n",
    "- Deaths - newly reported in last 24 hours \n",
    "- Transmission Classification\n",
    "\n",
    "Bi·∫øt r·∫±ng tr∆∞·ªùng `Cases - cumulative total` cho ta bi·∫øt t·ªïng s·ªë ca m·∫Øc covid c·ªßa t·ª´ng qu·ªëc gia ·ªü m·ªôt s·ªë khu v·ª±c \n",
    "\n",
    "D·ª±a v√†o c·ªôt `Cases - cumulative total` , y√™u c·∫ßu: \n",
    "- ƒê·∫øm xem c√≥ bao nhi√™u qu·ªëc gia ·ªü khu v·ª±c Ch√¢u √Å b·ªã nhi·ªÖm Covid\n",
    "- Ch·ªâ ra qu·ªëc gia c√≥ t·ªïng s·ªë ca l·ªõn nh·∫•t ·ªü khu v·ª±c n√†y\n",
    "- L·∫•y ra 3 qu·ªëc gia c√≥ t·ªïng s·ªë ca nh·ªè nh·∫•t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nam\\anaconda3\\envs\\py3_9\\lib\\site-packages\\pyspark\\sql\\context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# nh·∫≠p d·ªØ li·ªáu\n",
    "sqlc = SQLContext(sc)\n",
    "df = sqlc.read.csv(\"WHO-COVID-19-20210601-213841.tsv\",\n",
    "                   header=True,\n",
    "                   sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQLContext gi√∫p ta kh·ªüi t·∫°o dataframe t∆∞∆°ng t·ª± nh∆∞ khi ch√∫ng ta l√†m vi·ªác v·ªõi pandas. \n",
    "\n",
    "L∆∞u √Ω nh·ªè: Khi kh·ªüi t·∫°o, ch√∫ng ta lu√¥n ch·ªâ ƒë·ªãnh `header=True` ƒë·ªÉ ƒë·∫£m b·∫£o nh·∫•t qu√°n ki·ªÉu d·ªØ li·ªáu. ƒê√£ c√≥ nhi·ªÅu case m√¨nh kh√¥ng l√†m ƒëi·ªÅu n√†y v√† m·∫Øc l·ªói column b·ªã t·ª± ƒë·ªông convert th√†nh `object` ho·∫∑c `string` m·∫∑c d√π ki·ªÉu c·ªßa ch√∫ng l√† numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ƒê·∫øm xem c√≥ bao nhi√™u qu·ªëc gia ·ªü khu v·ª±c Ch√¢u √Å b·ªã nhi·ªÖm Covid-19\n",
    "asianCount = df.where(df['WHO Region'] == 'South-East Asia') \\\n",
    "    .select(\"Cases - cumulative total\") \\\n",
    "    .rdd.count()\n",
    "     \n",
    "asianCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Cases - cumulative total='800,540.000')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ch·ªâ ra qu·ªëc gia c√≥ t·ªïng s·ªë ca l·ªõn nh·∫•t ·ªü khu v·ª±c n√†y\n",
    "maxAsian = df.where(df['WHO Region'] == 'South-East Asia') \\\n",
    "    .select(\"Cases - cumulative total\") \\\n",
    "    .rdd \\\n",
    "    .max()\n",
    "    \n",
    "maxAsian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Bangladesh', Cases - cumulative total='800,540.000'),\n",
       " Row(Name='Bhutan', Cases - cumulative total='1,620.000'),\n",
       " Row(Name=\"Democratic People's Republic of Korea\", Cases - cumulative total='0.000')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L·∫•y ra 3 qu·ªëc gia c√≥ t·ªïng s·ªë ca nh·ªè nh·∫•t\n",
    "three_min = df.where(df['WHO Region'] == 'South-East Asia') \\\n",
    "    .select(\"Name\",'Cases - cumulative total') \\\n",
    "    .rdd \\\n",
    "    .takeOrdered(3, key=lambda x: x[0])\n",
    "    \n",
    "three_min "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## House price predicting üìä"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B√†i to√°n d·ª± ƒëo√°n gi√° nh√† kinh ƒëi·ªÉn trong machine learning. Gi·ªù ch√∫ng ta s·∫Ω gi·∫£i quy·∫øt n√≥ v·ªõi pyspark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B·ªô d·ªØ li·ªáu c√≥ c√°c tr∆∞·ªùng:\n",
    "\n",
    "- longitude\n",
    "- latitude\n",
    "- housing_median_age\n",
    "- total_rooms\n",
    "- total_bedrooms\n",
    "- population\n",
    "- households\n",
    "- median_income\n",
    "- median_house_value\n",
    "\n",
    "Trong ƒë√≥, `median_house_value` s·∫Ω l√† target c·ªßa m√¥ h√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
      "|  -122.05|   37.37|              27.0|     3885.0|         661.0|    1537.0|     606.0|       6.6085|          344700.0|\n",
      "|   -118.3|   34.26|              43.0|     1510.0|         310.0|     809.0|     277.0|        3.599|          176500.0|\n",
      "|  -117.81|   33.78|              27.0|     3589.0|         507.0|    1484.0|     495.0|       5.7934|          270500.0|\n",
      "|  -118.36|   33.82|              28.0|       67.0|          15.0|      49.0|      11.0|       6.1359|          330000.0|\n",
      "|  -119.67|   36.33|              19.0|     1241.0|         244.0|     850.0|     237.0|       2.9375|           81700.0|\n",
      "|  -119.56|   36.51|              37.0|     1018.0|         213.0|     663.0|     204.0|       1.6635|           67000.0|\n",
      "|  -121.43|   38.63|              43.0|     1009.0|         225.0|     604.0|     218.0|       1.6641|           67000.0|\n",
      "|  -120.65|   35.48|              19.0|     2310.0|         471.0|    1341.0|     441.0|        3.225|          166900.0|\n",
      "|  -122.84|    38.4|              15.0|     3080.0|         617.0|    1446.0|     599.0|       3.6696|          194400.0|\n",
      "|  -118.02|   34.08|              31.0|     2402.0|         632.0|    2830.0|     603.0|       2.3333|          164200.0|\n",
      "|  -118.24|   33.98|              45.0|      972.0|         249.0|    1288.0|     261.0|       2.2054|          125000.0|\n",
      "|  -119.12|   35.85|              37.0|      736.0|         166.0|     564.0|     138.0|       2.4167|           58300.0|\n",
      "|  -121.93|   37.25|              36.0|     1089.0|         182.0|     535.0|     170.0|         4.69|          252600.0|\n",
      "|  -117.03|   32.97|              16.0|     3936.0|         694.0|    1935.0|     659.0|       4.5625|          231200.0|\n",
      "|  -117.97|   33.73|              27.0|     2097.0|         325.0|    1217.0|     331.0|       5.7121|          222500.0|\n",
      "|  -117.99|   33.81|              42.0|      161.0|          40.0|     157.0|      50.0|          2.2|          153100.0|\n",
      "|  -120.81|   37.53|              15.0|      570.0|         123.0|     189.0|     107.0|        1.875|          181300.0|\n",
      "|   -121.2|   38.69|              26.0|     3077.0|         607.0|    1603.0|     595.0|       2.7174|          137500.0|\n",
      "|  -118.88|   34.21|              26.0|     1590.0|         196.0|     654.0|     199.0|       6.5851|          300000.0|\n",
      "|  -122.59|   38.01|              35.0|     8814.0|        1307.0|    3450.0|    1258.0|       6.1724|          414300.0|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlc.read.csv('california_housing_test.csv',\n",
    "                   header=True,\n",
    "                   inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- housing_median_age: double (nullable = true)\n",
      " |-- total_rooms: double (nullable = true)\n",
      " |-- total_bedrooms: double (nullable = true)\n",
      " |-- population: double (nullable = true)\n",
      " |-- households: double (nullable = true)\n",
      " |-- median_income: double (nullable = true)\n",
      " |-- median_house_value: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# kh·∫£o s√°t th√¥ng tin t·ª´ng feature\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `VectorAssembler`: Chuy·ªÉn ƒë·ªïi nhi·ªÅu c·ªôt ƒë·∫∑c tr∆∞ng th√†nh m·ªôt vector ƒë·∫ßu v√†o.\n",
    "- `LinearRegression`: M√¥ h√¨nh h·ªìi quy tuy·∫øn t√≠nh ƒë·ªÉ d·ª± ƒëo√°n gi√° nh√†.\n",
    "- `RegressionEvaluator`: ƒê√°nh gi√° hi·ªáu su·∫•t m√¥ h√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------------+\n",
      "|            features|median_house_value|        prediction|\n",
      "+--------------------+------------------+------------------+\n",
      "|[-124.16,40.77,35...|           85600.0|160170.67677656002|\n",
      "|[-124.15,40.78,41...|          104200.0|171261.05998632498|\n",
      "|[-124.14,40.72,18...|          100500.0|133515.70980375214|\n",
      "|[-124.01,40.97,21...|          102700.0| 141594.6981221405|\n",
      "|[-123.74,40.66,25...|          136000.0| 136850.7204207722|\n",
      "|[-123.47,39.8,18....|           79200.0|142440.13013207726|\n",
      "|[-123.22,39.16,32...|          154600.0|206050.45340655604|\n",
      "|[-123.08,40.4,10....|           37500.0| 44796.16892597685|\n",
      "|[-122.84,38.4,15....|          194400.0|217804.47134948615|\n",
      "|[-122.84,38.98,21...|           75000.0|102171.95075276727|\n",
      "|[-122.79,38.54,5....|          213800.0|193491.70622318145|\n",
      "|[-122.78,38.97,11...|           97300.0|163766.73291285057|\n",
      "|[-122.75,38.48,4....|          197400.0|228880.58287195629|\n",
      "|[-122.73,38.43,29...|          143200.0|167864.13682480715|\n",
      "|[-122.7,38.35,14....|           67500.0|138859.12424935494|\n",
      "|[-122.68,38.01,41...|          255400.0| 278193.2617957154|\n",
      "|[-122.65,38.96,27...|           63200.0|130705.83174953284|\n",
      "|[-122.53,37.86,38...|          478600.0|238300.72375222156|\n",
      "|[-122.51,37.92,32...|          375000.0|253384.80942504387|\n",
      "|[-122.5,37.75,46....|          272400.0|266366.34324173443|\n",
      "+--------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = ['longitude', 'latitude', 'housing_median_age', \n",
    "     'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income']\n",
    "y = 'median_house_value'\n",
    "\n",
    "# Kh·ªüi t·∫°o ascsembler\n",
    "assembler = VectorAssembler(inputCols=X, outputCol='features')\n",
    "\n",
    "# ch·ªçn ƒë·∫∑c tr∆∞ng v√† label\n",
    "data = assembler.transform(df)\n",
    "data = data.select('features', y)\n",
    "\n",
    "# chia d·ªØ li·ªáu th√†nh t·∫≠p train v√† val\n",
    "train, val = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Hu·∫•n luy·ªán m√¥ h√¨nh     \n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=y)\n",
    "model = lr.fit(train_df)\n",
    "\n",
    "# D·ª± ƒëo√°n\n",
    "predictions = model.transform(val)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|            features|median_house_value|\n",
      "+--------------------+------------------+\n",
      "|[-124.16,40.77,35...|           85600.0|\n",
      "|[-124.15,40.78,41...|          104200.0|\n",
      "|[-124.14,40.72,18...|          100500.0|\n",
      "|[-124.01,40.97,21...|          102700.0|\n",
      "|[-123.74,40.66,25...|          136000.0|\n",
      "+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE): 74921.51407182051\n"
     ]
    }
   ],
   "source": [
    "# ƒë√°nh gi√° m√¥ h√¨nh\n",
    "predictions = model.transform(val)\n",
    "predictions.select('features', y).show(5)\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=y, \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"rmse\"\n",
    "    )\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|        prediction|\n",
      "+------------------+\n",
      "|353612.26725589857|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_data = spark.createDataFrame(\n",
    "    data = [(-122.05, 37.37, 27.0 ,3885.0, 661.0,1537.0, 606.0,6.6085, 344700.0)], \n",
    "    schema = X\n",
    "    )\n",
    "\n",
    "new_data = assembler.transform(new_data)\n",
    "new_pred = model.transform(new_data)\n",
    "new_pred.select(\"prediction\").show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
